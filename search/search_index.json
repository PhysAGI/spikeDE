{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to spikeDE's documentation","text":"<p>spikeDE is a powerful,  PyTorch-native library designed to build, train, and deploy Fractional-Order Spiking Neural Networks (f-SNNs).</p> <p>While traditional Spiking Neural Networks (SNNs) rely on integer-order differential equations (e.g., standard Leaky Integrate-and-Fire models) that assume Markovian dynamics\u2014where the current state depends solely on the immediate past\u2014spikeDE introduces a generalized fractional calculus framework. By utilizing Caputo fractional derivatives (\\(D^\\alpha\\)), our library enables neural units with inherent long-term memory and non-Markovian properties, closely mimicking the complex temporal dynamics and fractal structures observed in biological neurons.</p>"},{"location":"#why-fractional","title":"Why Fractional?","text":"<p>The core innovation of spikeDE lies in its ability to treat fractional order as a strict generalization of integer order. It is not merely an alternative model; it is a comprehensive superset framework.</p> Feature Traditional Integer-Order SNNs (\\(\\alpha = 1\\)) spikeDE Fractional-Order SNNs (\\(0 &lt; \\alpha \\le 1\\)) Dynamics Forgets history rapidly; strictly Markovian. Integrates history over long time horizons with heavy-tailed memory. Memory Mechanism Short-term memory with a fixed time constant. Simulating long dependencies requires large, deep networks. A single fractional neuron naturally captures multi-scale temporal correlations without extra depth. Expressivity Limited by fixed decay rates and finite timescales. Theoretical analysis proves that one fractional neuron cannot be equated to any finite ensemble of integer-order neurons. Robustness Often sensitive to input noise and parameter perturbations. Fractional dynamics provide enhanced robustness against disturbances and noise. <p>In spikeDE, setting the fractional order \\(\\alpha = 1\\) seamlessly recovers standard SNN behavior, ensuring backward compatibility. Conversely, setting \\(0 &lt; \\alpha &lt; 1\\) unlocks the power of fractional dynamics, offering superior performance in tasks requiring complex temporal processing, such as neuromorphic vision, speech recognition, and event-based graph learning.</p>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<p>Navigate through our comprehensive guides to get started, master the core concepts, or dive into advanced API usage.</p> <ul> <li> <p> Getting Started</p> <p>New to spikeDE? Start here to install the package and launch your first fractional spiking network with our step-by-step quickstart guide.</p> <p> Installation &amp; Quickstart</p> </li> <li> <p> Tutorials</p> <p>From core concepts to advanced mastery. Deepen your understanding of the mathematical foundations and learn how to optimize your spikeDE workflows.</p> <p> Explore Tutorials</p> </li> <li> <p> API Reference</p> <p>Comprehensive documentation for spikeDE's core modules. Build custom architectures with full control.</p> <p> Browse API Docs</p> </li> <li> <p> Blog &amp; Updates</p> <p>Stay updated with the latest posts on new features, performance benchmarks, and insights from the community.</p> <p> Visit Blog</p> </li> </ul> <p>Happy spiking with fractional dynamics!</p>"},{"location":"about/","title":"About spikeDE","text":"<p>Traditional SNNs, governed by integer-order differential equations, often struggle to capture long-range temporal dependencies without resorting to deep, computationally expensive architectures. Our team recognized that biological neurons operate with complex, non-Markovian dynamics\u2014properties naturally described by fractional calculus.</p> <p>The goal of spikeDE is to bridge the gap between advanced mathematical theory and practical deep learning applications. By providing a robust,  PyTorch-native implementation of Fractional-Order SNNs (f-SNNs), we aim to empower researchers and engineers to build more expressive, robust, and biologically plausible neural models with minimal effort.</p>"},{"location":"about/#the-research-behind-spikede","title":"The Research Behind spikeDE","text":"<p>The core algorithms and theoretical foundations of spikeDE are based on our paper, \"Fractional-order Spiking Neural Network\", which has been accepted by ICLR 2026 (International Conference on Learning Representations).</p>"},{"location":"about/#paper-highlights","title":"Paper Highlights","text":"<p>Our work introduces a paradigm shift from Markovian integer-order dynamics to non-Markovian fractional-order dynamics. Here are the key contributions:</p> <ul> <li> <p> Capturing Long-Range Dependencies</p> <p>Unlike traditional LIF neurons that rely on exponential decay, our f-LIF neurons utilize power-law relaxation via the Mittag-Leffler function. This allows the membrane potential to retain a \"heavy-tailed\" memory of past inputs, naturally modeling the complex temporal correlations observed in biological neurons.</p> </li> <li> <p> Proven Robustness &amp; Stability</p> <p>We provide theoretical guarantees showing that fractional-order dynamics suppress perturbation accumulation sub-linearly (\\(t^\\alpha\\) vs \\(t\\)). Experiments confirm that spikeDE models maintain superior accuracy under heavy noise injection, occlusion, and temporal jitter compared to integer-order baselines.</p> </li> <li> <p> Strict Generalization</p> <p>The spikeDE framework is a strict superset of traditional SNNs. By setting the fractional order \\(\\alpha=1\\), it recovers standard IF/LIF dynamics. This ensures seamless compatibility with existing architectures like CNNs, Transformers, and GNNs, requiring only a drop-in replacement of the neuron module.</p> </li> <li> <p> Efficiency Without Compromise</p> <p>Despite the added expressivity, our optimized solvers (using short-memory principles and FFT-based convolution) ensure that f-SNNs achieve comparable energy efficiency to traditional SNNs while delivering state-of-the-art performance on neuromorphic vision and graph learning tasks.</p> </li> </ul> <p>Key Theoretical Insight</p> <p>A single fractional-order neuron represents a continuum of timescales that would require infinitely many integer-order units for exact equivalence. This irreducibility grants f-SNNs fundamentally richer expressive power.</p>"},{"location":"about/#citation","title":"Citation","text":"<p>If you use spikeDE in your research or applications, please consider citing our paper.</p> <pre><code>@misc{ge2026fractionalorderspikingneuralnetwork,\n      title={Fractional-order Spiking Neural Network}, \n      author={Chengjie Ge and Yufeng Peng and Zihao Li and Qiyu Kang and Xueyang Fu and Xuhao Li and Qixin Zhang and Junhao Ren and Zheng-Jun Zha},\n      year={2026},\n      eprint={2507.16937},\n      archivePrefix={arXiv},\n      primaryClass={cs.NE},\n      url={https://arxiv.org/abs/2507.16937}, \n}\n</code></pre> <p> Read the full paper on arXiv</p> <p> View on OpenReview (ICLR 2026)</p>"},{"location":"about/#community-contribution","title":"Community &amp; Contribution","text":"<p>spikeDE is an open-source project licensed under the MIT License. We welcome contributions from the community!</p> <p>Whether you find a bug, have a feature request, want to improve documentation, or wish to contribute new fractional solvers/neuron models, please feel free to open an  issue or submit a  Pull Request on our  GitHub Repository.</p> <p>Let's build the future of Spiking Neural Networks together.</p>"},{"location":"api/","title":"API References","text":"<p>Welcome to the spikeDE API Reference. This documentation provides a comprehensive guide to the internal modules that power our continuous-time Spiking Neural Network framework.</p> <p>Designed with a Continuous Dynamics First philosophy, spikeDE bridges standard integer-order SNNs with advanced Fractional-Order Calculus, enabling infinite memory and complex temporal dependencies without altering your core model logic.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":"<ul> <li> <p> spikeDE.neuron</p> <p>The foundation of continuous spiking dynamics. Defines stateless neuron modules that compute instantaneous derivatives independent of history.</p> <p> View Module</p> </li> <li> <p> spikeDE.odefunc</p> <p>The engine for Spiking Neural ODEs. Uses PyTorch FX to symbolically trace and transform discrete SNNs into continuous vector field functions.</p> <p> View Module</p> </li> <li> <p> spikeDE.solver</p> <p>A differentiable numerical engine for Fractional Differential Equations (FDEs). Implements high-order discretization schemes like Gr\u00fcnwald-Letnikov.</p> <p> View Module</p> </li> <li> <p> spikeDE.snn</p> <p>The high-level wrapper (<code>SNNWrapper</code>) that unifies the ecosystem. Converts standard PyTorch SNNs into fractional systems with flexible configuration.</p> <p> View Module</p> </li> <li> <p> spikeDE.surrogate</p> <p>Essential tools for training SNNs via backpropagation. Provides smooth approximations for the non-differentiable spiking operation.</p> <p> View Module</p> </li> <li> <p> spikeDE.layer</p> <p>Specialized output modules for aggregating spatiotemporal spiking activities. Provides layers to prepare task-ready predictions.</p> <p> View Module</p> </li> </ul>"},{"location":"api/#design-philosophy","title":"Design Philosophy","text":"<p>The spikeDE architecture is built on the separation of concerns:</p> <ol> <li>Dynamics Definition: Neurons define what changes (\\(dv/dt\\)).</li> <li>State Evolution: Solvers define how it changes over time (integration).</li> <li>Graph Transformation: FX traces bridge the gap between discrete PyTorch modules and continuous mathematical systems.</li> </ol> <p>This modular design allows you to upgrade any standard SNN to a Fractional-Order SNN simply by wrapping it, unlocking powerful temporal modeling capabilities with minimal code changes.</p>"},{"location":"api/layer/","title":"spikeDE.layer","text":"<p>This module provides the essential building blocks for constructing the output stages of Spiking Neural Networks (SNNs) within the spikeDE framework. While the core neuron dynamics and ODE solvers handle continuous temporal evolution, this layer focuses on aggregating high-dimensional spiking activities into compact, task-ready representations.</p>"},{"location":"api/layer/#key-features","title":"Key Features","text":"<ul> <li>Structured Voting: Implements <code>VotingLayer</code> to perform group-wise averaging along the feature dimension, effectively reducing noise and compressing information based on predefined voting sizes.</li> <li>Spatiotemporal Aggregation: The <code>ClassificationHead</code> automatically handles multi-dimensional inputs, performing intelligent averaging across patch and time dimensions before applying the final linear projection.</li> </ul>"},{"location":"api/layer/#spikeDE.layer.VotingLayer","title":"VotingLayer","text":"<pre><code>VotingLayer(voting_size: int = 10, strict: bool = True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A layer that performs voting by averaging groups of elements along the last dimension.</p> <p>This layer reshapes the input tensor along the last dimension into groups of size <code>voting_size</code> and computes the mean for each group. It optionally enforces that the last dimension must be strictly divisible by <code>voting_size</code>.</p> <p>Attributes:</p> <ul> <li> <code>voting_size</code>               (<code>int</code>)           \u2013            <p>The size of the group to average over. Defaults to 10.</p> </li> <li> <code>strict</code>               (<code>bool</code>)           \u2013            <p>If True, raises an error if the last dimension is not divisible by <code>voting_size</code>. If False, truncates the excess elements. Defaults to True.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>voting_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The number of elements to group together for voting.</p> </li> <li> <code>strict</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to enforce strict divisibility of the last dimension.</p> </li> </ul> Source code in <code>spikeDE/layer.py</code> <pre><code>def __init__(self, voting_size: int = 10, strict: bool = True) -&gt; None:\n    \"\"\"Initializes the VotingLayer.\n\n    Args:\n        voting_size (int): The number of elements to group together for voting.\n        strict (bool): Whether to enforce strict divisibility of the last dimension.\n    \"\"\"\n    super().__init__()\n    self.voting_size = voting_size\n    self.strict = strict\n</code></pre>"},{"location":"api/layer/#spikeDE.layer.VotingLayer.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the voting operation on the input tensor.</p> <p>Reshapes the last dimension of the input into chunks of <code>voting_size</code> and calculates the mean across these chunks. If <code>strict</code> is False, the trailing elements that do not form a complete chunk are discarded.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape [..., L], where L is the length of the last dimension.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Output tensor of shape [..., L // voting_size], containing the averaged values.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>strict</code> is True and the last dimension of <code>x</code> is not divisible by <code>voting_size</code>.</p> </li> </ul> Source code in <code>spikeDE/layer.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the voting operation on the input tensor.\n\n    Reshapes the last dimension of the input into chunks of `voting_size` and\n    calculates the mean across these chunks. If `strict` is False, the trailing\n    elements that do not form a complete chunk are discarded.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape [..., L], where L is the length\n            of the last dimension.\n\n    Returns:\n        Output tensor of shape [..., L // voting_size], containing\n            the averaged values.\n\n    Raises:\n        ValueError: If `strict` is True and the last dimension of `x` is not\n            divisible by `voting_size`.\n    \"\"\"\n    v = self.voting_size\n    L = x.size(-1)\n    if self.strict and L % v != 0:\n        raise ValueError(f\"last dim ({L}) must be divisible by voting_size ({v})\")\n    L = (L // v) * v\n    x = x[..., :L]\n    return x.reshape(*x.shape[:-1], L // v, v).mean(dim=-1)\n</code></pre>"},{"location":"api/layer/#spikeDE.layer.ClassificationHead","title":"ClassificationHead","text":"<pre><code>ClassificationHead(embed_dims: int, num_classes: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A classification head that averages embeddings over patches and time steps.</p> <p>This module takes a multi-dimensional input tensor representing embeddings over time and patches, averages them to produce a single representation per batch item, and then applies a linear transformation to generate class logits.</p> <p>Attributes:</p> <ul> <li> <code>embed_dims</code>               (<code>int</code>)           \u2013            <p>The dimensionality of the input embeddings.</p> </li> <li> <code>num_classes</code>               (<code>int</code>)           \u2013            <p>The number of output classes.</p> </li> <li> <code>head</code>               (<code>Module</code>)           \u2013            <p>The linear layer for classification, or an Identity layer if <code>num_classes</code> is 0 or less.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>embed_dims</code>               (<code>int</code>)           \u2013            <p>The number of features in the input embedding.</p> </li> <li> <code>num_classes</code>               (<code>int</code>)           \u2013            <p>The number of target classes for classification.</p> </li> </ul> Source code in <code>spikeDE/layer.py</code> <pre><code>def __init__(self, embed_dims: int, num_classes: int) -&gt; None:\n    \"\"\"Initializes the ClassificationHead.\n\n    Args:\n        embed_dims (int): The number of features in the input embedding.\n        num_classes (int): The number of target classes for classification.\n    \"\"\"\n    super().__init__()\n    self.embed_dims = embed_dims\n    self.num_classes = num_classes\n    self.head = (\n        nn.Linear(embed_dims, num_classes) if num_classes &gt; 0 else nn.Identity()\n    )\n</code></pre>"},{"location":"api/layer/#spikeDE.layer.ClassificationHead.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Processes the input tensor to produce classification logits.</p> <p>The input is expected to have the shape [T, B, embed_dims, num_patches]. The method first averages over the patch dimension, then applies the linear classification head, and finally averages over the time dimension.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape [T, B, embed_dims, num_patches], where: - T: Time steps - B: Batch size - embed_dims: Embedding dimension - num_patches: Number of patches</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Output tensor of shape [B, num_classes] (or [B, embed_dims] if num_classes &lt;= 0), representing the classification scores.</p> </li> </ul> Source code in <code>spikeDE/layer.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Processes the input tensor to produce classification logits.\n\n    The input is expected to have the shape [T, B, embed_dims, num_patches].\n    The method first averages over the patch dimension, then applies the linear\n    classification head, and finally averages over the time dimension.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape [T, B, embed_dims, num_patches],\n            where:\n            - T: Time steps\n            - B: Batch size\n            - embed_dims: Embedding dimension\n            - num_patches: Number of patches\n\n    Returns:\n        Output tensor of shape [B, num_classes] (or [B, embed_dims]\n            if num_classes &lt;= 0), representing the classification scores.\n    \"\"\"\n    x = x.mean(-1)\n    x = self.head(x)\n    x = x.mean(0)\n    return x\n</code></pre>"},{"location":"api/neuron/","title":"spikeDE.neuron","text":"<p>This module provides a flexible and extensible framework for building Spiking Neural Networks (SNNs) that seamlessly bridge standard integer-order dynamics with advanced fractional-order calculus. Unlike traditional frameworks that rely on discrete step-by-step updates, spikeDE reimagines neurons as continuous dynamical systems. This architectural shift allows users to upgrade standard models into Fractional-Order Spiking Neurons, endowing them with infinite memory and complex temporal dependencies without altering core logic.</p> <p>At the heart of this module is the separation of concerns: neuron classes define instantaneous dynamics (computing derivatives), while external solvers (via <code>SNNWrapper</code>) handle state evolution and fractional integration. This design supports a wide range of models, from classic Integrate-and-Fire variants to sophisticated noisy-threshold and hard-reset mechanisms, all compatible with surrogate gradient learning.</p>"},{"location":"api/neuron/#key-features","title":"Key Features","text":"<ul> <li>Modular Architecture: Stateless neuron modules compute derivatives (\\(dv/dt\\)) independently of state history, allowing them to work interchangeably with standard (<code>odeint</code>) and fractional (<code>fdeint</code>) solvers.</li> <li>Learnable Parameters: Supports learnable membrane time constants (\\(\\tau\\)) via exponential reparameterization and customizable surrogate gradient functions (e.g., arctan, sigmoid) for effective backpropagation through non-differentiable spikes.</li> <li>Extensibility: Provides a clear <code>BaseNeuron</code> interface for defining custom dynamics, ensuring that user-defined neurons automatically inherit fractional capabilities when wrapped in the appropriate solver.</li> </ul>"},{"location":"api/neuron/#spikeDE.neuron.BaseNeuron","title":"BaseNeuron","text":"<pre><code>BaseNeuron(\n    tau: float = 0.5,\n    threshold: float = 1.0,\n    surrogate_grad_scale: float = 5.0,\n    surrogate_opt: str = \"arctan_surrogate\",\n    tau_learnable: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Base class for spiking neuron models with configurable membrane time constant and surrogate gradients.</p> <p>This abstract class provides the foundational structure for spiking neurons. It supports learnable or fixed membrane time constant (\\(\\tau\\)) and customizable surrogate gradient functions for backpropagation through non-differentiable spikes.</p> <p>The effective membrane time constant is computed as:</p> \\[ \\tau = \\begin{cases}     \\tau_0 \\cdot (1 + e^{\\theta}) &amp; \\text{if } \\texttt{tau_learnable=True} \\\\     \\tau_0 &amp; \\text{otherwise} \\end{cases} \\] <p>where \\(\\tau_0\\) is the initial value and \\(\\theta\\) is a learnable parameter.</p> <p>Subclasses must implement the <code>forward</code> method to define specific dynamics.</p> <p>Attributes:</p> <ul> <li> <code>initial_tau</code>               (<code>float</code>)           \u2013            <p>Initial value of the membrane time constant \\(\\tau_0\\).</p> </li> <li> <code>tau_param</code>               (<code>Parameter | None</code>)           \u2013            <p>Learnable parameter \\(\\theta\\) if <code>tau_learnable=True</code>; otherwise <code>None</code>.</p> </li> <li> <code>tau</code>               (<code>float</code>)           \u2013            <p>Fixed \\(\\tau\\) used when <code>tau_learnable=False</code>.</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Firing threshold \\(V_{\\text{th}}\\).</p> </li> <li> <code>surrogate_grad_scale</code>               (<code>float</code>)           \u2013            <p>Scaling factor for surrogate gradient steepness.</p> </li> <li> <code>surrogate_f</code>               (<code>Callable</code>)           \u2013            <p>Surrogate gradient function (e.g., arctan-based).</p> </li> <li> <code>tau_learnable</code>               (<code>bool</code>)           \u2013            <p>Whether \\(\\tau\\) is trainable.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>tau</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The base membrane time constant \\(\\tau\\). Used directly if <code>tau_learnable=False</code>, or as a scaling factor if <code>tau_learnable=True</code>.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>The membrane potential threshold at which the neuron fires a spike.</p> </li> <li> <code>surrogate_grad_scale</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Scaling factor applied inside the surrogate gradient function to control gradient magnitude during backpropagation.</p> </li> <li> <code>surrogate_opt</code>               (<code>str</code>, default:                   <code>'arctan_surrogate'</code> )           \u2013            <p>Name of the surrogate gradient function to use. Must be a key in the global <code>surrogate_f</code> dictionary (e.g., <code>\"arctan_surrogate\"</code>).</p> </li> <li> <code>tau_learnable</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, \\(\\tau\\) becomes a learnable parameter. If <code>False</code>, \\(\\tau\\) remains fixed.</p> </li> </ul> Subclassed by: <ul> <li> API Reference spikeDE.neuron IFNeuron </li> <li> API Reference spikeDE.neuron LIFNeuron </li> </ul> Source code in <code>spikeDE/neuron.py</code> <pre><code>def __init__(\n    self,\n    tau: float = 0.5,\n    threshold: float = 1.0,\n    surrogate_grad_scale: float = 5.0,\n    surrogate_opt: str = \"arctan_surrogate\",\n    tau_learnable: bool = False,\n) -&gt; None:\n    r\"\"\"Initializes the BaseNeuron module.\n\n    Args:\n        tau: The base membrane time constant $\\tau$. Used directly if `tau_learnable=False`,\n            or as a scaling factor if `tau_learnable=True`.\n        threshold: The membrane potential threshold at which the neuron fires a spike.\n        surrogate_grad_scale: Scaling factor applied inside the surrogate gradient function\n            to control gradient magnitude during backpropagation.\n        surrogate_opt: Name of the surrogate gradient function to use.\n            Must be a key in the global `surrogate_f` dictionary (e.g., `\"arctan_surrogate\"`).\n        tau_learnable: If `True`, $\\tau$ becomes a learnable parameter.\n            If `False`, $\\tau$ remains fixed.\n    \"\"\"\n    super(BaseNeuron, self).__init__()\n    self.initial_tau = tau\n\n    if tau_learnable:\n        self.tau_param = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n    else:\n        self.tau_param = None\n        self.tau = tau\n\n    self.threshold = threshold\n    self.surrogate_grad_scale = surrogate_grad_scale\n    self.surrogate_f = surrogate_f[surrogate_opt]\n    self.tau_learnable = tau_learnable\n</code></pre>"},{"location":"api/neuron/#spikeDE.neuron.BaseNeuron.forward","title":"forward","text":"<pre><code>forward(v_mem: Tensor, current_input: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Performs one step of neuron state update.</p> <p>Must be overridden by subclasses to implement specific spiking dynamics.</p> <p>Parameters:</p> <ul> <li> <code>v_mem</code>               (<code>Tensor</code>)           \u2013            <p>Membrane potential tensor of shape <code>(batch_size, ...)</code>.</p> </li> <li> <code>current_input</code>               (<code>Tensor</code>)           \u2013            <p>Input current tensor, same shape as <code>v_mem</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, Tensor]</code>           \u2013            <p>A tuple <code>(dv_dt, spike)</code> where:</p> <ul> <li><code>dv_dt</code>: Effective derivative of membrane potential.</li> <li><code>spike</code>: Continuous spike approximation in [0, 1].</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Always raised here; subclass must implement.</p> </li> </ul> Source code in <code>spikeDE/neuron.py</code> <pre><code>def forward(\n    self, v_mem: torch.Tensor, current_input: torch.Tensor\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Performs one step of neuron state update.\n\n    Must be overridden by subclasses to implement specific spiking dynamics.\n\n    Args:\n        v_mem: Membrane potential tensor of shape `(batch_size, ...)`.\n        current_input: Input current tensor, same shape as `v_mem`.\n\n    Returns:\n        A tuple `(dv_dt, spike)` where:\n\n            - `dv_dt`: Effective derivative of membrane potential.\n            - `spike`: Continuous spike approximation in [0, 1].\n\n    Raises:\n        NotImplementedError: Always raised here; subclass must implement.\n    \"\"\"\n    raise NotImplementedError(\"Neuron forward method must be overridden.\")\n</code></pre>"},{"location":"api/neuron/#spikeDE.neuron.BaseNeuron.get_tau","title":"get_tau","text":"<pre><code>get_tau() -&gt; float\n</code></pre> <p>Returns the effective membrane time constant \\(\\tau\\).</p> <p>Ensures positivity via exponential reparameterization when learnable.</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Scalar tensor representing \\(\\tau\\).</p> </li> </ul> Source code in <code>spikeDE/neuron.py</code> <pre><code>def get_tau(self) -&gt; float:\n    r\"\"\"Returns the effective membrane time constant $\\tau$.\n\n    Ensures positivity via exponential reparameterization when learnable.\n\n    Returns:\n        Scalar tensor representing $\\tau$.\n    \"\"\"\n    if self.tau_learnable:\n        return self.initial_tau * (1 + torch.exp(self.tau_param))\n    else:\n        return self.tau\n</code></pre>"},{"location":"api/neuron/#spikeDE.neuron.IFNeuron","title":"IFNeuron","text":"<pre><code>IFNeuron(\n    tau: float = 0.5,\n    threshold: float = 1.0,\n    surrogate_grad_scale: float = 5.0,\n    surrogate_opt: str = \"arctan_surrogate\",\n    tau_learnable: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseNeuron</code></p> <p>Integrate-and-Fire (IF) spiking neuron model with surrogate gradients.</p> <p>This model integrates input without leakage. The dynamics follow:</p> \\[     \\tau\\frac{\\text{d}v}{\\text{d}t} = I(t), \\quad     \\text{spike} = \\sigma(v - V_{\\text{th}}) \\] <p>where \\(\\sigma\\) is a differentiable surrogate.</p> Note <p>Despite inheriting <code>tau</code>, this model behaves as a pure integrator when leakage is disabled (i.e., no decay term on <code>v_mem</code>).</p> Source code in <code>spikeDE/neuron.py</code> <pre><code>def __init__(\n    self,\n    tau: float = 0.5,\n    threshold: float = 1.0,\n    surrogate_grad_scale: float = 5.0,\n    surrogate_opt: str = \"arctan_surrogate\",\n    tau_learnable: bool = False,\n) -&gt; None:\n    r\"\"\"Initializes the BaseNeuron module.\n\n    Args:\n        tau: The base membrane time constant $\\tau$. Used directly if `tau_learnable=False`,\n            or as a scaling factor if `tau_learnable=True`.\n        threshold: The membrane potential threshold at which the neuron fires a spike.\n        surrogate_grad_scale: Scaling factor applied inside the surrogate gradient function\n            to control gradient magnitude during backpropagation.\n        surrogate_opt: Name of the surrogate gradient function to use.\n            Must be a key in the global `surrogate_f` dictionary (e.g., `\"arctan_surrogate\"`).\n        tau_learnable: If `True`, $\\tau$ becomes a learnable parameter.\n            If `False`, $\\tau$ remains fixed.\n    \"\"\"\n    super(BaseNeuron, self).__init__()\n    self.initial_tau = tau\n\n    if tau_learnable:\n        self.tau_param = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n    else:\n        self.tau_param = None\n        self.tau = tau\n\n    self.threshold = threshold\n    self.surrogate_grad_scale = surrogate_grad_scale\n    self.surrogate_f = surrogate_f[surrogate_opt]\n    self.tau_learnable = tau_learnable\n</code></pre>"},{"location":"api/neuron/#spikeDE.neuron.IFNeuron.forward","title":"forward","text":"<pre><code>forward(\n    v_mem: Tensor, current_input: Tensor | None = None\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Forward pass for IF neuron dynamics (discrete-time, <code>dt=1.0</code>).</p> <p>Parameters:</p> <ul> <li> <code>v_mem</code>               (<code>Tensor</code>)           \u2013            <p>Current membrane potential.</p> </li> <li> <code>current_input</code>               (<code>Tensor | None</code>, default:                   <code>None</code> )           \u2013            <p>Input current (same shape as <code>v_mem</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, Tensor]</code>           \u2013            <p>Tuple <code>(dv_dt, spike)</code> representing effective derivative and spike output.</p> </li> </ul> Source code in <code>spikeDE/neuron.py</code> <pre><code>def forward(\n    self, v_mem: torch.Tensor, current_input: Optional[torch.Tensor] = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Forward pass for IF neuron dynamics (discrete-time, `dt=1.0`).\n\n    Args:\n        v_mem: Current membrane potential.\n        current_input: Input current (same shape as `v_mem`).\n\n    Returns:\n        Tuple `(dv_dt, spike)` representing effective derivative and spike output.\n    \"\"\"\n    if current_input is None:\n        return v_mem\n    tau = self.get_tau()\n    dt = 1.0\n    dv_no_reset = (current_input) / tau\n    v_post_charge = v_mem + dt * dv_no_reset\n    spike = self.surrogate_f(\n        v_post_charge - self.threshold, self.surrogate_grad_scale\n    )\n    dv_dt = dv_no_reset - (spike.detach() * self.threshold) / tau\n    return dv_dt, spike\n</code></pre>"},{"location":"api/neuron/#spikeDE.neuron.LIFNeuron","title":"LIFNeuron","text":"<pre><code>LIFNeuron(\n    tau: float = 0.5,\n    threshold: float = 1.0,\n    surrogate_grad_scale: float = 5.0,\n    surrogate_opt: str = \"arctan_surrogate\",\n    tau_learnable: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseNeuron</code></p> <p>Leaky Integrate-and-Fire (LIF) spiking neuron model with surrogate gradients.</p> <p>Implements classic leaky dynamics governed by:</p> \\[     \\tau \\frac{\\text{d}v}{\\text{d}t} = -v + I(t), \\quad     \\text{spike} = \\sigma(v - V_{\\text{th}}) \\] <p>where \\(\\sigma\\) is a differentiable surrogate.</p> Source code in <code>spikeDE/neuron.py</code> <pre><code>def __init__(\n    self,\n    tau: float = 0.5,\n    threshold: float = 1.0,\n    surrogate_grad_scale: float = 5.0,\n    surrogate_opt: str = \"arctan_surrogate\",\n    tau_learnable: bool = False,\n) -&gt; None:\n    r\"\"\"Initializes the BaseNeuron module.\n\n    Args:\n        tau: The base membrane time constant $\\tau$. Used directly if `tau_learnable=False`,\n            or as a scaling factor if `tau_learnable=True`.\n        threshold: The membrane potential threshold at which the neuron fires a spike.\n        surrogate_grad_scale: Scaling factor applied inside the surrogate gradient function\n            to control gradient magnitude during backpropagation.\n        surrogate_opt: Name of the surrogate gradient function to use.\n            Must be a key in the global `surrogate_f` dictionary (e.g., `\"arctan_surrogate\"`).\n        tau_learnable: If `True`, $\\tau$ becomes a learnable parameter.\n            If `False`, $\\tau$ remains fixed.\n    \"\"\"\n    super(BaseNeuron, self).__init__()\n    self.initial_tau = tau\n\n    if tau_learnable:\n        self.tau_param = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n    else:\n        self.tau_param = None\n        self.tau = tau\n\n    self.threshold = threshold\n    self.surrogate_grad_scale = surrogate_grad_scale\n    self.surrogate_f = surrogate_f[surrogate_opt]\n    self.tau_learnable = tau_learnable\n</code></pre>"},{"location":"api/neuron/#spikeDE.neuron.LIFNeuron.forward","title":"forward","text":"<pre><code>forward(\n    v_mem: Tensor, current_input: Tensor | None = None\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Forward pass for LIF neuron dynamics (discrete-time, <code>dt=1.0</code>).</p> <p>Parameters:</p> <ul> <li> <code>v_mem</code>               (<code>Tensor</code>)           \u2013            <p>Current membrane potential.</p> </li> <li> <code>current_input</code>               (<code>Tensor | None</code>, default:                   <code>None</code> )           \u2013            <p>Input current (same shape as <code>v_mem</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, Tensor]</code>           \u2013            <p>Tuple <code>(dv_dt, spike)</code> representing effective derivative and spike output.</p> </li> </ul> Source code in <code>spikeDE/neuron.py</code> <pre><code>def forward(\n    self, v_mem: torch.Tensor, current_input: Optional[torch.Tensor] = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    r\"\"\"Forward pass for LIF neuron dynamics (discrete-time, `dt=1.0`).\n\n    Args:\n        v_mem: Current membrane potential.\n        current_input: Input current (same shape as `v_mem`).\n\n    Returns:\n        Tuple `(dv_dt, spike)` representing effective derivative and spike output.\n    \"\"\"\n    if current_input is None:\n        return v_mem\n    tau = self.get_tau()\n    dt = 1.0\n    dv_no_reset = (-v_mem + current_input) / tau\n    v_post_charge = v_mem + dt * dv_no_reset\n    spike = self.surrogate_f(\n        v_post_charge - self.threshold, self.surrogate_grad_scale\n    )\n    dv_dt = dv_no_reset - (spike.detach() * self.threshold) / tau\n    return dv_dt, spike\n</code></pre>"},{"location":"api/odefunc/","title":"SpikeDE.odefunc","text":"<p>This module serves as the core engine for Spiking Neural ODEs within the SpikeDE framework. It provides the <code>ODEFuncFromFX</code> class, which automatically transforms standard Spiking Neural Networks (SNNs) into continuous-time neural ODE systems suitable for integration with adaptive solvers (e.g., <code>torchdiffeq</code>).</p>"},{"location":"api/odefunc/#key-features","title":"Key Features","text":"<ul> <li>Automatic Graph Transformation: Leverages PyTorch FX to symbolically trace SNN backbones, separating the continuous dynamics (membrane potential evolution) from discrete post-processing layers (e.g., voting or classification).</li> <li>Continuous Input Reconstruction: Supports high-precision reconstruction of discrete input spike trains at arbitrary time steps \\(t\\) during integration.</li> <li>Pure PyTorch Interpolation: Implements four interpolation strategies entirely in PyTorch to ensure seamless GPU acceleration without CPU-GPU data transfer overhead:<ul> <li><code>linear</code>: Standard linear interpolation.</li> <li><code>nearest</code>: Zero-order hold (nearest neighbor).</li> <li><code>cubic</code>: Catmull-Rom cubic splines for smooth trajectories.</li> <li><code>akima</code>: Akima splines for robustness against oscillations.</li> </ul> </li> <li>Solver Compatibility: The generated vector field functions are fully compatible with standard ODE solvers, enabling efficient backpropagation through the integration process.</li> </ul>"},{"location":"api/odefunc/#spikeDE.odefunc.ODEFuncFromFX","title":"ODEFuncFromFX","text":"<pre><code>ODEFuncFromFX(backbone: Module, interpolation_method: str = 'linear')\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Wrapper that converts a Spiking Neural Network (SNN) into an ODE-compatible form.</p> <p>This class leverages PyTorch FX to symbolically trace the input <code>backbone</code> SNN and restructure its computation graph into two distinct parts:</p> <ol> <li>ODE Graph (<code>ode_gm</code>): Contains all operations up to and including the last    spiking neuron layer. It outputs:</li> <li>The time derivatives of all neuronal membrane potentials (\\(dv/dt\\)).</li> <li> <p>Boundary values (spikes or intermediate tensors) required by downstream layers.</p> </li> <li> <p>Post-Neuron Module (<code>post_neuron_module</code>): Contains all operations occurring    after the last neuron (e.g., voting layers, classifiers). This module is decoupled    from the ODE integration loop and applied only after solving the ODE system.</p> </li> </ol> <p>Input signals are assumed to be sampled at discrete time points. To support continuous-time ODE solvers, inputs are interpolated on-the-fly using the specified <code>interpolation_method</code>.</p> <p>The resulting object can be passed directly to numerical ODE solvers (e.g., <code>torchdiffeq</code>) as the vector field function \\(f(t, v) = \\text{d}v/\\text{d}t\\).</p> <p>Attributes:</p> <ul> <li> <code>interpolation_method</code>               (<code>str</code>)           \u2013            <p>Interpolation scheme for continuous input reconstruction.</p> </li> <li> <code>neuron_count</code>               (<code>int</code>)           \u2013            <p>Number of <code>BaseNeuron</code> instances detected in the backbone.</p> </li> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Cached input tensor (shape: <code>(T, ...)</code>).</p> </li> <li> <code>x_time</code>               (<code>Tensor</code>)           \u2013            <p>Time stamps corresponding to input samples (shape: <code>(T,)</code>).</p> </li> <li> <code>nfe</code>               (<code>int</code>)           \u2013            <p>Number of function evaluations performed (useful for profiling solver cost).</p> </li> <li> <code>ode_gm</code>               (<code>GraphModule</code>)           \u2013            <p>The ODE-compatible computation graph.</p> </li> <li> <code>post_neuron_module</code>               (<code>Module</code>)           \u2013            <p>Module containing post-neuron operations.</p> </li> <li> <code>traced</code>               (<code>GraphModule</code>)           \u2013            <p>The original traced backbone for reference.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>backbone</code>               (<code>Module</code>)           \u2013            <p>The original SNN model containing <code>BaseNeuron</code> layers. Must be FX-traceable and contain at least one neuron layer.</p> </li> <li> <code>interpolation_method</code>               (<code>str</code>, default:                   <code>'linear'</code> )           \u2013            <p>Method used to interpolate discrete inputs to continuous time. Supported options:</p> <ul> <li><code>'linear'</code>: Linear interpolation between adjacent samples.</li> <li><code>'nearest'</code>: Hold last value (zero-order hold).</li> <li><code>'cubic'</code>: Catmull-Rom cubic spline interpolation.</li> <li><code>'akima'</code>: Akima spline interpolation (reduces overshoot).</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unsupported node operation is encountered during graph rewriting.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def __init__(\n    self, backbone: nn.Module, interpolation_method: str = \"linear\"\n) -&gt; None:\n    r\"\"\"\n    Initializes the ODE-compatible wrapper from a spiking neural network.\n\n    Args:\n        backbone (nn.Module): The original SNN model containing `BaseNeuron` layers.\n            Must be FX-traceable and contain at least one neuron layer.\n        interpolation_method (str): Method used to interpolate discrete inputs to continuous time.\n            Supported options:\n\n            - `'linear'`: Linear interpolation between adjacent samples.\n            - `'nearest'`: Hold last value (zero-order hold).\n            - `'cubic'`: Catmull-Rom cubic spline interpolation.\n            - `'akima'`: Akima spline interpolation (reduces overshoot).\n\n    Raises:\n        ValueError: If an unsupported node operation is encountered during graph rewriting.\n    \"\"\"\n    super().__init__()\n    self.interpolation_method = interpolation_method\n    self.neuron_count = 0\n    self.x = None\n    self.x_time = None\n    self.nfe = 0\n\n    # Step 1: Symbolically trace\n    traced: fx.GraphModule = symbolic_trace_leaf_neurons(backbone)\n    modules = dict(traced.named_modules())\n\n    # Step 2: Create New ODE-Compatible Graph\n    new_graph = fx.Graph()\n    node_map: Dict[fx.Node, fx.Node] = {}  # Old Node -&gt; New Node\n\n    # Create ODE specific inputs\n    t_node = new_graph.placeholder(\"t\")\n    v_mems_node = new_graph.placeholder(\"v_mems\")\n    x_node = new_graph.placeholder(\"x\")\n    x_time_node = new_graph.placeholder(\"x_time\")\n\n    # Step 3: Input Interpolation\n    # Robustly find the first placeholder for input mapping, ignoring name 'x'\n    first_placeholder = None\n    for node in traced.graph.nodes:\n        if node.op == \"placeholder\":\n            first_placeholder = node\n            break\n\n    current_input = new_graph.call_function(\n        interpolate,  # Ensure this function is in scope or imported\n        args=(x_node, x_time_node, t_node, self.interpolation_method),\n    )\n\n    if first_placeholder:\n        node_map[first_placeholder] = current_input\n\n    # Initialize variables\n    current_output = current_input\n    dv_dt_list = []\n    neuron_index = 0\n\n    # Step 4: Find Last Neuron (Pre-scan)\n    last_neuron_node = None\n    neuron_nodes = []\n    for node in traced.graph.nodes:\n        if node.op == \"call_module\" and isinstance(\n            modules[node.target], BaseNeuron\n        ):\n            last_neuron_node = node\n            neuron_nodes.append(node)\n            # (Optional) Save threshold/surrogate here if needed\n\n    # Step 5: FIRST PASS - Build ODE Graph Content\n    # We need to distinguish between nodes that MUST be in ODE (up to last neuron)\n    # and nodes that MIGHT be in ODE (dependencies of post-neuron nodes)\n\n    nodes_in_ode_graph = []  # To track topological order in new graph\n\n    for node in traced.graph.nodes:\n        # Skip the original placeholder as we handled it manually\n        if node.op == \"placeholder\":\n            continue\n\n        if node.op == \"call_module\":\n            submodule = modules[node.target]\n            if isinstance(submodule, BaseNeuron):\n                # --- NEURON LOGIC ---\n                v_mem_current = new_graph.call_function(\n                    operator.getitem, args=(v_mems_node, neuron_index)\n                )\n\n                # Assume BaseNeuron.forward returns (dv/dt, spike) in this context\n                mapped_input = self._map_arguments(node.args, node_map)\n                out_node = new_graph.call_module(\n                    node.target,\n                    args=(\n                        v_mem_current,\n                        (\n                            mapped_input[0]\n                            if isinstance(mapped_input, tuple)\n                            else mapped_input\n                        ),\n                    ),\n                )\n\n                dv_dt = new_graph.call_function(\n                    operator.getitem, args=(out_node, 0)\n                )\n                spike_output = new_graph.call_function(\n                    operator.getitem, args=(out_node, 1)\n                )\n\n                dv_dt_list.append(dv_dt)\n\n                # For the graph flow, the neuron output is the spike\n                node_map[node] = spike_output\n                nodes_in_ode_graph.append(node)\n                neuron_index += 1\n                continue\n\n        # --- STANDARD NODE LOGIC ---\n        # Map arguments using the node_map\n        mapped_args = self._map_arguments(node.args, node_map)\n        mapped_kwargs = self._map_arguments(node.kwargs, node_map)\n\n        # Recreate node in new graph\n        if node.op == \"call_module\":\n            new_node = new_graph.call_module(\n                node.target, args=mapped_args, kwargs=mapped_kwargs\n            )\n        elif node.op == \"call_function\":\n            new_node = new_graph.call_function(\n                node.target, args=mapped_args, kwargs=mapped_kwargs\n            )\n        elif node.op == \"call_method\":\n            new_node = new_graph.call_method(\n                node.target, args=mapped_args, kwargs=mapped_kwargs\n            )\n        elif node.op == \"get_attr\":\n            new_node = new_graph.get_attr(node.target)\n            node_map[node] = new_node\n            # Don't add to nodes_in_ode_graph - it's not part of main data flow\n            continue\n        elif node.op == \"output\":\n            continue\n        else:\n            # Should never happen, but just in case\n            raise ValueError(f\"Unexpected node op: {node.op}\")\n\n        node_map[node] = new_node\n        nodes_in_ode_graph.append(node)\n\n    # Step 6: INTELLIGENT CUT LOGIC\n    # We need to determine which nodes act as the \"Boundary\" between ODE and Post-Process.\n    # Ideally, everything after `last_neuron_node` should be post-process.\n    # However, if a post-process node depends on a node `N` inside the ODE graph,\n    # `N` must be an output of the ODE graph.\n\n    post_neuron_nodes = []\n    ode_graph_outputs = {}  # Map original_node -&gt; new_graph_node to be outputted\n\n    # Identify nodes strictly after the last neuron\n    start_collecting = False\n    for node in traced.graph.nodes:\n        if start_collecting and node.op != \"output\":\n            post_neuron_nodes.append(node)\n        if node == last_neuron_node:\n            start_collecting = True\n\n    # Check dependencies of post-neuron nodes\n    # If a post-node depends on a node NOT in `post_neuron_nodes`, that dependency is a boundary.\n    boundary_nodes = set()\n\n    # Check dependencies of post-neuron nodes\n    post_neuron_set = set(post_neuron_nodes)\n    for p_node in post_neuron_nodes:\n\n        def register_boundary(arg):\n            if isinstance(arg, fx.Node):\n                if arg not in post_neuron_set and arg.op != \"placeholder\":\n                    boundary_nodes.add(arg)\n\n        fx.map_arg(p_node.args, register_boundary)\n        fx.map_arg(p_node.kwargs, register_boundary)\n\n    # If no last neuron found, or no post nodes, boundary is just the last non-output node\n    # ============================================================\n    if post_neuron_nodes and not boundary_nodes:\n        # Post-neuron nodes exist but don't depend on anything from ODE graph\n        # This is unusual, but we should still return the last neuron's spike\n        if last_neuron_node:\n            boundary_nodes.add(last_neuron_node)\n\n    if not post_neuron_nodes:\n        # No post-neuron nodes at all - ODE graph IS the whole network\n        # Return the last meaningful output\n        if last_neuron_node:\n            boundary_nodes.add(last_neuron_node)\n\n    # Step 7: Finalize ODE Graph Output\n    # The output is: (*dv_dt_list, boundary_val_1, boundary_val_2, ...)\n\n    # Create order mapping from ORIGINAL traced graph (always complete and correct)\n    node_order = {node: i for i, node in enumerate(traced.graph.nodes)}\n    sorted_boundary_nodes = sorted(\n        list(boundary_nodes), key=lambda n: node_order.get(n, float(\"inf\"))\n    )\n\n    boundary_values = [node_map[n] for n in sorted_boundary_nodes]\n    output_tuple = tuple(dv_dt_list) + tuple(boundary_values)\n    new_graph.output(output_tuple)\n\n    print(\"=\" * 50)\n    print(\"ODE Graph:\")\n    print(new_graph)\n\n    self.ode_gm = fx.GraphModule(traced, new_graph)\n    self.ode_gm.graph.eliminate_dead_code()  # This will clean up unused nodes in ODE graph!\n    self.ode_gm.recompile()  # Add this line\n\n    # Step 8: Build Post-Neuron Graph\n    self.boundary_map = {}  # To remember which index corresponds to which node\n\n    if post_neuron_nodes:\n        post_graph = fx.Graph()\n        post_node_map = {}\n\n        # Create placeholders for boundary inputs\n        # Input to post-net is the tuple of boundary values returned by ODE\n        # But usually we wrap this. Let's assume input is unpacked or we use index.\n        # Strategy: The post-module input will be the tuple of boundary values.\n\n        if len(sorted_boundary_nodes) == 1:\n            # SINGLE BOUNDARY: Use direct input (preserves original interface)\n            post_input = post_graph.placeholder(\"x\")\n            post_node_map[sorted_boundary_nodes[0]] = post_input\n        else:\n            # MULTIPLE BOUNDARIES: Use tuple unpacking\n            post_input_tuple = post_graph.placeholder(\"boundary_inputs\")\n            for idx, orig_node in enumerate(sorted_boundary_nodes):\n                val = post_graph.call_function(\n                    operator.getitem, args=(post_input_tuple, idx)\n                )\n                post_node_map[orig_node] = val\n\n        # Recreate post nodes\n        current_post_out = None\n        for node in post_neuron_nodes:\n            mapped_args = self._map_arguments(node.args, post_node_map)\n            mapped_kwargs = self._map_arguments(node.kwargs, post_node_map)\n\n            if node.op == \"call_module\":\n                new_node = post_graph.call_module(\n                    node.target, args=mapped_args, kwargs=mapped_kwargs\n                )\n            elif node.op == \"call_function\":\n                new_node = post_graph.call_function(\n                    node.target, args=mapped_args, kwargs=mapped_kwargs\n                )\n            elif node.op == \"call_method\":\n                new_node = post_graph.call_method(\n                    node.target, args=mapped_args, kwargs=mapped_kwargs\n                )\n            elif node.op == \"get_attr\":\n                new_node = post_graph.get_attr(node.target)\n                post_node_map[node] = new_node\n                continue  # \u2190 Don't update current_post_out\n            else:\n                raise ValueError(\n                    f\"Unexpected node op in post_neuron_nodes: {node.op}\"\n                )\n\n            post_node_map[node] = new_node\n            current_post_out = new_node\n\n        post_graph.output(current_post_out)\n        self.post_neuron_module = fx.GraphModule(traced, post_graph)\n        print(\"-\" * 50)\n        print(\"Post-Neuron Graph:\")\n        print(post_graph)\n\n    else:\n        # Identity or specialized handler if purely ODE\n        self.post_neuron_module = nn.Identity()\n        print(\"-\" * 50)\n        print(\"No post-neuron operations found, set nn.Identity()\")\n\n    self.neuron_count = neuron_index\n    self.traced = traced\n    self.nfe = 0\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.ODEFuncFromFX.forward","title":"forward","text":"<pre><code>forward(t: float, v_mems: tuple) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Computes the vector field \\(f(t, v) = \\text{d}v/\\text{d}t\\) for ODE solvers.</p> <p>This method is called repeatedly by the ODE integrator. It evaluates the ODE graph at time <code>t</code> using the current membrane potentials <code>v_mems</code> and interpolated input.</p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>float</code>)           \u2013            <p>Current time (scalar).</p> </li> <li> <code>v_mems</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Tuple of membrane potential tensors, one per neuron layer, each of shape <code>(batch_size, ...)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Tuple[torch.Tensor, ...]: A tuple containing: - <code>dv_dt_i</code>: Time derivative of membrane potential for the i-th neuron. - <code>boundary_val_j</code>: Intermediate values needed by the post-neuron module,   in topological order.</p> </li> <li> <code>...</code>           \u2013            <p>Total length is <code>neuron_count + num_boundary_values</code>.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def forward(self, t: float, v_mems: Tuple) -&gt; Tuple[torch.Tensor, ...]:\n    r\"\"\"\n    Computes the vector field $f(t, v) = \\text{d}v/\\text{d}t$ for ODE solvers.\n\n    This method is called repeatedly by the ODE integrator. It evaluates the ODE graph\n    at time `t` using the current membrane potentials `v_mems` and interpolated input.\n\n    Args:\n        t (float): Current time (scalar).\n        v_mems (Tuple[torch.Tensor, ...]): Tuple of membrane potential tensors,\n            one per neuron layer, each of shape `(batch_size, ...)`.\n\n    Returns:\n        Tuple[torch.Tensor, ...]: A tuple containing:\n            - `dv_dt_i`: Time derivative of membrane potential for the *i*-th neuron.\n            - `boundary_val_j`: Intermediate values needed by the post-neuron module,\n              in topological order.\n\n        Total length is `neuron_count + num_boundary_values`.\n    \"\"\"\n    x = self.x\n    x_time = self.x_time\n    self.nfe += 1\n\n    return self.ode_gm(t, v_mems, x, x_time)\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.ODEFuncFromFX.get_ode_module","title":"get_ode_module","text":"<pre><code>get_ode_module() -&gt; GraphModule\n</code></pre> <p>Returns the FX graph module implementing the ODE vector field.</p> <p>This module encapsulates the entire ODE-compatible computation graph and can be inspected, saved, or modified independently.</p> <p>Returns:</p> <ul> <li> <code>GraphModule</code>           \u2013            <p>fx.GraphModule: The internal ODE evaluation graph.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def get_ode_module(self) -&gt; fx.GraphModule:\n    r\"\"\"\n    Returns the FX graph module implementing the ODE vector field.\n\n    This module encapsulates the entire ODE-compatible computation graph and can be\n    inspected, saved, or modified independently.\n\n    Returns:\n        fx.GraphModule: The internal ODE evaluation graph.\n    \"\"\"\n    return self.ode_gm\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.ODEFuncFromFX.get_post_neuron_module","title":"get_post_neuron_module","text":"<pre><code>get_post_neuron_module() -&gt; Module\n</code></pre> <p>Returns the module that processes outputs after the last spiking neuron.</p> <p>This module should be applied to the boundary values returned by the ODE solver to produce the final network prediction.</p> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: The post-neuron computation path.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def get_post_neuron_module(self) -&gt; nn.Module:\n    r\"\"\"\n    Returns the module that processes outputs after the last spiking neuron.\n\n    This module should be applied to the boundary values returned by the ODE solver\n    to produce the final network prediction.\n\n    Returns:\n        nn.Module: The post-neuron computation path.\n    \"\"\"\n    return self.post_neuron_module\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.ODEFuncFromFX.set_inputs","title":"set_inputs","text":"<pre><code>set_inputs(x: Tensor, x_time: Tensor) -&gt; None\n</code></pre> <p>Caches the input signal and its sampling timestamps for interpolation.</p> <p>These inputs are used during the ODE solve to reconstruct \\(x(t)\\) at arbitrary times.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>(T, batch_size, ...)</code> where <code>T</code> is the number of time steps.</p> </li> <li> <code>x_time</code>               (<code>Tensor</code>)           \u2013            <p>Corresponding time stamps of shape <code>(T,)</code> or <code>(batch_size, T)</code>, typically monotonically increasing.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def set_inputs(self, x: torch.Tensor, x_time: torch.Tensor) -&gt; None:\n    r\"\"\"\n    Caches the input signal and its sampling timestamps for interpolation.\n\n    These inputs are used during the ODE solve to reconstruct $x(t)$ at arbitrary times.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape `(T, batch_size, ...)` where `T`\n            is the number of time steps.\n        x_time (torch.Tensor): Corresponding time stamps of shape `(T,)` or\n            `(batch_size, T)`, typically monotonically increasing.\n    \"\"\"\n    self.x = x\n    self.x_time = x_time\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.SNNLeafTracer","title":"SNNLeafTracer","text":"<p>               Bases: <code>Tracer</code></p> <p>Custom FX Tracer that treats specific modules as leaf nodes.</p> <p>This tracer ensures that <code>BaseNeuron</code>, <code>VotingLayer</code>, and <code>ClassificationHead</code> modules are not decomposed during symbolic tracing, preserving their internal logic as single graph nodes.</p>"},{"location":"api/odefunc/#spikeDE.odefunc.SNNLeafTracer.is_leaf_module","title":"is_leaf_module","text":"<pre><code>is_leaf_module(m: Module, module_qualified_name: str) -&gt; bool\n</code></pre> <p>Determine if a module should be treated as a leaf node.</p> <p>Parameters:</p> <ul> <li> <code>m</code>               (<code>Module</code>)           \u2013            <p>The module instance being traced.</p> </li> <li> <code>module_qualified_name</code>               (<code>str</code>)           \u2013            <p>The qualified name of the module.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the module is a leaf node (should not be traced internally).</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def is_leaf_module(self, m: nn.Module, module_qualified_name: str) -&gt; bool:\n    \"\"\"\n    Determine if a module should be treated as a leaf node.\n\n    Args:\n        m (nn.Module): The module instance being traced.\n        module_qualified_name (str): The qualified name of the module.\n\n    Returns:\n        bool: True if the module is a leaf node (should not be traced internally).\n    \"\"\"\n    if isinstance(m, BaseNeuron):\n        return True\n    if isinstance(m, VotingLayer):\n        return True\n    if isinstance(m, ClassificationHead):\n        return True\n    return super().is_leaf_module(m, module_qualified_name)\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.linear_interpolate_batched","title":"linear_interpolate_batched","text":"<pre><code>linear_interpolate_batched(x: Tensor, x_time: Tensor, t: Tensor) -&gt; Tensor\n</code></pre> <p>Perform batched linear interpolation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor <code>[T, B, ...]</code>.</p> </li> <li> <code>x_time</code>               (<code>Tensor</code>)           \u2013            <p>Time points <code>[B, T]</code>.</p> </li> <li> <code>t</code>               (<code>Tensor</code>)           \u2013            <p>Query times <code>[B]</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: Interpolated values <code>[B, ...]</code>.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def linear_interpolate_batched(\n    x: torch.Tensor, x_time: torch.Tensor, t: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Perform batched linear interpolation.\n\n    Args:\n        x (torch.Tensor): Input tensor `[T, B, ...]`.\n        x_time (torch.Tensor): Time points `[B, T]`.\n        t (torch.Tensor): Query times `[B]`.\n\n    Returns:\n        torch.Tensor: Interpolated values `[B, ...]`.\n    \"\"\"\n    B, T = x_time.shape\n    batch_idx = torch.arange(B, device=x.device)\n\n    idx = torch.searchsorted(x_time, t.unsqueeze(1)).squeeze(1)\n    idx = (idx - 1).clamp(0, T - 2)\n\n    t0 = x_time[batch_idx, idx]\n    t1 = x_time[batch_idx, idx + 1]\n    x0 = x[idx, batch_idx]\n    x1 = x[idx + 1, batch_idx]\n\n    ratio = _expand_to((t - t0) / (t1 - t0 + 1e-10), x0)\n    return x0 + ratio * (x1 - x0)\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.nearest_interpolate_batched","title":"nearest_interpolate_batched","text":"<pre><code>nearest_interpolate_batched(x: Tensor, x_time: Tensor, t: Tensor) -&gt; Tensor\n</code></pre> <p>Perform batched nearest neighbor interpolation.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor <code>[T, B, ...]</code>.</p> </li> <li> <code>x_time</code>               (<code>Tensor</code>)           \u2013            <p>Time points <code>[B, T]</code>.</p> </li> <li> <code>t</code>               (<code>Tensor</code>)           \u2013            <p>Query times <code>[B]</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: Values at nearest time points <code>[B, ...]</code>.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def nearest_interpolate_batched(\n    x: torch.Tensor, x_time: torch.Tensor, t: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Perform batched nearest neighbor interpolation.\n\n    Args:\n        x (torch.Tensor): Input tensor `[T, B, ...]`.\n        x_time (torch.Tensor): Time points `[B, T]`.\n        t (torch.Tensor): Query times `[B]`.\n\n    Returns:\n        torch.Tensor: Values at nearest time points `[B, ...]`.\n    \"\"\"\n    B = x_time.shape[0]\n    batch_idx = torch.arange(B, device=x.device)\n\n    idx = torch.abs(x_time - t.unsqueeze(1)).argmin(dim=1)\n    return x[idx, batch_idx]\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.cubic_interpolate_batched","title":"cubic_interpolate_batched","text":"<pre><code>cubic_interpolate_batched(x: Tensor, x_time: Tensor, t: Tensor) -&gt; Tensor\n</code></pre> <p>Perform batched cubic (Catmull-Rom) interpolation.</p> <p>Requires at least 4 time points (T &gt;= 4).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor <code>[T, B, ...]</code>.</p> </li> <li> <code>x_time</code>               (<code>Tensor</code>)           \u2013            <p>Time points <code>[B, T]</code>.</p> </li> <li> <code>t</code>               (<code>Tensor</code>)           \u2013            <p>Query times <code>[B]</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: Interpolated values <code>[B, ...]</code>.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def cubic_interpolate_batched(\n    x: torch.Tensor, x_time: torch.Tensor, t: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Perform batched cubic (Catmull-Rom) interpolation.\n\n    Requires at least 4 time points (T &gt;= 4).\n\n    Args:\n        x (torch.Tensor): Input tensor `[T, B, ...]`.\n        x_time (torch.Tensor): Time points `[B, T]`.\n        t (torch.Tensor): Query times `[B]`.\n\n    Returns:\n        torch.Tensor: Interpolated values `[B, ...]`.\n    \"\"\"\n    B, T = x_time.shape\n    batch_idx = torch.arange(B, device=x.device)\n\n    idx = torch.searchsorted(x_time, t.unsqueeze(1)).squeeze(1)\n    idx = (idx - 1).clamp(0, T - 2)\n\n    i1 = idx\n    i0 = (i1 - 1).clamp(0, T - 1)\n    i2 = (i1 + 1).clamp(0, T - 1)\n    i3 = (i1 + 2).clamp(0, T - 1)\n\n    p0, p1, p2, p3 = (\n        x[i0, batch_idx],\n        x[i1, batch_idx],\n        x[i2, batch_idx],\n        x[i3, batch_idx],\n    )\n    t1, t2 = x_time[batch_idx, i1], x_time[batch_idx, i2]\n\n    dt = torch.where(t2 == t1, torch.ones_like(t2), t2 - t1)\n    u = _expand_to(((t - t1) / dt).clamp(0, 1), p0)\n\n    u2, u3 = u * u, u * u * u\n    return (\n        (-0.5 * p0 + 1.5 * p1 - 1.5 * p2 + 0.5 * p3) * u3\n        + (p0 - 2.5 * p1 + 2.0 * p2 - 0.5 * p3) * u2\n        + (-0.5 * p0 + 0.5 * p2) * u\n        + p1\n    )\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.akima_interpolate_batched","title":"akima_interpolate_batched","text":"<pre><code>akima_interpolate_batched(x: Tensor, x_time: Tensor, t: Tensor) -&gt; Tensor\n</code></pre> <p>Perform batched Akima interpolation.</p> <p>Akima interpolation uses local slopes to reduce oscillations common in cubic splines. Requires at least 5 time points (T &gt;= 5).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor <code>[T, B, ...]</code>.</p> </li> <li> <code>x_time</code>               (<code>Tensor</code>)           \u2013            <p>Time points <code>[B, T]</code>.</p> </li> <li> <code>t</code>               (<code>Tensor</code>)           \u2013            <p>Query times <code>[B]</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: Interpolated values <code>[B, ...]</code>.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def akima_interpolate_batched(\n    x: torch.Tensor, x_time: torch.Tensor, t: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Perform batched Akima interpolation.\n\n    Akima interpolation uses local slopes to reduce oscillations common in cubic splines.\n    Requires at least 5 time points (T &gt;= 5).\n\n    Args:\n        x (torch.Tensor): Input tensor `[T, B, ...]`.\n        x_time (torch.Tensor): Time points `[B, T]`.\n        t (torch.Tensor): Query times `[B]`.\n\n    Returns:\n        torch.Tensor: Interpolated values `[B, ...]`.\n    \"\"\"\n    B, T = x_time.shape\n    batch_idx = torch.arange(B, device=x.device)\n\n    # FIX: Corrected clamp range to allow interpolation in boundary intervals\n    idx = torch.searchsorted(x_time, t.unsqueeze(1)).squeeze(1)\n    idx = (idx - 1).clamp(0, T - 2)\n\n    # Clamping indices handles the \"ghost points\" by duplicating boundary values\n    i0 = (idx - 2).clamp(0, T - 1)\n    i1 = (idx - 1).clamp(0, T - 1)\n    i2 = idx\n    i3 = (idx + 1).clamp(0, T - 1)\n    i4 = (idx + 2).clamp(0, T - 1)\n\n    x0, x1, x2, x3, x4 = (\n        x[i0, batch_idx],\n        x[i1, batch_idx],\n        x[i2, batch_idx],\n        x[i3, batch_idx],\n        x[i4, batch_idx],\n    )\n    t0, t1, t2, t3, t4 = (\n        x_time[batch_idx, i0],\n        x_time[batch_idx, i1],\n        x_time[batch_idx, i2],\n        x_time[batch_idx, i3],\n        x_time[batch_idx, i4],\n    )\n\n    def safe_slope(xa, xb, ta, tb):\n        dt = torch.where(tb == ta, torch.full_like(tb, 1e-10), tb - ta)\n        return (xb - xa) / _expand_to(dt, xa)\n\n    m0, m1, m2, m3 = (\n        safe_slope(x0, x1, t0, t1),\n        safe_slope(x1, x2, t1, t2),\n        safe_slope(x2, x3, t2, t3),\n        safe_slope(x3, x4, t3, t4),\n    )\n\n    dm0, dm1, dm2 = torch.abs(m1 - m0), torch.abs(m2 - m1), torch.abs(m3 - m2)\n    eps = 1e-10\n\n    denom_left = dm1 + dm0 + eps\n    s1 = torch.where(denom_left &gt; eps, (dm1 * m0 + dm0 * m1) / denom_left, m1)\n\n    denom_right = dm1 + dm2 + eps\n    s2 = torch.where(denom_right &gt; eps, (dm1 * m2 + dm2 * m1) / denom_right, m2)\n\n    h = torch.where(t3 == t2, torch.ones_like(t3), t3 - t2)\n    # Ensure u is calculated relative to the correct interval start (t2)\n    u = _expand_to(((t - t2) / h).clamp(0, 1), x2)\n    h = _expand_to(h, x2)\n\n    a = x2\n    b = s1 * h\n    c = 3 * (x3 - x2) - h * (2 * s1 + s2)\n    d = 2 * (x2 - x3) + h * (s1 + s2)\n\n    u2, u3 = u * u, u * u * u\n    return a + b * u + c * u2 + d * u3\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.interpolate","title":"interpolate","text":"<pre><code>interpolate(\n    x: Tensor, x_time: Tensor, t: float | Tensor, method: str = \"linear\"\n) -&gt; Tensor\n</code></pre> <p>Interpolate batched input data at arbitrary query time(s) <code>t</code>.</p> <p>This function reconstructs continuous-time input signals from discrete samples to support numerical ODE solvers that evaluate the vector field at non-integer time steps.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape <code>[T, B, ...]</code>, where <code>T</code> is the number of time points and <code>B</code> is the batch size.</p> </li> <li> <code>x_time</code>               (<code>Tensor</code>)           \u2013            <p>Time points tensor. Can be: - <code>[T]</code>: Shared timestamps for all batches. - <code>[B, T]</code> or <code>[T, B]</code>: Batch-specific timestamps.</p> </li> <li> <code>t</code>               (<code>float | Tensor</code>)           \u2013            <p>Query time(s). Can be: - <code>float</code> or <code>scalar tensor</code>: Same time for all batches. - <code>[B]</code> tensor: Different time per batch.</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>'linear'</code> )           \u2013            <p>Interpolation algorithm. Options: - <code>'linear'</code>: Linear interpolation (default). - <code>'nearest'</code>: Nearest neighbor. - <code>'cubic'</code>: Catmull-Rom cubic spline (requires T &gt;= 4). - <code>'akima'</code>: Akima spline, robust against oscillations (requires T &gt;= 5).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>torch.Tensor: Interpolated tensor of shape <code>[B, ...]</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unsupported interpolation method is specified.</p> </li> <li> <code>AssertionError</code>             \u2013            <p>If input shapes are inconsistent.</p> </li> </ul> Note <p>Values outside the time range <code>[min(x_time), max(x_time)]</code> are clamped to the boundary values.</p> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def interpolate(\n    x: torch.Tensor,\n    x_time: torch.Tensor,\n    t: Union[float, torch.Tensor],\n    method: str = \"linear\",\n) -&gt; torch.Tensor:\n    \"\"\"\n    Interpolate batched input data at arbitrary query time(s) `t`.\n\n    This function reconstructs continuous-time input signals from discrete samples\n    to support numerical ODE solvers that evaluate the vector field at non-integer time steps.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape `[T, B, ...]`, where `T` is the number\n            of time points and `B` is the batch size.\n        x_time (torch.Tensor): Time points tensor. Can be:\n            - `[T]`: Shared timestamps for all batches.\n            - `[B, T]` or `[T, B]`: Batch-specific timestamps.\n        t (Union[float, torch.Tensor]): Query time(s). Can be:\n            - `float` or `scalar tensor`: Same time for all batches.\n            - `[B]` tensor: Different time per batch.\n        method (str): Interpolation algorithm. Options:\n            - `'linear'`: Linear interpolation (default).\n            - `'nearest'`: Nearest neighbor.\n            - `'cubic'`: Catmull-Rom cubic spline (requires T &gt;= 4).\n            - `'akima'`: Akima spline, robust against oscillations (requires T &gt;= 5).\n\n    Returns:\n        torch.Tensor: Interpolated tensor of shape `[B, ...]`.\n\n    Raises:\n        ValueError: If an unsupported interpolation method is specified.\n        AssertionError: If input shapes are inconsistent.\n\n    Note:\n        Values outside the time range `[min(x_time), max(x_time)]` are clamped\n        to the boundary values.\n    \"\"\"\n    x, x_time, t, B, T = _prepare_inputs(x, x_time, t)\n\n    t_min, t_max = x_time[:, 0], x_time[:, -1]\n    at_min, at_max = t &lt;= t_min, t &gt;= t_max\n    in_range = ~at_min &amp; ~at_max\n\n    result = torch.zeros((B,) + x.shape[2:], dtype=x.dtype, device=x.device)\n\n    if at_min.any():\n        result[at_min] = x[0, at_min]\n    if at_max.any():\n        result[at_max] = x[-1, at_max]\n\n    if in_range.any():\n        methods = {\n            \"linear\": linear_interpolate_batched,\n            \"nearest\": nearest_interpolate_batched,\n            \"cubic\": cubic_interpolate_batched,\n            \"akima\": akima_interpolate_batched,\n        }\n        if method not in methods:\n            raise ValueError(\n                f\"Unknown method: {method}. Choose from {list(methods.keys())}\"\n            )\n        result[in_range] = methods[method](\n            x[:, in_range], x_time[in_range], t[in_range]\n        )\n\n    return result\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.remove_dead_code","title":"remove_dead_code","text":"<pre><code>remove_dead_code(m: Module) -&gt; Module\n</code></pre> <p>Remove dead code from a traced FX graph.</p> <p>Parameters:</p> <ul> <li> <code>m</code>               (<code>Module</code>)           \u2013            <p>A traced GraphModule.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>nn.Module: A new GraphModule with unused nodes eliminated.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def remove_dead_code(m: nn.Module) -&gt; nn.Module:\n    \"\"\"\n    Remove dead code from a traced FX graph.\n\n    Args:\n        m (nn.Module): A traced GraphModule.\n\n    Returns:\n        nn.Module: A new GraphModule with unused nodes eliminated.\n    \"\"\"\n    graph = fx.Tracer().trace(m)\n\n    # \u81ea\u52a8\u5220\u9664\u6ca1\u6709\u88ab\u4f7f\u7528\u7684\u8282\u70b9\n    graph.eliminate_dead_code()\n\n    return fx.GraphModule(m, graph)\n</code></pre>"},{"location":"api/odefunc/#spikeDE.odefunc.symbolic_trace_leaf_neurons","title":"symbolic_trace_leaf_neurons","text":"<pre><code>symbolic_trace_leaf_neurons(module: Module) -&gt; GraphModule\n</code></pre> <p>Symbolically trace a module using the custom <code>SNNLeafTracer</code>.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The PyTorch module to trace.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>GraphModule</code>           \u2013            <p>fx.GraphModule: The traced graph module with leaf neurons preserved.</p> </li> </ul> Source code in <code>spikeDE/odefunc.py</code> <pre><code>def symbolic_trace_leaf_neurons(module: nn.Module) -&gt; fx.GraphModule:\n    \"\"\"\n    Symbolically trace a module using the custom `SNNLeafTracer`.\n\n    Args:\n        module (nn.Module): The PyTorch module to trace.\n\n    Returns:\n        fx.GraphModule: The traced graph module with leaf neurons preserved.\n    \"\"\"\n    tracer = SNNLeafTracer()\n    graph = tracer.trace(module)\n    return fx.GraphModule(module, graph)\n</code></pre>"},{"location":"api/snn/","title":"SpikeDE.snn","text":"<p>This module provides the <code>SNNWrapper</code> class, which converts standard Spiking Neural Networks (SNNs) into Fractional Differential Equation (FDE) systems. It supports flexible configuration of  fractional orders (\\(\\alpha\\)) per neuron layer, including single-term, multi-term (distributed order), and learnable parameters.</p>"},{"location":"api/snn/#key-features","title":"Key Features","text":"<ul> <li>Per-layer fractional orders (single-term or multi-term).</li> <li>Learnable \\(\\alpha\\) and coefficients via backpropagation.</li> <li>Automatic shape inference and parameter registration.</li> <li>Support for various FDE solvers (Grunwald-Letnikov, L1, etc.).</li> </ul>"},{"location":"api/snn/#spikeDE.snn.PerLayerAlphaConfig","title":"PerLayerAlphaConfig","text":"<pre><code>PerLayerAlphaConfig(\n    alpha: float | list[float] | list[list[float]],\n    n_layers: int,\n    multi_coefficient: list[float] | list[list[float]] | None = None,\n    learn_alpha: bool | list[bool] = False,\n    learn_coefficient: bool | list[bool] = False,\n    alpha_mode: str = \"auto\",\n    device: device = None,\n    dtype: dtype = float32,\n)\n</code></pre> <p>Configuration parser and validator for per-layer fractional orders (\\(\\alpha\\)).</p> <p>This class normalizes user inputs into a consistent internal format based on a decision table logic. It handles three primary configuration cases:</p> <ol> <li> <p>Case A (Per-layer Single-term): Each layer has a single \\(\\alpha\\) value.    Example: <code>alpha=[0.3, 0.5]</code> with <code>alpha_mode='per_layer'</code>.</p> </li> <li> <p>Case B (Multi-term Broadcast): The same multi-term configuration applies to all layers.    Example: <code>alpha=[0.3, 0.5, 0.7]</code> with <code>alpha_mode='multiterm'</code>.</p> </li> <li> <p>Case C (Per-layer Multi-term): Each layer has its own multi-term configuration.    Example: <code>alpha=[[0.3, 0.5], [0.4, 0.6]]</code> (auto-detected).</p> </li> </ol> <p>Attributes:</p> <ul> <li> <code>n_layers</code>               (<code>int</code>)           \u2013            <p>Number of neuron layers in the network.</p> </li> <li> <code>case</code>               (<code>str</code>)           \u2013            <p>Detected configuration case.</p> </li> <li> <code>per_layer_alpha</code>               (<code>list[list[float]]</code>)           \u2013            <p>Normalized alpha values per layer.</p> </li> <li> <code>per_layer_coefficient</code>               (<code>list[list[float]]</code>)           \u2013            <p>Normalized coefficients per layer.</p> </li> <li> <code>per_layer_is_multi_term</code>               (<code>list[bool]</code>)           \u2013            <p>Boolean flag per layer indicating multi-term usage.</p> </li> <li> <code>per_layer_learn_alpha</code>               (<code>list[bool]</code>)           \u2013            <p>Learnable flag for alpha per layer.</p> </li> <li> <code>per_layer_learn_coefficient</code>               (<code>list[bool]</code>)           \u2013            <p>Learnable flag for coefficients per layer.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>float | list[float] | list[list[float]]</code>)           \u2013            <p>Fractional order(s). Accepts:</p> <ul> <li><code>float</code>: Single value for all layers.</li> <li><code>List[float]</code>: Interpretation depends on <code>alpha_mode</code>.</li> <li><code>List[List[float]]</code>: Per-layer multi-term configuration.</li> <li><code>torch.Tensor</code>: Will be converted to list.</li> </ul> </li> <li> <code>n_layers</code>               (<code>int</code>)           \u2013            <p>Total number of neuron layers in the SNN.</p> </li> <li> <code>multi_coefficient</code>               (<code>list[float] | list[list[float]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Coefficients for multi-term FDEs. Accepts:</p> <ul> <li><code>None</code>: Defaults to ones.</li> <li><code>List[float]</code>: Shared coefficients for all layers (Case B).</li> <li><code>List[List[float]]</code>: Per-layer coefficients (Case C).</li> </ul> </li> <li> <code>learn_alpha</code>               (<code>bool | list[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Whether \\(\\alpha\\) values are learnable parameters.</p> <ul> <li><code>bool</code>: Applied globally to all layers.</li> <li><code>List[bool]</code>: Per-layer learnable flags.</li> </ul> </li> <li> <code>learn_coefficient</code>               (<code>bool | list[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Whether coefficients are learnable parameters.</p> <ul> <li><code>bool</code>: Applied globally.</li> <li><code>List[bool]</code>: Per-layer learnable flags.</li> </ul> </li> <li> <code>alpha_mode</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>Disambiguation mode for flat list inputs.</p> <ul> <li><code>'auto'</code>: Automatic detection (default).</li> <li><code>'per_layer'</code>: Force Case A.</li> <li><code>'multiterm'</code>: Force Case B.</li> </ul> </li> <li> <code>device</code>               (<code>device</code>, default:                   <code>None</code> )           \u2013            <p>Target device for tensors.</p> </li> <li> <code>dtype</code>               (<code>dtype</code>, default:                   <code>float32</code> )           \u2013            <p>Data type for tensors (default: float32).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If input dimensions mismatch, invalid modes are provided,         or coefficient lengths do not match alpha lengths.</p> </li> </ul> Source code in <code>spikeDE/snn.py</code> <pre><code>def __init__(\n    self,\n    alpha: Union[float, List[float], List[List[float]]],\n    n_layers: int,\n    multi_coefficient: Optional[Union[List[float], List[List[float]]]] = None,\n    learn_alpha: Union[bool, List[bool]] = False,\n    learn_coefficient: Union[bool, List[bool]] = False,\n    alpha_mode: str = \"auto\",\n    device: torch.device = None,\n    dtype: torch.dtype = torch.float32,\n):\n    r\"\"\"\n    Initialize the Per-Layer Alpha Configuration.\n\n    Args:\n        alpha: Fractional order(s). Accepts:\n\n            - `float`: Single value for all layers.\n            - `List[float]`: Interpretation depends on `alpha_mode`.\n            - `List[List[float]]`: Per-layer multi-term configuration.\n            - `torch.Tensor`: Will be converted to list.\n        n_layers: Total number of neuron layers in the SNN.\n        multi_coefficient: Coefficients for multi-term FDEs. Accepts:\n\n            - `None`: Defaults to ones.\n            - `List[float]`: Shared coefficients for all layers (Case B).\n            - `List[List[float]]`: Per-layer coefficients (Case C).\n        learn_alpha: Whether $\\alpha$ values are learnable parameters.\n\n            - `bool`: Applied globally to all layers.\n            - `List[bool]`: Per-layer learnable flags.\n        learn_coefficient: Whether coefficients are learnable parameters.\n\n            - `bool`: Applied globally.\n            - `List[bool]`: Per-layer learnable flags.\n        alpha_mode: Disambiguation mode for flat list inputs.\n\n            - `'auto'`: Automatic detection (default).\n            - `'per_layer'`: Force Case A.\n            - `'multiterm'`: Force Case B.\n        device: Target device for tensors.\n        dtype: Data type for tensors (default: float32).\n\n    Raises:\n        ValueError: If input dimensions mismatch, invalid modes are provided,\n                    or coefficient lengths do not match alpha lengths.\n    \"\"\"\n    self.n_layers = n_layers\n    self.device = device\n    self.dtype = dtype\n\n    if alpha_mode not in (\"auto\", \"per_layer\", \"multiterm\"):\n        raise ValueError(\n            f\"alpha_mode must be 'auto', 'per_layer', or 'multiterm', got '{alpha_mode}'\"\n        )\n\n    # Parse using decision table\n    self.case, self.per_layer_alpha, self.per_layer_coefficient = self._parse(\n        alpha, multi_coefficient, n_layers, alpha_mode\n    )\n\n    # Determine is_multi_term per layer\n    self.per_layer_is_multi_term = [len(a) &gt; 1 for a in self.per_layer_alpha]\n\n    # Parse learnable flags\n    self._parse_learn_flags(learn_alpha, learn_coefficient)\n</code></pre>"},{"location":"api/snn/#spikeDE.snn.PerLayerAlphaConfig.print_config","title":"print_config","text":"<pre><code>print_config()\n</code></pre> <p>Print configuration summary to stdout.</p> Source code in <code>spikeDE/snn.py</code> <pre><code>def print_config(self):\n    \"\"\"Print configuration summary to stdout.\"\"\"\n    print(f\"\\n[Per-Layer Alpha Configuration]\")\n    print(f\"  Case: {self.case}\")\n    print(f\"  Layers: {self.n_layers}\")\n    for i in range(self.n_layers):\n        alpha = self.per_layer_alpha[i]\n        coef = self.per_layer_coefficient[i]\n        is_multi = self.per_layer_is_multi_term[i]\n        learn_a = self.per_layer_learn_alpha[i]\n        learn_c = self.per_layer_learn_coefficient[i]\n\n        if is_multi:\n            print(\n                f\"  Layer {i}: {len(alpha)}-term, \u03b1={alpha}, coef={coef}, \"\n                f\"learn_\u03b1={learn_a}, learn_coef={learn_c}\"\n            )\n        else:\n            print(f\"  Layer {i}: single-term, \u03b1={alpha[0]}, learn_\u03b1={learn_a}\")\n</code></pre>"},{"location":"api/snn/#spikeDE.snn.PerLayerAlphaConfig.register_parameters","title":"register_parameters","text":"<pre><code>register_parameters(module: Module) -&gt; None\n</code></pre> <p>Register alpha and coefficient as parameters or buffers in the given module.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The nn.Module to register parameters into.</p> </li> </ul> Source code in <code>spikeDE/snn.py</code> <pre><code>def register_parameters(self, module: nn.Module) -&gt; None:\n    \"\"\"\n    Register alpha and coefficient as parameters or buffers in the given module.\n\n    Args:\n        module: The nn.Module to register parameters into.\n    \"\"\"\n    module.per_layer_alpha_params = nn.ParameterList()\n    module.per_layer_coefficient_params = nn.ParameterList()\n    module._alpha_is_param = []\n    module._coef_is_param = []\n\n    for i in range(self.n_layers):\n        alpha_vals = self.per_layer_alpha[i]\n        coef_vals = self.per_layer_coefficient[i]\n        learn_alpha = self.per_layer_learn_alpha[i]\n        learn_coef = self.per_layer_learn_coefficient[i]\n\n        # Create tensors\n        alpha_tensor = torch.tensor(alpha_vals, dtype=self.dtype)\n        coef_tensor = torch.tensor(coef_vals, dtype=self.dtype)\n\n        # Register alpha\n        if learn_alpha:\n            module.per_layer_alpha_params.append(nn.Parameter(alpha_tensor))\n            #\n            module._alpha_is_param.append(True)\n        else:\n            module.per_layer_alpha_params.append(\n                nn.Parameter(alpha_tensor, requires_grad=False)\n            )\n            module._alpha_is_param.append(False)\n\n        # Register coefficient\n        if learn_coef:\n            module.per_layer_coefficient_params.append(nn.Parameter(coef_tensor))\n            module._coef_is_param.append(True)\n        else:\n            module.per_layer_coefficient_params.append(\n                nn.Parameter(coef_tensor, requires_grad=False)\n            )\n            module._coef_is_param.append(False)\n\n    # Store metadata\n    module._per_layer_is_multi_term = self.per_layer_is_multi_term.copy()\n    module._alpha_case = self.case\n</code></pre>"},{"location":"api/snn/#spikeDE.snn.SNNWrapper","title":"SNNWrapper","text":"<pre><code>SNNWrapper(\n    base: Module,\n    integrator: str = \"odeint\",\n    interpolation_method: str = \"linear\",\n    alpha: float | list[float] | list[list[float]] = 0.5,\n    multi_coefficient: list[float] | list[list[float]] | None = None,\n    learn_alpha: bool | list[bool] = False,\n    learn_coefficient: bool | list[bool] = False,\n    alpha_mode: str = \"auto\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>SNN Wrapper with per-layer fractional order support.</p> <p>This class wraps a standard PyTorch SNN model, converting its forward pass into a numerical integration of Fractional Differential Equations (FDEs). It supports flexible configuration of fractional orders (\\(\\alpha\\)) per layer.</p> <p>Supported Features:</p> <ul> <li>Per-layer alpha (single-term or multi-term).</li> <li>Per-layer learnable alpha and coefficients.</li> <li>Multiple FDE solvers (Grunwald-Letnikov, L1, etc.).</li> <li>Automatic shape inference via FX tracing.</li> </ul> <p>Parameters:</p> <ul> <li> <code>base</code>               (<code>Module</code>)           \u2013            <p>Base neural network model (nn.Module) containing neuron layers.</p> </li> <li> <code>integrator</code>               (<code>str</code>, default:                   <code>'odeint'</code> )           \u2013            <p>Integrator type (<code>'odeint'</code>, <code>'fdeint'</code>, etc.).</p> </li> <li> <code>interpolation_method</code>               (<code>str</code>, default:                   <code>'linear'</code> )           \u2013            <p>Input interpolation method.</p> </li> <li> <code>alpha</code>               (<code>float | list[float] | list[list[float]]</code>, default:                   <code>0.5</code> )           \u2013            <p>Fractional order(s). Can be:</p> <ul> <li><code>float</code>: same alpha for all layers (single-term).</li> <li><code>List[float]</code>: interpretation depends on alpha_mode.</li> <li><code>List[List[float]]</code>: per-layer multi-term (Case C).</li> </ul> </li> <li> <code>multi_coefficient</code>               (<code>list[float] | list[list[float]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Coefficients for multi-term FDE. Can be:</p> <ul> <li><code>None</code>: auto-fill with ones.</li> <li><code>List[float]</code>: same for all multi-term layers (Case B).</li> <li><code>List[List[float]]</code>: per-layer (Case C).</li> </ul> </li> <li> <code>learn_alpha</code>               (<code>bool | list[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Whether alpha is learnable. Can be:</p> <ul> <li><code>bool</code>: applies to all layers.</li> <li><code>List[bool]</code>: per-layer.</li> </ul> </li> <li> <code>learn_coefficient</code>               (<code>bool | list[bool]</code>, default:                   <code>False</code> )           \u2013            <p>Whether coefficients are learnable. Can be:</p> <ul> <li><code>bool</code>: applies to all layers.</li> <li><code>List[bool]</code>: per-layer.</li> </ul> </li> <li> <code>alpha_mode</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>How to interpret flat list alpha. Options:</p> <ul> <li><code>'auto'</code>: Try to detect based on length and multi_coefficient.</li> <li><code>'per_layer'</code>: Force Case A (each element is one layer's alpha).</li> <li><code>'multiterm'</code>: Force Case B (broadcast multi-term to all layers). Ignored if alpha contains nested lists (always Case C).</li> </ul> </li> </ul> Returned by: <ul> <li> API Reference SpikeDE.snn SNNWrapper eval </li> <li> API Reference SpikeDE.snn SNNWrapper train </li> </ul> Source code in <code>spikeDE/snn.py</code> <pre><code>def __init__(\n    self,\n    base: nn.Module,\n    integrator: str = \"odeint\",\n    interpolation_method: str = \"linear\",\n    alpha: Union[float, List[float], List[List[float]]] = 0.5,\n    multi_coefficient: Optional[Union[List[float], List[List[float]]]] = None,\n    learn_alpha: Union[bool, List[bool]] = False,\n    learn_coefficient: Union[bool, List[bool]] = False,\n    alpha_mode: str = \"auto\",\n):\n    \"\"\"\n    Initialize SNNWrapper with per-layer alpha support.\n\n    Args:\n        base: Base neural network model (nn.Module) containing neuron layers.\n        integrator: Integrator type (`'odeint'`, `'fdeint'`, etc.).\n        interpolation_method: Input interpolation method.\n        alpha: Fractional order(s). Can be:\n\n            - `float`: same alpha for all layers (single-term).\n            - `List[float]`: interpretation depends on alpha_mode.\n            - `List[List[float]]`: per-layer multi-term (Case C).\n        multi_coefficient: Coefficients for multi-term FDE. Can be:\n\n            - `None`: auto-fill with ones.\n            - `List[float]`: same for all multi-term layers (Case B).\n            - `List[List[float]]`: per-layer (Case C).\n        learn_alpha: Whether alpha is learnable. Can be:\n\n            - `bool`: applies to all layers.\n            - `List[bool]`: per-layer.\n        learn_coefficient: Whether coefficients are learnable. Can be:\n\n            - `bool`: applies to all layers.\n            - `List[bool]`: per-layer.\n        alpha_mode: How to interpret flat list alpha. Options:\n\n            - `'auto'`: Try to detect based on length and multi_coefficient.\n            - `'per_layer'`: Force Case A (each element is one layer's alpha).\n            - `'multiterm'`: Force Case B (broadcast multi-term to all layers).\n            Ignored if alpha contains nested lists (always Case C).\n    \"\"\"\n    super().__init__()\n\n    self.integrator_indicator = integrator\n    self.interpolation_method = interpolation_method\n    self.integrator = get_integrator(integrator)\n\n    # Store alpha config (will be finalized in _set_neuron_shapes)\n    self._alpha_spec = alpha\n    self._multi_coefficient_spec = multi_coefficient\n    self._learn_alpha_spec = learn_alpha\n    self._learn_coefficient_spec = learn_coefficient\n    self._alpha_mode_spec = alpha_mode\n\n    # Build FX-based ODEFunc\n    self.ode_func = ODEFuncFromFX(base, interpolation_method=interpolation_method)\n    self.traced_backbone = self.ode_func.traced\n    self.post_neuron_module = self.ode_func.get_post_neuron_module()\n    self.neuron_instances = None\n    # Initialize as None to track initialization status\n    self.neuron_shapes = None\n\n    self._is_initialized = False  # Add initialization flag\n    # we must call _set_neuron_shapes before run\n    self._alpha_config = None\n\n    # Store direct references BEFORE compiling\n    self._ode_gm = self.ode_func.ode_gm\n    self._ode_func_uncompiled = self.ode_func  # Add this for set_inputs\n</code></pre>"},{"location":"api/snn/#spikeDE.snn.SNNWrapper.eval","title":"eval","text":"<pre><code>eval() -&gt; SNNWrapper\n</code></pre> <p>Sets the module in evaluation mode and propagates to submodules.</p> Source code in <code>spikeDE/snn.py</code> <pre><code>def eval(self) -&gt; \"SNNWrapper\":\n    \"\"\"Sets the module in evaluation mode and propagates to submodules.\"\"\"\n    super().eval()\n    self.traced_backbone.eval()\n    self._ode_gm.eval()  # Use stored reference\n    if hasattr(self, \"post_neuron_module\") and self.post_neuron_module is not None:\n        self.post_neuron_module.eval()\n    return self\n</code></pre>"},{"location":"api/snn/#spikeDE.snn.SNNWrapper.forward","title":"forward","text":"<pre><code>forward(\n    x: Tensor,\n    x_time: Tensor,\n    output_time: Tensor | None = None,\n    method: str = \"euler\",\n    options: dict[str, Any] = {\"step_size\": 0.1},\n) -&gt; Tensor\n</code></pre> <p>Perform the forward pass of the Fractional SNN.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor of shape [Time_Steps, Batch, ...].</p> </li> <li> <code>x_time</code>               (<code>Tensor</code>)           \u2013            <p>Time points corresponding to input steps, shape [Time_Steps,].</p> </li> <li> <code>output_time</code>               (<code>Tensor | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional time points for output. If None, auto-generated.</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>'euler'</code> )           \u2013            <p>Integration method (<code>'euler'</code>, <code>'gl'</code>, <code>'trap'</code>, <code>'l1'</code>, etc.).</p> </li> <li> <code>options</code>               (<code>dict[str, Any]</code>, default:                   <code>{'step_size': 0.1}</code> )           \u2013            <p>Dictionary of solver options (e.g., <code>{'step_size': 0.1}</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Output tensor processed by the post-neuron module.</p> </li> </ul> Source code in <code>spikeDE/snn.py</code> <pre><code>@requires_initialization\ndef forward(\n    self,\n    x: torch.Tensor,\n    x_time: torch.Tensor,\n    output_time: Optional[torch.Tensor] = None,\n    method: str = \"euler\",\n    options: Dict[str, Any] = {\"step_size\": 0.1},\n) -&gt; torch.Tensor:\n    \"\"\"\n    Perform the forward pass of the Fractional SNN.\n\n    Args:\n        x: Input tensor of shape [Time_Steps, Batch, ...].\n        x_time: Time points corresponding to input steps, shape [Time_Steps,].\n        output_time: Optional time points for output. If None, auto-generated.\n        method: Integration method (`'euler'`, `'gl'`, `'trap'`, `'l1'`, etc.).\n        options: Dictionary of solver options (e.g., `{'step_size': 0.1}`).\n\n    Returns:\n        Output tensor processed by the post-neuron module.\n    \"\"\"\n    time_steps, batch_size = x.shape[:2]\n    if len(x_time) != time_steps:\n        x_time = x_time[0:time_steps]\n    self._ode_func_uncompiled.set_inputs(x, x_time)\n\n    # Initialize neuron membrane potentials\n    adjusted_neuron_shapes = [\n        (batch_size, *shape[1:]) for shape in self.neuron_shapes\n    ]\n    v_mems = [torch.zeros(s, device=x.device) for s in adjusted_neuron_shapes]\n    # Initialize boundary outputs with correct shapes\n    boundary_inits = [\n        torch.zeros((batch_size, *shape[1:]), device=x.device)\n        for shape in self.boundary_shapes\n    ]\n\n    # Initial state:\n    initial_state = (*v_mems, *boundary_inits)\n\n    # Helper function to avoid code duplication\n    def process_boundaries(v_mem_all_time_and_final_spike):\n        if self.n_boundaries == 1:\n            finalspike_out = torch.stack(v_mem_all_time_and_final_spike[-1], dim=0)\n            return self.post_neuron_module(finalspike_out)\n        else:\n            boundary_outputs = tuple(\n                torch.stack(\n                    v_mem_all_time_and_final_spike[self.neuron_count + i], dim=0\n                )\n                for i in range(self.n_boundaries)\n            )\n            return self.post_neuron_module(boundary_outputs)\n\n    # 2) now run the real integration\n    if True:  # output_time is None:\n        # create output_time to add one more element to x_time\n        if len(x_time) &gt; 1:\n            dt = x_time[1] - x_time[0]\n        else:\n            dt = options.get(\"step_size\", 1.0)\n\n        # Create the next time point on the same device\n        next_t = (x_time[-1] + dt).unsqueeze(0)\n\n        # Concatenate to form the full output time vector\n        output_time = torch.cat((x_time, next_t), dim=0)\n\n    # Get current per-layer alpha and coefficient\n    per_layer_alpha = self.get_per_layer_alpha()\n    per_layer_coefficient = self.get_per_layer_coefficient()\n\n    # check if it is fdeint or odeint\n    if self.integrator_indicator == \"odeint\" and method == \"euler\":\n        v_mem_all_time_and_final_spike = euler_integrate_tuple_compiled(\n            self.ode_func, initial_state, output_time, self.neuron_count\n        )\n        return process_boundaries(v_mem_all_time_and_final_spike)\n\n    elif self.integrator_indicator == \"fdeint\":\n        memory = None if options.get(\"memory\", -1) == -1 else options[\"memory\"]\n\n        # Determine solver based on alpha case:\n        # Case A: per-layer single-term \u2192 use requested solver\n        # Case B/C: multi-term involved \u2192 always use multiterm solver\n        use_multiterm = self._alpha_case in (\"B\", \"C\") or method == \"glmulti\"\n\n        if use_multiterm:\n            # Use multiterm solver for Case B, C, or explicit request\n            if self._alpha_case in (\"B\", \"C\") and method not in (\"gl\", \"glmulti\"):\n                import warnings\n\n                warnings.warn(\n                    f\"Alpha configuration is Case {self._alpha_case} (multi-term). \"\n                    f\"Method '{method}' not supported for multi-term, \"\n                    f\"using GrunwaldLetnikovMultitermSNN instead.\",\n                    UserWarning,\n                )\n\n            v_mem_all_time_and_final_spike = glmethod_multiterm_integrate_tuple(\n                self.ode_func,\n                initial_state,\n                per_layer_alpha,\n                output_time,\n                memory=memory,\n                per_layer_coefficient=per_layer_coefficient,\n            )\n        else:\n            # Case A (per-layer single-term): use requested solver\n            integrate_method = SOLVERS[method]\n            v_mem_all_time_and_final_spike = integrate_method(\n                self.ode_func,\n                initial_state,\n                per_layer_alpha,\n                output_time,\n                memory=memory,\n                per_layer_coefficient=per_layer_coefficient,\n            )\n\n        return process_boundaries(v_mem_all_time_and_final_spike)\n\n    # ----------------------- please ignore the following temporarily-----------------------\n\n    if self.integrator_indicator == \"odeint_mem\":\n        v_mem_all_time_and_final_spike = step_dynamics(\n            self.ode_func, initial_state, output_time\n        )\n        finalspike_out = torch.stack(v_mem_all_time_and_final_spike, dim=0)\n        final_output = self.post_neuron_module(finalspike_out)\n        return final_output\n\n    elif (\n        self.integrator_indicator == \"odeint_adjoint\"\n        or self.integrator_indicator == \"odeint\"\n    ):\n        v_mem_all_time_and_cumulated_spike = self.integrator(\n            self.ode_func,\n            initial_state,\n            output_time,\n            method=method,\n            options=options,\n        )\n        finalspike_out_sum = v_mem_all_time_and_cumulated_spike[-1][-1:, ...]\n\n        final_output = self.post_neuron_module(finalspike_out_sum)\n        return final_output\n\n    elif self.integrator_indicator == \"fdeint_mem\":\n\n        v_mem_all_time_and_final_spike = step_dynamics(\n            self.ode_func, initial_state, output_time\n        )\n        # finalspike_out = v_mem_all_time_and_final_spike\n        finalspike_out = torch.stack(v_mem_all_time_and_final_spike, dim=0)\n        final_output = self.post_neuron_module(finalspike_out)\n        return final_output\n\n    elif self.integrator_indicator == \"fdeint_adjoint\":\n        memory = None if options.get(\"memory\", -1) == -1 else options[\"memory\"]\n\n        v_mem_all_time_and_cumulated_spike = fdeint_adjoint(\n            self.ode_func,\n            initial_state,\n            per_layer_alpha[0],\n            output_time,\n            method=method,\n            memory=memory,\n        )\n        # print('v_mem_all_time_and_cumulated_spike.shape: ', v_mem_all_time_and_cumulated_spike[-1].shape)\n        finalspike_out_sum = v_mem_all_time_and_cumulated_spike[-1].unsqueeze(0)\n        final_output = self.post_neuron_module(finalspike_out_sum)\n        return final_output\n\n    elif (\n        False\n    ):  # self.integrator_indicator == \"fdeint_adjoint\" or self.integrator_indicator == \"fdeint\":\n        # print(f\"output_time: {output_time}\")# print('using', self.integrator_indicator, 'for integration')\n        # raise NotImplementedError(\"please only use odeint+euler or fdeint+gl\")\n        T = output_time[-1]\n        step_size = output_time[-1] - output_time[-2]\n        v_mem_all_time_and_cumulated_spike = self.integrator(\n            self.ode_func,\n            (*v_mems, spike_sum_init),\n            torch.tensor(self.alpha),\n            t=T,\n            step_size=step_size,\n            method=method,\n            options=options,\n        )\n        # print(v_mem_all_time_and_cumulated_spike[-1].shape)\n        finalspike_out_sum = v_mem_all_time_and_cumulated_spike[-1].unsqueeze(0)\n        # v_scaled = final_v_mem - self.ode_func.finalneuron_threshold\n        # finalspike_out = self.ode_func.finalneuron_surrogate_f(v_scaled, self.ode_func.finalneuron_surrogate_grad_scale)\n        final_output = self.post_neuron_module(finalspike_out_sum)\n        return final_output\n</code></pre>"},{"location":"api/snn/#spikeDE.snn.SNNWrapper.get_per_layer_alpha","title":"get_per_layer_alpha","text":"<pre><code>get_per_layer_alpha() -&gt; list[Tensor]\n</code></pre> <p>Get current per-layer alpha values as tensors. Always returns tensors to maintain gradient flow for learnable alphas.</p> <p>Returns:</p> <ul> <li> <code>list[Tensor]</code>           \u2013            <p>List of alpha tensors for each layer.</p> </li> </ul> Source code in <code>spikeDE/snn.py</code> <pre><code>def get_per_layer_alpha(self) -&gt; List[torch.Tensor]:\n    \"\"\"\n    Get current per-layer alpha values as tensors.\n    Always returns tensors to maintain gradient flow for learnable alphas.\n\n    Returns:\n        List of alpha tensors for each layer.\n    \"\"\"\n    result = []\n    for i in range(len(self.per_layer_alpha_params)):\n        alpha_param = self.per_layer_alpha_params[i]\n        # Always return tensor to maintain gradient flow\n        # Don't call .item() as it detaches from computation graph!\n        result.append(alpha_param)\n    return result\n</code></pre>"},{"location":"api/snn/#spikeDE.snn.SNNWrapper.get_per_layer_coefficient","title":"get_per_layer_coefficient","text":"<pre><code>get_per_layer_coefficient() -&gt; list[Tensor]\n</code></pre> <p>Get current per-layer coefficient values as tensors. Always returns tensors to maintain gradient flow.</p> Note <p>For single-term layers (Case A), coefficients are technically unused by solvers but returned as <code>[1.0]</code> for interface consistency.</p> <p>Returns:</p> <ul> <li> <code>list[Tensor]</code>           \u2013            <p>List of coefficient tensors for each layer.</p> </li> </ul> Source code in <code>spikeDE/snn.py</code> <pre><code>def get_per_layer_coefficient(self) -&gt; List[torch.Tensor]:\n    \"\"\"\n    Get current per-layer coefficient values as tensors.\n    Always returns tensors to maintain gradient flow.\n\n    Note: \n        For single-term layers (Case A), coefficients are technically unused by solvers but returned as `[1.0]` for interface consistency.\n\n    Returns:\n        List of coefficient tensors for each layer.\n    \"\"\"\n    result = []\n    for i in range(len(self.per_layer_coefficient_params)):\n        # Always return the coefficient tensor\n        # Don't return None as it breaks gradient flow\n        result.append(self.per_layer_coefficient_params[i])\n    return result\n</code></pre>"},{"location":"api/snn/#spikeDE.snn.SNNWrapper.print_alpha_info","title":"print_alpha_info","text":"<pre><code>print_alpha_info()\n</code></pre> <p>Print current alpha values and their learnable status.</p> Source code in <code>spikeDE/snn.py</code> <pre><code>def print_alpha_info(self):\n    \"\"\"Print current alpha values and their learnable status.\"\"\"\n    if not self._is_initialized:\n        print(\"Not initialized yet. Call _set_neuron_shapes first.\")\n        return\n\n    print(\"\\n[Current Alpha Values]\")\n    for i in range(len(self.per_layer_alpha_params)):\n        alpha = self.per_layer_alpha_params[i]\n        is_multi = self._per_layer_is_multi_term[i]\n        is_learnable = self._alpha_is_param[i]\n\n        if is_multi:\n            coef = self.per_layer_coefficient_params[i]\n            coef_learnable = self._coef_is_param[i]\n            print(f\"  Layer {i} (multi-term):\")\n            print(f\"    Alpha: {alpha.data.tolist()} (learnable: {is_learnable})\")\n            print(\n                f\"    Coefficient: {coef.data.tolist()} (learnable: {coef_learnable})\"\n            )\n        else:\n            print(\n                f\"  Layer {i} (single-term): alpha = {alpha.item():.4f} (learnable: {is_learnable})\"\n            )\n</code></pre>"},{"location":"api/snn/#spikeDE.snn.SNNWrapper.train","title":"train","text":"<pre><code>train(mode: bool = True) -&gt; SNNWrapper\n</code></pre> <p>Sets the module in training mode and propagates to submodules.</p> Source code in <code>spikeDE/snn.py</code> <pre><code>def train(self, mode: bool = True) -&gt; \"SNNWrapper\":\n    \"\"\"Sets the module in training mode and propagates to submodules.\"\"\"\n    super().train(mode)\n    self.traced_backbone.train(mode)\n    self._ode_gm.train(mode)  # Use stored reference\n    if hasattr(self, \"post_neuron_module\") and self.post_neuron_module is not None:\n        self.post_neuron_module.train(mode)\n    return self\n</code></pre>"},{"location":"api/solver/","title":"SpikeDE.solver","text":"<p>This module delivers a comprehensive, differentiable numerical engine designed to simulate Spiking Neural Networks (SNNs) governed by Fractional Differential Equations (FDEs). Bridging the gap between fractional calculus and deep learning, this module supports both Riemann-Liouville and Caputo formulations through a diverse array of high-order discretization schemes, including Gr\u00fcnwald-Letnikov (GL), Product Trapezoidal, L1, and Adams-Bashforth methods. It enables precise modeling of complex temporal dynamics while maintaining full compatibility with gradient-based optimization.</p> <p>Whether used for forward inference via <code>snn_solve</code> or for training sophisticated fractional SNNs, the module provides a mathematically rigorous foundation for next-generation neural dynamics. Its architecture is built to handle advanced requirements such as per-layer fractional orders, multi-term distributed-order equations, and efficient memory management, ensuring scalability for long-sequence modeling.</p>"},{"location":"api/solver/#key-features","title":"Key Features","text":"<ul> <li>Diverse Discretization Schemes: Implements multiple high-precision numerical methods (Gr\u00fcnwald-Letnikov, Product Trapezoidal, L1, Adams-Bashforth) to solve FDEs under both Riemann-Liouville and Caputo definitions.</li> <li>Advanced Fractional Configurations: Natively supports per-layer fractional orders, allowing different layers to exhibit distinct memory properties, and handles multi-term distributed-order equations for complex dynamical systems.</li> <li>Flexible Solver Interface: Provides a unified API (<code>snn_solve</code>) alongside low-level integration primitives (<code>gl_integrate_tuple</code>, <code>l1_integrate_tuple</code>, etc.) for custom solver development and fine-grained control over state evolution.</li> </ul>"},{"location":"api/solver/#spikeDE.solver.PerLayerAlphaInfo","title":"PerLayerAlphaInfo  <code>dataclass</code>","text":"<pre><code>PerLayerAlphaInfo(\n    alpha: Tensor,\n    is_multi_term: bool,\n    coefficient: Tensor | None = None,\n    h_alpha: Tensor | None = None,\n    h_alpha_gamma: Tensor | None = None,\n    h_alpha_over_alpha_gamma: Tensor | None = None,\n)\n</code></pre> <p>Metadata container for the fractional order (\\(\\alpha\\)) configuration of a single layer.</p> <p>Stores the fractional order(s) and precomputed constants required for numerical integration. To ensure gradient flow during backpropagation when \\(\\alpha\\) is learnable, all values are stored as <code>torch.Tensor</code> objects rather than Python floats.</p> <p>Attributes:</p> <ul> <li> <code>alpha</code>               (<code>Tensor</code>)           \u2013            <p>A tensor containing the fractional order(s).    Shape <code>(1,)</code> for single-term, shape <code>(M,)</code> for multi-term with \\(M\\) terms.</p> </li> <li> <code>is_multi_term</code>               (<code>bool</code>)           \u2013            <p>Boolean flag indicating if the layer has multiple fractional terms (\\(M &gt; 1\\)).</p> </li> <li> <code>coefficient</code>               (<code>Tensor | None</code>)           \u2013            <p>Optional tensor of coefficients \\([c_1, ..., c_M]\\) for multi-term equations.          Defaults to ones if not provided.</p> </li> <li> <code>h_alpha</code>               (<code>Tensor | None</code>)           \u2013            <p>Precomputed \\(h^\\alpha\\) (Single-term only).</p> </li> <li> <code>h_alpha_gamma</code>               (<code>Tensor | None</code>)           \u2013            <p>Precomputed \\(h^\\alpha \\cdot \\Gamma(2-\\alpha)\\) (Single-term only).</p> </li> <li> <code>h_alpha_over_alpha_gamma</code>               (<code>Tensor | None</code>)           \u2013            <p>Precomputed \\(h^\\alpha / (\\alpha \\cdot \\Gamma(\\alpha))\\) (Single-term only).</p> </li> </ul> Used by: <ul> <li> API Reference SpikeDE.solver SNNSolverConfig </li> </ul>"},{"location":"api/solver/#spikeDE.solver.SNNSolverConfig","title":"SNNSolverConfig  <code>dataclass</code>","text":"<pre><code>SNNSolverConfig(\n    N: int,\n    h: Tensor,\n    device: device,\n    dtype: dtype,\n    n_components: int,\n    n_integrate: int,\n    per_layer_info: list[PerLayerAlphaInfo] = list(),\n)\n</code></pre> <p>Central configuration object for SNN fractional solvers.</p> <p>Aggregates simulation parameters, device information, and per-layer fractional metadata to streamline the solver execution loop.</p> <p>Attributes:</p> <ul> <li> <code>N</code>               (<code>int</code>)           \u2013            <p>Number of time points in the grid.</p> </li> <li> <code>h</code>               (<code>Tensor</code>)           \u2013            <p>Step size tensor (scalar), assumed uniform \\(h = t_{k+1} - t_k\\).</p> </li> <li> <code>device</code>               (<code>device</code>)           \u2013            <p>Torch device for computation.</p> </li> <li> <code>dtype</code>               (<code>dtype</code>)           \u2013            <p>Torch data type for computation.</p> </li> <li> <code>n_components</code>               (<code>int</code>)           \u2013            <p>Total number of state components (neurons + boundaries).</p> </li> <li> <code>n_integrate</code>               (<code>int</code>)           \u2013            <p>Number of components to integrate (excludes boundary outputs).</p> </li> <li> <code>per_layer_info</code>               (<code>list[PerLayerAlphaInfo]</code>)           \u2013            <p>List of <code>PerLayerAlphaInfo</code> objects, one per integrated layer.</p> </li> </ul> Returned by: <ul> <li> API Reference SpikeDE.solver SNNSolverConfig from_inputs </li> </ul> Used by: <ul> <li> API Reference SpikeDE.solver SNNFractionalMethod compute_convolution </li> <li> API Reference SpikeDE.solver SNNFractionalMethod compute_update_for_layer </li> <li> API Reference SpikeDE.solver SNNFractionalMethod compute_weights_for_layer </li> <li> API Reference SpikeDE.solver SNNFractionalMethod initialize </li> </ul>"},{"location":"api/solver/#spikeDE.solver.SNNSolverConfig.from_inputs","title":"from_inputs  <code>classmethod</code>","text":"<pre><code>from_inputs(\n    y0_tuple: tuple[Tensor, ...],\n    per_layer_alpha: list[Any],\n    t_grid: Tensor,\n    per_layer_coefficient: list[Tensor | None] | None = None,\n) -&gt; SNNSolverConfig\n</code></pre> <p>Constructs a solver configuration from user inputs.</p> <p>Processes raw alpha inputs (scalars, lists, or tensors) into standardized <code>PerLayerAlphaInfo</code> objects. Precomputes constants involving the Gamma function for single-term solvers to optimize the main integration loop.</p> <p>Parameters:</p> <ul> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Tuple of initial state tensors. Used to infer device and dtype.</p> </li> <li> <code>per_layer_alpha</code>               (<code>list[Any]</code>)           \u2013            <p>List of alpha values. Each element can be:</p> <ul> <li><code>float</code>: Single-term scalar.</li> <li><code>torch.Tensor</code>: 1-element (single-term) or M-element (multi-term).</li> <li><code>list</code>: Converted to tensor.</li> </ul> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time grid tensor.</p> </li> <li> <code>per_layer_coefficient</code>               (<code>list[Tensor | None] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of coefficient tensors for multi-term layers.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SNNSolverConfig</code>           \u2013            <p>A configured <code>SNNSolverConfig</code> instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If <code>t_grid</code> has fewer than 2 points or coefficient dimensions mismatch alpha.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>@classmethod\ndef from_inputs(\n    cls,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    per_layer_alpha: List[Any],\n    t_grid: torch.Tensor,\n    per_layer_coefficient: Optional[List[Optional[torch.Tensor]]] = None,\n) -&gt; \"SNNSolverConfig\":\n    r\"\"\"\n    Constructs a solver configuration from user inputs.\n\n    Processes raw alpha inputs (scalars, lists, or tensors) into standardized\n    `PerLayerAlphaInfo` objects. Precomputes constants involving the Gamma function\n    for single-term solvers to optimize the main integration loop.\n\n    Args:\n        y0_tuple: Tuple of initial state tensors. Used to infer device and dtype.\n        per_layer_alpha: List of alpha values. Each element can be:\n\n            - `float`: Single-term scalar.\n            - `torch.Tensor`: 1-element (single-term) or M-element (multi-term).\n            - `list`: Converted to tensor.\n        t_grid: Time grid tensor.\n        per_layer_coefficient: Optional list of coefficient tensors for multi-term layers.\n\n    Returns:\n        A configured `SNNSolverConfig` instance.\n\n    Raises:\n        AssertionError: If `t_grid` has fewer than 2 points or coefficient dimensions mismatch alpha.\n    \"\"\"\n    device = y0_tuple[0].device\n    dtype = y0_tuple[0].dtype\n\n    N = len(t_grid)\n    assert N &gt;= 2, \"t_grid must have at least 2 points\"\n\n    t_grid = t_grid.to(device=device, dtype=dtype)\n    h = t_grid[-1] - t_grid[-2]\n\n    n_components = len(y0_tuple)\n    # length of (dv1/dt, dv2/dt, ..., dvN/dt, boundary_1, boundary_2, ...)\n\n    n_integrate = len(per_layer_alpha)\n    # n_integrate is the number of neurons,\n    # i.e., length of (dv1/dt, dv2/dt, ..., dvN/dt)\n\n    # Build per-layer info\n    per_layer_info = []\n    for i, alpha in enumerate(per_layer_alpha):\n        # Determine if this layer is multi-term based on number of elements\n        if isinstance(alpha, torch.Tensor):\n            is_multi_term = alpha.numel() &gt; 1\n            alpha_tensor = alpha.to(device=device, dtype=dtype)\n        elif isinstance(alpha, (list, tuple)):\n            is_multi_term = len(alpha) &gt; 1\n            alpha_tensor = torch.tensor(alpha, dtype=dtype, device=device)\n        else:\n            # Scalar float\n            is_multi_term = False\n            alpha_tensor = torch.tensor([alpha], dtype=dtype, device=device)\n\n        # Get coefficient for this layer\n        coeff = None\n        if per_layer_coefficient is not None and i &lt; len(per_layer_coefficient):\n            coeff = per_layer_coefficient[i]\n            if coeff is not None and not isinstance(coeff, torch.Tensor):\n                coeff = torch.tensor(coeff, dtype=dtype, device=device)\n            elif coeff is not None:\n                coeff = coeff.to(device=device, dtype=dtype)\n\n                # Now compare dimensions when coeff exists\n            assert (\n                coeff.numel() == alpha_tensor.numel()\n            ), f\"Coefficient tensor size mismatch: {coeff.numel()} vs {alpha_tensor.numel()}\"\n\n        # Default coefficient to ones if not provided (for both single and multi-term)\n        if coeff is None:\n            coeff = torch.ones(alpha_tensor.numel(), dtype=dtype, device=device)\n\n        # For single-term, precompute constants (used by non-multiterm solvers)\n        if not is_multi_term:\n            alpha_val = (\n                alpha_tensor.squeeze()\n            )  # Keep as 0-dim tensor for gradient flow\n            h_alpha = torch.pow(h, alpha_val)\n            gamma_2_minus_alpha = math.gamma(\n                2 - alpha_val.item()\n            )  # gamma needs float\n            gamma_alpha = math.gamma(alpha_val.item())\n\n            info = PerLayerAlphaInfo(\n                alpha=alpha_tensor,  # Keep as tensor for gradient flow\n                is_multi_term=False,\n                coefficient=coeff,\n                h_alpha=h_alpha,\n                h_alpha_gamma=h_alpha * gamma_2_minus_alpha,\n                h_alpha_over_alpha_gamma=h_alpha / (alpha_val * gamma_alpha),\n            )\n        else:\n            info = PerLayerAlphaInfo(\n                alpha=alpha_tensor,\n                is_multi_term=True,\n                coefficient=coeff,\n            )\n\n        per_layer_info.append(info)\n\n    return cls(\n        N=N,\n        h=h,\n        device=device,\n        dtype=dtype,\n        n_components=n_components,\n        n_integrate=n_integrate,\n        per_layer_info=per_layer_info,\n    )\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.SNNFractionalMethod","title":"SNNFractionalMethod","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class (ABC) for SNN fractional differential equation solvers.</p> <p>Defines the interface for various numerical methods (GL, L1, Trapezoidal, etc.). Implementations must define how weights are computed, how convolutions are performed, and how the state update is calculated.</p> <p>Subclasses distinguish themselves by:</p> <ol> <li>The formulation used (Riemann-Liouville vs. Caputo).</li> <li>The type of history stored (\\(y\\) values vs. \\(f(t,y)\\) values).</li> <li>Support for single-term vs. multi-term equations.</li> </ol> Subclassed by: <ul> <li> API Reference SpikeDE.solver AdamsBashforthSNN </li> <li> API Reference SpikeDE.solver GrunwaldLetnikovMultitermSNN </li> <li> API Reference SpikeDE.solver GrunwaldLetnikovSNN </li> <li> API Reference SpikeDE.solver L1MethodSNN </li> <li> API Reference SpikeDE.solver ProductTrapezoidalSNN </li> </ul> Used by: <ul> <li> API Reference SpikeDE.solver snn_solve </li> </ul>"},{"location":"api/solver/#spikeDE.solver.SNNFractionalMethod.stores_f_history","title":"stores_f_history  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>stores_f_history: bool\n</code></pre> <p>Indicates whether the method stores function evaluations \\(f(t, y)\\) or state values \\(y\\) in history.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p><code>True</code> if the method (e.g., Adams-Bashforth) relies on \\(f\\)-history, <code>False</code> if the method (e.g., GL, L1) relies on \\(y\\)-history.</p> </li> </ul>"},{"location":"api/solver/#spikeDE.solver.SNNFractionalMethod.compute_convolution","title":"compute_convolution  <code>abstractmethod</code>","text":"<pre><code>compute_convolution(\n    k: int,\n    start_idx: int,\n    weights: Any,\n    history_i: list[Tensor],\n    config: SNNSolverConfig,\n    layer_idx: int,\n) -&gt; Any\n</code></pre> <p>Computes the weighted sum (convolution) of history values.</p> <p>Calculates \\(\\sum w_j \\cdot h_j\\), where \\(h_j\\) is either \\(y_j\\) or \\(f_j\\) depending on <code>stores_f_history</code>.</p> <p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Current time step index.</p> </li> <li> <code>start_idx</code>               (<code>int</code>)           \u2013            <p>Start index of the history window.</p> </li> <li> <code>weights</code>               (<code>Any</code>)           \u2013            <p>Weights computed by <code>compute_weights_for_layer</code>.</p> </li> <li> <code>history_i</code>               (<code>list[Tensor]</code>)           \u2013            <p>List of historical tensors for the current component.</p> </li> <li> <code>config</code>               (<code>SNNSolverConfig</code>)           \u2013            <p>Solver configuration.</p> </li> <li> <code>layer_idx</code>               (<code>int</code>)           \u2013            <p>Index of the layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The result of the convolution sum (tensor).</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>@abstractmethod\ndef compute_convolution(\n    self,\n    k: int,\n    start_idx: int,\n    weights: Any,\n    history_i: List[torch.Tensor],\n    config: SNNSolverConfig,\n    layer_idx: int,\n) -&gt; Any:\n    r\"\"\"\n    Computes the weighted sum (convolution) of history values.\n\n    Calculates $\\sum w_j \\cdot h_j$, where $h_j$ is either $y_j$ or $f_j$ depending on `stores_f_history`.\n\n    Args:\n        k: Current time step index.\n        start_idx: Start index of the history window.\n        weights: Weights computed by `compute_weights_for_layer`.\n        history_i: List of historical tensors for the current component.\n        config: Solver configuration.\n        layer_idx: Index of the layer.\n\n    Returns:\n        The result of the convolution sum (tensor).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.SNNFractionalMethod.compute_update_for_layer","title":"compute_update_for_layer  <code>abstractmethod</code>","text":"<pre><code>compute_update_for_layer(\n    f_k_i: Tensor, convolution_sum: Any, config: SNNSolverConfig, layer_idx: int\n) -&gt; Tensor\n</code></pre> <p>Computes the next state \\(y_{k+1}\\) for a specific layer.</p> <p>Combines the current derivative \\(f_k\\) and the convolution sum according to the method's formula. Note: The method name in the original code was slightly misleading; this function computes the update, while <code>compute_convolution</code> computes the sum. Based on usage in <code>snn_solve</code>, this function applies the final formula. However, looking at the implementation in subclasses, <code>compute_convolution</code> actually performs the summation loop, and this function applies the scaling.</p> <p>Correction based on code analysis: <code>compute_convolution</code> returns the sum \\(\\sum w_j h_j\\). <code>compute_update_for_layer</code> takes that sum and \\(f_k\\) to return \\(y_{k+1}\\).</p> <p>Parameters:</p> <ul> <li> <code>f_k_i</code>               (<code>Tensor</code>)           \u2013            <p>The derivative/value \\(f(t_k, y_k)\\) for this layer.</p> </li> <li> <code>convolution_sum</code>               (<code>Any</code>)           \u2013            <p>The result of the history convolution.</p> </li> <li> <code>config</code>               (<code>SNNSolverConfig</code>)           \u2013            <p>Solver configuration.</p> </li> <li> <code>layer_idx</code>               (<code>int</code>)           \u2013            <p>Index of the layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The updated state tensor \\(y_{k+1}\\).</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>@abstractmethod\ndef compute_update_for_layer(\n    self,\n    f_k_i: torch.Tensor,\n    convolution_sum: Any,\n    config: SNNSolverConfig,\n    layer_idx: int,\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Computes the next state $y_{k+1}$ for a specific layer.\n\n    Combines the current derivative $f_k$ and the convolution sum according to the method's formula.\n    Note: The method name in the original code was slightly misleading; this function computes the update,\n    while `compute_convolution` computes the sum. Based on usage in `snn_solve`, this function\n    applies the final formula. However, looking at the implementation in subclasses,\n    `compute_convolution` actually performs the summation loop, and this function applies the scaling.\n\n    Correction based on code analysis:\n    `compute_convolution` returns the sum $\\sum w_j h_j$.\n    `compute_update_for_layer` takes that sum and $f_k$ to return $y_{k+1}$.\n\n    Args:\n        f_k_i: The derivative/value $f(t_k, y_k)$ for this layer.\n        convolution_sum: The result of the history convolution.\n        config: Solver configuration.\n        layer_idx: Index of the layer.\n\n    Returns:\n        The updated state tensor $y_{k+1}$.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.SNNFractionalMethod.compute_weights_for_layer","title":"compute_weights_for_layer  <code>abstractmethod</code>","text":"<pre><code>compute_weights_for_layer(\n    k: int, start_idx: int, config: SNNSolverConfig, layer_idx: int\n) -&gt; Any\n</code></pre> <p>Computes the convolution weights for a specific layer at time step \\(k\\).</p> <p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Current time step index.</p> </li> <li> <code>start_idx</code>               (<code>int</code>)           \u2013            <p>Start index of the history window.</p> </li> <li> <code>config</code>               (<code>SNNSolverConfig</code>)           \u2013            <p>Solver configuration containing layer metadata.</p> </li> <li> <code>layer_idx</code>               (<code>int</code>)           \u2013            <p>Index of the layer being processed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>A tensor or structure containing the weights \\(w_j\\) for the convolution sum.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>@abstractmethod\ndef compute_weights_for_layer(\n    self, k: int, start_idx: int, config: SNNSolverConfig, layer_idx: int\n) -&gt; Any:\n    r\"\"\"\n    Computes the convolution weights for a specific layer at time step $k$.\n\n    Args:\n        k: Current time step index.\n        start_idx: Start index of the history window.\n        config: Solver configuration containing layer metadata.\n        layer_idx: Index of the layer being processed.\n\n    Returns:\n        A tensor or structure containing the weights $w_j$ for the convolution sum.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.SNNFractionalMethod.initialize","title":"initialize","text":"<pre><code>initialize(config: SNNSolverConfig) -&gt; None\n</code></pre> <p>Optional hook for method-specific precomputation before the time loop.</p> <p>Used to precompute static coefficients (e.g., GL binomial coefficients) that depend on \\(\\alpha\\) and \\(N\\) but not on the state \\(y\\).</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>SNNSolverConfig</code>)           \u2013            <p>Solver configuration.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def initialize(self, config: SNNSolverConfig) -&gt; None:\n    r\"\"\"\n    Optional hook for method-specific precomputation before the time loop.\n\n    Used to precompute static coefficients (e.g., GL binomial coefficients) that depend\n    on $\\alpha$ and $N$ but not on the state $y$.\n\n    Args:\n        config: Solver configuration.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.GrunwaldLetnikovSNN","title":"GrunwaldLetnikovSNN","text":"<pre><code>GrunwaldLetnikovSNN()\n</code></pre> <p>               Bases: <code>SNNFractionalMethod</code></p> <p>Gr\u00fcnwald-Letnikov (GL) solver for single-term Riemann-Liouville Fractional Differential Equations (FDEs).</p> <p>This class implements the standard GL discretization scheme, which approximates the Riemann-Liouville fractional derivative \\(D^\\alpha y(t)\\) using a finite difference convolution.</p> <p>Mathematical Formulation: The update rule for the state \\(y\\) at step \\(k+1\\) is given by:</p> \\[y_{k+1} = h^\\alpha f(t_k, y_k) - \\sum_{j=0}^{k} c_{k-j}^{(\\alpha)} y_j\\] <p>where \\(h\\) is the step size, \\(f(t, y)\\) is the ODE function, and \\(c_j^{(\\alpha)}\\) are the Gr\u00fcnwald-Letnikov coefficients generated recursively:</p> \\[c_0^{(\\alpha)} = 1, \\quad c_j^{(\\alpha)} = \\left(1 - \\frac{1+\\alpha}{j}\\right)c_{j-1}^{(\\alpha)} \\quad \\text{for } j \\ge 1\\] <p>Key Characteristics:</p> <ul> <li>Formulation: Riemann-Liouville.</li> <li>Accuracy: First-order \\(O(h)\\).</li> <li>Memory: Requires full history of states \\(y\\) unless truncated.</li> <li>Constraint: Strictly supports single-term fractional orders (\\(\\alpha\\) is a scalar per layer).   Attempting to use multi-term \\(\\alpha\\) will raise a <code>ValueError</code>. For multi-term support,   use <code>GrunwaldLetnikovMultitermSNN</code>.</li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the solver with empty coefficient storage.\"\"\"\n    self._c_per_layer: Optional[List[torch.Tensor]] = None\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.ProductTrapezoidalSNN","title":"ProductTrapezoidalSNN","text":"<p>               Bases: <code>SNNFractionalMethod</code></p> <p>Product Trapezoidal solver for single-term Riemann-Liouville FDEs.</p> <p>This method offers higher accuracy (\\(O(h^2)\\)) compared to the Gr\u00fcnwald-Letnikov scheme by using a piecewise linear interpolation of the integrand. It is particularly effective for smooth solutions.</p> <p>The update rule is:</p> \\[y_{k+1} = \\frac{h^\\alpha}{\\Gamma(2-\\alpha)} f(t_k, y_k) - \\sum_{j=0}^{k} A_{j,k+1} y_j\\] <p>The weights \\(A_{j,k+1}\\) are position-dependent and defined as:</p> <ul> <li>For \\(j=0\\):</li> </ul> <p>$\\(A_{0,k+1} = k^{1-\\alpha} - (k+\\alpha)(k+1)^{-\\alpha}\\)$</p> <ul> <li>For \\(j \\ge 1\\):</li> </ul> <p>$\\(A_{j,k+1} = (k+2-j)^{1-\\alpha} + (k-j)^{1-\\alpha} - 2(k+1-j)^{1-\\alpha}\\)$</p> <p>Key Characteristics:</p> <ul> <li>Formulation: Riemann-Liouville.</li> <li>Accuracy: Second-order \\(O(h^2)\\).</li> <li>Constraint: Supports single-term \\(\\alpha\\) only.</li> </ul>"},{"location":"api/solver/#spikeDE.solver.L1MethodSNN","title":"L1MethodSNN","text":"<p>               Bases: <code>SNNFractionalMethod</code></p> <p>L1 scheme solver for single-term Caputo Fractional Differential Equations.</p> <p>The L1 method is the most widely used numerical scheme for Caputo derivatives, offering an accuracy of \\(O(h^{2-\\alpha})\\) for smooth solutions. It approximates the fractional derivative using piecewise linear interpolation of the function.</p> <p>Mathematical Formulation: The update rule is:</p> \\[y_{k+1} = \\frac{h^\\alpha}{\\Gamma(2-\\alpha)} f(t_k, y_k) - \\sum_{j=0}^{k} c_j^{(k)} y_j\\] <p>The coefficients \\(c_j^{(k)}\\) are defined as:</p> <ul> <li>For \\(j=0\\):</li> </ul> <p>$\\(c_0^{(k)} = -\\left((k+1)^{1-\\alpha} - k^{1-\\alpha}\\right)\\)$</p> <ul> <li>For \\(j \\ge 1\\):</li> </ul> <p>$\\(c_j^{(k)} = (k-j+2)^{1-\\alpha} - 2(k-j+1)^{1-\\alpha} + (k-j)^{1-\\alpha}\\)$</p> <p>Key Characteristics:</p> <ul> <li>Formulation: Caputo.</li> <li>Accuracy: \\(O(h^{2-\\alpha})\\).</li> <li>Constraint: Single-term \\(\\alpha\\) only.</li> </ul>"},{"location":"api/solver/#spikeDE.solver.AdamsBashforthSNN","title":"AdamsBashforthSNN","text":"<p>               Bases: <code>SNNFractionalMethod</code></p> <p>Adams-Bashforth predictor method for single-term Caputo FDEs.</p> <p>This method serves as a predictor step in predictor-corrector schemes (like PECE). Unlike the other methods which convolve state history \\(y_j\\), Adams-Bashforth convolves the history of function evaluations \\(f(t_j, y_j)\\).</p> <p>The update rule is:</p> \\[y_{k+1} = \\sum_{j=0}^{k} b_{j,k+1} f(t_j, y_j)\\] <p>where the weights are:</p> \\[b_{j,k+1} = \\frac{h^\\alpha}{\\alpha \\Gamma(\\alpha)} \\left[ (k+1-j)^\\alpha - (k-j)^\\alpha \\right]\\] <p>Key Characteristics:</p> <ul> <li>Formulation: Caputo (Predictor).</li> <li>History Type: Stores \\(f(t, y)\\) instead of \\(y\\).</li> <li>Constraint: Single-term \\(\\alpha\\) only.</li> </ul>"},{"location":"api/solver/#spikeDE.solver.GrunwaldLetnikovMultitermSNN","title":"GrunwaldLetnikovMultitermSNN","text":"<pre><code>GrunwaldLetnikovMultitermSNN()\n</code></pre> <p>               Bases: <code>SNNFractionalMethod</code></p> <p>Unified Gr\u00fcnwald-Letnikov solver for multi-term Riemann-Liouville FDEs.</p> <p>This solver handles distributed-order or multi-term equations of the form:</p> \\[\\sum_{m=1}^{M} c_m D^{\\alpha_m} y(t) = f(t, y(t))\\] <p>It generalizes the single-term GL method by aggregating the coefficients from each term into a single effective convolution kernel.</p> <p>The discretization leads to the update rule:</p> \\[y_{k+1} = \\frac{1}{\\tilde{c}_0} \\left( f(t_k, y_k) - \\sum_{j=0}^{k} \\tilde{c}_{k-j} y_j \\right)\\] <p>where the aggregated coefficients \\(\\tilde{c}_m\\) are computed as:</p> \\[\\tilde{c}_m = \\sum_{i=1}^{M} c_i h^{-\\alpha_i} c_m^{(\\alpha_i)}\\] <p>Here, \\(c_i\\) are the user-defined equation coefficients, \\(h^{-\\alpha_i}\\) scales by step size, and \\(c_m^{(\\alpha_i)}\\) are the standard GL coefficients for order \\(\\alpha_i\\).</p> <p>Key Characteristics:</p> <ul> <li>Formulation: Riemann-Liouville (Multi-term).</li> <li>Flexibility: Supports both single-term (as a 1-term case) and multi-term layers.</li> <li>Gradient Flow: Fully differentiable with respect to \\(\\alpha_m\\) and coefficients \\(c_m\\).</li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def __init__(self):\n    self._c_tilde_per_layer: Optional[List[torch.Tensor]] = None\n    self._c_tilde_0_inv_per_layer: Optional[List[torch.Tensor]] = None\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.FDEAdjointMethod","title":"FDEAdjointMethod","text":"<p>               Bases: <code>Function</code></p> <p>Custom Autograd Function for Fractional Differential Equations with Adjoint Sensitivity.</p> <p>This class implements the forward and backward passes required for differentiating through FDE solvers. It supports various numerical schemes (GL, Trapezoidal, L1, Adams-Bashforth) and handles the complexity of fractional memory terms during backpropagation.</p> <p>Mathematical Formulation: The adjoint state \\(\\lambda(t)\\) satisfies the fractional adjoint equation:</p> \\[D^\\alpha \\lambda(t) = -\\left(\\frac{\\partial f}{\\partial y}\\right)^T \\lambda(t)\\] <p>solved backwards from \\(t=T\\) to \\(t=0\\). Parameter gradients are computed via:</p> \\[\\frac{dL}{d\\theta} = \\int_0^T \\lambda(t)^T \\frac{\\partial f}{\\partial \\theta} dt\\]"},{"location":"api/solver/#spikeDE.solver.FDEAdjointMethod.backward","title":"backward  <code>staticmethod</code>","text":"<pre><code>backward(ctx: FunctionCtx, *grad_output: Tensor) -&gt; tuple[Any | None, ...]\n</code></pre> <p>Performs the backward adjoint integration to compute gradients.</p> <p>Reconstructs the augmented dynamics system and solves it backwards in time to obtain gradients with respect to initial states (\\(y_0\\)) and model parameters (\\(\\theta\\)).</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object containing saved tensors from forward pass.</p> </li> <li> <code>*grad_output</code>               (<code>Tensor</code>, default:                   <code>()</code> )           \u2013            <p>Gradients of the loss with respect to the output states \\(y(t_{end})\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Any | None, ...]</code>           \u2013            <p>A tuple of gradients corresponding to the inputs of <code>forward</code>: <code>(grad_func, grad_n_state, grad_n_params, grad_y0..., grad_alpha, grad_t_grid, grad_method, grad_params..., grad_memory)</code>. Non-tensor inputs return <code>None</code>.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>@staticmethod\ndef backward(\n    ctx: torch.autograd.function.FunctionCtx, *grad_output: torch.Tensor\n) -&gt; Tuple[Optional[Any], ...]:\n    r\"\"\"\n    Performs the backward adjoint integration to compute gradients.\n\n    Reconstructs the augmented dynamics system and solves it backwards in time to obtain\n    gradients with respect to initial states ($y_0$) and model parameters ($\\theta$).\n\n    Args:\n        ctx: Context object containing saved tensors from forward pass.\n        *grad_output: Gradients of the loss with respect to the output states $y(t_{end})$.\n\n    Returns:\n        A tuple of gradients corresponding to the inputs of `forward`: `(grad_func, grad_n_state, grad_n_params, grad_y0..., grad_alpha, grad_t_grid, grad_method, grad_params..., grad_memory)`. Non-tensor inputs return `None`.\n    \"\"\"\n    # \u65e9\u9000\uff1a\u4e0d\u9700\u8981\u53cd\u4f20\u65f6\uff0c\u8fd4\u56de\u6b63\u786e\u6570\u91cf\u7684 None\n    if not hasattr(ctx, \"yhistory\"):\n        n_state = ctx.n_state\n        n_params = ctx.n_params\n        grads = []\n        grads.append(None)  # ode_func\n        grads.append(None)  # n_state\n        grads.append(None)  # n_params\n        grads.extend([None] * n_state)  # y0_1,...,y0_n\n        grads.append(None)  # alpha\n        grads.append(None)  # t_grid\n        grads.append(None)  # method\n        grads.extend([None] * n_params)  # p1,...,pm\n        grads.append(None)  # memory\n        return tuple(grads)\n\n    # \u6062\u590d\u4fdd\u5b58\u7684\u5f20\u91cf\u548c\u5c5e\u6027\n    t_grid = ctx.saved_tensors[0]\n    func_params = ctx.func_params\n    yhistory = ctx.yhistory\n    # yhistory is the last states for euler and the full history for other methods\n\n    func = ctx.ode_func\n    alpha = ctx.alpha\n    method = ctx.method\n    memory = ctx.memory\n    n_tensors = len(yhistory)\n\n    # \u521b\u5efa augmented dynamics\n    class AugDynamics:\n        def __init__(self, func, n_tensors, func_params):\n            self.func = func\n            self.n_tensors = n_tensors\n            self.f_params = func_params  # \u4f7f\u7528\u4f20\u5165\u7684\u53c2\u6570\n\n        def __call__(self, t, y_aug):\n            y, adj_y, adj_params = y_aug\n\n            with torch.set_grad_enabled(True):\n                # detach \u5e76\u8bbe\u7f6e requires_grad\n                y = tuple(y_.detach().requires_grad_(True) for y_ in y)\n                func_eval = self.func(t, y)\n\n                # \u8ba1\u7b97 VJP\n                vjp_y_and_params = torch.autograd.grad(\n                    func_eval,\n                    y + self.f_params,\n                    tuple(adj_y),\n                    allow_unused=True,\n                    retain_graph=False,  # \u4e0d\u4fdd\u7559\u56fe\n                    create_graph=False,\n                )\n\n            vjp_y = vjp_y_and_params[: self.n_tensors]\n            vjp_params = vjp_y_and_params[self.n_tensors :]\n\n            # \u5904\u7406 None \u68af\u5ea6\n            vjp_y = tuple(\n                torch.zeros_like(y_) if vjp_y_ is None else vjp_y_\n                for vjp_y_, y_ in zip(vjp_y, y)\n            )\n\n            vjp_params = tuple(\n                torch.zeros_like(p) if vp is None else vp\n                for vp, p in zip(vjp_params, self.f_params)\n            )\n\n            return (func_eval, vjp_y, vjp_params)\n\n    # \u521b\u5efa augmented dynamics \u5b9e\u4f8b\n    augmented_dynamics = AugDynamics(func, n_tensors, func_params)\n    t_grid_flip = t_grid.flip(0)\n\n    with torch.no_grad():\n        adj_y = grad_output\n\n        # \u521d\u59cb\u5316\u53c2\u6570\u68af\u5ea6\n        if func_params:\n            adj_params = tuple(torch.zeros_like(p) for p in func_params)\n        else:\n            adj_params = ()\n\n        # \u8bbe\u7f6e\u521d\u59cb\u589e\u5e7f\u72b6\u6001\n        # \u6ce8\u610f\uff1ayhistory \u4f5c\u4e3a\u521d\u59cb y \u72b6\u6001\n        aug_y0 = ([], adj_y, adj_params)\n\n        # \u8c03\u7528\u53cd\u5411\u6c42\u89e3\u5668\n        adj_y, adj_params = SOLVERS_Backward[method](\n            augmented_dynamics,\n            aug_y0,\n            alpha,\n            t_grid_flip,\n            yhistory,  # \u4f20\u9012\u6700\u7ec8\u72b6\u6001\n            memory,\n        )\n\n    # \u6e05\u7406\n    # del augmented_dynamics\n    # del ctx.yhistory\n\n    # \u5728\u6700\u540e\uff0c\u786e\u4fdd\u6e05\u7406\u6240\u6709\u5c40\u90e8\u53d8\u91cf\n    del augmented_dynamics\n    del yhistory  # \u4e5f\u8981\u5220\u9664\u5c40\u90e8\u53d8\u91cf\n    del func\n    del func_params\n    del ctx.yhistory\n    del ctx.ode_func\n    del ctx.func_params\n    del ctx.alpha\n    del ctx.method\n\n    # \u51c6\u5907\u8fd4\u56de\u503c\n    # \u8fd4\u56de\u683c\u5f0f\uff1a(grad_func, grad_y0_tuple, grad_alpha, grad_t_grid, grad_method, grad_func_params, grad_memory)\n    grad_y0 = adj_y\n    grad_params = adj_params\n\n    return None, None, None, *grad_y0, None, None, None, *grad_params, None\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.FDEAdjointMethod.forward","title":"forward  <code>staticmethod</code>","text":"<pre><code>forward(\n    ctx: FunctionCtx,\n    ode_func: Callable,\n    n_state: int,\n    n_params: int,\n    *args: Any,\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Performs the forward integration of the FDE.</p> <p>Unpacks arguments, selects the appropriate solver based on <code>method</code>, and computes the state trajectory. Saves necessary context for the backward pass.</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object to save tensors for backward pass.</p> </li> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>The ODE function \\(f(t, y)\\).</p> </li> <li> <code>n_state</code>               (<code>int</code>)           \u2013            <p>Number of state components in <code>y0_tuple</code>.</p> </li> <li> <code>n_params</code>               (<code>int</code>)           \u2013            <p>Number of learnable parameters in <code>ode_func</code>.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Packed arguments containing:</p> <ul> <li><code>y0_tuple</code>: Initial states (n_state tensors).</li> <li><code>alpha</code>: Fractional order.</li> <li><code>t_grid</code>: Time grid.</li> <li><code>method</code>: Solver method string.</li> <li><code>func_params</code>: Model parameters (n_params tensors).</li> <li><code>memory</code>: Memory truncation limit.</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, ...]</code>           \u2013            <p>A tuple of tensors representing the final state \\(y(t_{end})\\).</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>@staticmethod\ndef forward(\n    ctx: torch.autograd.function.FunctionCtx,\n    ode_func: Callable,\n    n_state: int,\n    n_params: int,\n    *args: Any,\n) -&gt; Tuple[torch.Tensor, ...]:\n    r\"\"\"\n    Performs the forward integration of the FDE.\n\n    Unpacks arguments, selects the appropriate solver based on `method`, and computes\n    the state trajectory. Saves necessary context for the backward pass.\n\n    Args:\n        ctx: Context object to save tensors for backward pass.\n        ode_func: The ODE function $f(t, y)$.\n        n_state: Number of state components in `y0_tuple`.\n        n_params: Number of learnable parameters in `ode_func`.\n        *args: Packed arguments containing:\n\n            - `y0_tuple`: Initial states (n_state tensors).\n            - `alpha`: Fractional order.\n            - `t_grid`: Time grid.\n            - `method`: Solver method string.\n            - `func_params`: Model parameters (n_params tensors).\n            - `memory`: Memory truncation limit.\n\n    Returns:\n        A tuple of tensors representing the final state $y(t_{end})$.\n    \"\"\"\n    n_state = int(n_state)\n    n_params = int(n_params)\n\n    # \u89e3\u6790\u4f4d\u7f6e\u53c2\u6570\uff1a y0_1,...,y0_n, alpha, t_grid, method, p1,...,pm, memory\n    y0_tuple = tuple(args[:n_state])  # Tensors\n    alpha = args[n_state]  # Tensor \u6216 float\uff08\u82e5\u8981\u5b66\u4e60\uff0c\u5fc5\u987b\u662f Tensor\uff09\n    t_grid = args[n_state + 1]  # Tensor\n    method = args[n_state + 2]  # str / enum\uff08\u975e Tensor\uff09\n    func_params = tuple(\n        args[n_state + 3 : n_state + 3 + n_params]\n    )  # Tensors (Parameters)\n    memory = args[n_state + 3 + n_params]  # \u4efb\u610f\u5bf9\u8c61\uff08\u975e Tensor\uff09\n\n    with torch.no_grad():\n        yhistory = SOLVERS_Forward[method](\n            ode_func=ode_func,\n            y0_tuple=y0_tuple,\n            alpha=alpha,\n            t_grid=t_grid,\n            memory=memory,\n        )\n\n    # \u68c0\u67e5\u662f\u5426\u9700\u8981\u68af\u5ea6\n    y0_needs_grad = any(t.requires_grad for t in y0_tuple)\n    params_need_grad = (\n        any(p.requires_grad for p in func_params) if func_params else False\n    )\n\n    ctx.n_state = n_state\n    ctx.n_params = n_params\n    if y0_needs_grad or params_need_grad:\n        # \u4fdd\u5b58\u5fc5\u8981\u7684\u5f20\u91cf\u7528\u4e8e\u53cd\u5411\u4f20\u64ad\n        # \u6ce8\u610f\uff1a\u4fdd\u5b58\u6574\u4e2a tuple \u5bf9\u8c61\uff0c\u800c\u4e0d\u662f\u5c55\u5f00\u7684\u5f20\u91cf\n        ctx.save_for_backward(t_grid)\n        ctx.func_params = func_params  # \u540c\u6837\u4fdd\u5b58\u4e3a\u5c5e\u6027\n        ctx.yhistory = yhistory  # \u4fdd\u5b58\u6700\u7ec8\u72b6\u6001\n        ctx.ode_func = ode_func\n        ctx.alpha = alpha\n        ctx.method = method\n        ctx.memory = memory\n\n    # \u8fd4\u56de\u7ed3\u679c\n    # if method == 'euler':\n    #     outs = tuple(yhistory)\n    # else:\n    #     # \u5982\u679c yhistory \u662f\u5d4c\u5957\u5217\u8868\uff0c\u53d6\u6700\u540e\u4e00\u4e2a\n    outs = tuple([y[-1] for y in yhistory])\n\n    return outs\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.snn_solve","title":"snn_solve","text":"<pre><code>snn_solve(\n    ode_func: Callable[[Tensor, Tuple], tuple],\n    y0_tuple: tuple[Tensor, ...],\n    per_layer_alpha: list[Any],\n    t_grid: Tensor,\n    method: SNNFractionalMethod,\n    memory: int | None = None,\n    per_layer_coefficient: list[Tensor | None] | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Unified driver function for solving SNN fractional differential equations.</p> <p>Orchestrates the time-stepping loop, managing state history, memory truncation, and dispatching to the specific numerical method provided.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable[[Tensor, Tuple], tuple]</code>)           \u2013            <p>Function <code>f(t, y_tuple)</code> returning derivatives.</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state tuple.</p> </li> <li> <code>per_layer_alpha</code>               (<code>list[Any]</code>)           \u2013            <p>List of fractional orders per layer.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time points tensor.</p> </li> <li> <code>method</code>               (<code>SNNFractionalMethod</code>)           \u2013            <p>Instance of <code>SNNFractionalMethod</code> (e.g., <code>GrunwaldLetnikovSNN</code>).</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional integer to limit history length for convolution.</p> </li> <li> <code>per_layer_coefficient</code>               (<code>list[Tensor | None] | None</code>, default:                   <code>None</code> )           \u2013            <p>Coefficients for multi-term layers.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>List of lists containing the trajectory of each state component.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def snn_solve(\n    ode_func: Callable[[torch.Tensor, Tuple], Tuple],\n    y0_tuple: Tuple[torch.Tensor, ...],\n    per_layer_alpha: List[Any],\n    t_grid: torch.Tensor,\n    method: SNNFractionalMethod,\n    memory: Optional[int] = None,\n    per_layer_coefficient: Optional[List[Optional[torch.Tensor]]] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Unified driver function for solving SNN fractional differential equations.\n\n    Orchestrates the time-stepping loop, managing state history, memory truncation,\n    and dispatching to the specific numerical method provided.\n\n    Args:\n        ode_func: Function `f(t, y_tuple)` returning derivatives.\n        y0_tuple: Initial state tuple.\n        per_layer_alpha: List of fractional orders per layer.\n        t_grid: Time points tensor.\n        method: Instance of `SNNFractionalMethod` (e.g., `GrunwaldLetnikovSNN`).\n        memory: Optional integer to limit history length for convolution.\n        per_layer_coefficient: Coefficients for multi-term layers.\n\n    Returns:\n        List of lists containing the trajectory of each state component.\n    \"\"\"\n    assert isinstance(y0_tuple, tuple), \"y0_tuple must be a tuple\"\n\n    # Create configuration with per-layer alpha\n    config = SNNSolverConfig.from_inputs(\n        y0_tuple, per_layer_alpha, t_grid, per_layer_coefficient\n    )\n\n    # Move t_grid to correct device/dtype\n    t_grid = t_grid.to(device=config.device, dtype=config.dtype)\n\n    # Initialize method\n    method.initialize(config)\n\n    # Initialize state\n    y_current = list(y0_tuple)\n\n    # History\n    y_history = [[] for _ in y0_tuple]\n\n    # For predictor method, we need f-history\n    if method.stores_f_history:\n        fhistory = [[] for _ in range(config.n_integrate)]\n    else:\n        fhistory = None\n\n    # Main loop\n    for k in range(config.N - 1):\n        t_k = t_grid[k]\n\n        # Evaluate f(t_k, y_k)\n        f_k = ode_func(t_k, tuple(y_current))\n\n        # Store function evaluations if needed\n        if method.stores_f_history:\n            for i in range(config.n_integrate):\n                fhistory[i].append(f_k[i])\n\n        # Determine memory range\n        start_idx, _ = get_memory_bounds(k, memory)\n\n        # Update each integrated component with its own alpha\n        for i in range(config.n_integrate):\n            # Compute weights for this specific layer\n            weights = method.compute_weights_for_layer(\n                k, start_idx, config, layer_idx=i\n            )\n\n            # Get appropriate history\n            history_i = fhistory[i] if method.stores_f_history else y_history[i]\n\n            # Compute convolution sum\n            convolution_sum = method.compute_convolution(\n                k, start_idx, weights, history_i, config, layer_idx=i\n            )\n\n            # Compute update for this layer\n            y_current[i] = method.compute_update_for_layer(\n                f_k[i], convolution_sum, config, layer_idx=i\n            )\n\n            # Store in history\n            if not method.stores_f_history:\n                y_history[i].append(y_current[i])\n\n        # Pass-through boundary output e.g. the final spike output\n        for i in range(config.n_integrate, config.n_components):\n            y_current[i] = f_k[i]\n            y_history[i].append(y_current[i])\n\n    # Cleanup\n    if fhistory is not None:\n        del fhistory\n\n    return y_history\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.euler_integrate_tuple","title":"euler_integrate_tuple","text":"<pre><code>euler_integrate_tuple(\n    ode_func: Callable[[Tensor, Tuple[Tensor, ...]], tuple[Tensor, ...]],\n    y0_tuple: tuple[Tensor, ...],\n    t_grid: Tensor,\n    neuron_count: int,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Performs standard explicit Euler integration for integer-order ODEs (\\(D^1 y = f(t, y)\\)).</p> <p>This function distinguishes between dynamic state variables (neurons) which are integrated, and boundary outputs (e.g., spike outputs) which are treated as pass-through values computed directly from the derivative without accumulation.</p> <p>The update rule for integrated components is:</p> \\[y_{k+1} = y_k + \\Delta t \\cdot f(t_k, y_k)\\] <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable[[Tensor, Tuple[Tensor, ...]], tuple[Tensor, ...]]</code>)           \u2013            <p>A callable <code>f(t, y_tuple)</code> returning a tuple of derivatives.       Expected format: <code>(dy_1, ..., dy_N, boundary_1, ...)</code>.</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>A tuple of initial state tensors <code>(y_1, ..., y_N, boundary_1, ...)</code>.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>A 1D tensor of time points <code>[t_0, t_1, ..., t_N]</code>. Step sizes can be non-uniform.</p> </li> <li> <code>neuron_count</code>               (<code>int</code>)           \u2013            <p>The number of components in the state tuple representing dynamic neurons           to be integrated. Components beyond this index are treated as pass-through boundaries.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>A list of lists, where <code>history[i][k]</code> is the state of component <code>i</code> at time step <code>k+1</code>.</p> </li> <li> <code>list[list[Tensor]]</code>           \u2013            <p>The length of each inner list is <code>len(t_grid) - 1</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>AssertionError</code>             \u2013            <p>If <code>y0_tuple</code> is not a tuple or <code>t_grid</code> has fewer than 2 points.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def euler_integrate_tuple(\n    ode_func: Callable[\n        [torch.Tensor, Tuple[torch.Tensor, ...]], Tuple[torch.Tensor, ...]\n    ],\n    y0_tuple: Tuple[torch.Tensor, ...],\n    t_grid: torch.Tensor,\n    neuron_count: int,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Performs standard explicit Euler integration for integer-order ODEs ($D^1 y = f(t, y)$).\n\n    This function distinguishes between dynamic state variables (neurons) which are integrated,\n    and boundary outputs (e.g., spike outputs) which are treated as pass-through values computed\n    directly from the derivative without accumulation.\n\n    The update rule for integrated components is:\n\n    $$y_{k+1} = y_k + \\Delta t \\cdot f(t_k, y_k)$$\n\n    Args:\n        ode_func: A callable `f(t, y_tuple)` returning a tuple of derivatives.\n                  Expected format: `(dy_1, ..., dy_N, boundary_1, ...)`.\n        y0_tuple: A tuple of initial state tensors `(y_1, ..., y_N, boundary_1, ...)`.\n        t_grid: A 1D tensor of time points `[t_0, t_1, ..., t_N]`. Step sizes can be non-uniform.\n        neuron_count: The number of components in the state tuple representing dynamic neurons\n                      to be integrated. Components beyond this index are treated as pass-through boundaries.\n\n    Returns:\n        A list of lists, where `history[i][k]` is the state of component `i` at time step `k+1`.\n        The length of each inner list is `len(t_grid) - 1`.\n\n    Raises:\n        AssertionError: If `y0_tuple` is not a tuple or `t_grid` has fewer than 2 points.\n    \"\"\"\n    assert isinstance(y0_tuple, tuple)\n    N = len(t_grid)\n    assert N &gt;= 2\n    device = y0_tuple[0].device\n    dtype = y0_tuple[0].dtype\n    t_grid = t_grid.to(device=device, dtype=dtype)\n\n    n_integrate = neuron_count\n    # n_integrate is the number of neurons,\n    # i.e., length of (dv1/dt, dv2/dt, ..., dvN/dt)\n    n_components = len(y0_tuple)\n    # length of (dv1/dt, dv2/dt, ..., dvN/dt, boundary_1, boundary_2, ...)\n\n    # Initialize history lists for each component\n    y_current = list(y0_tuple)\n    y_history = [[] for _ in y0_tuple]\n\n    # Euler integration: y_{k+1} = y_k + dt * f(t_k, y_k)\n    for k in range(N - 1):\n        tk = t_grid[k]\n        dt = t_grid[k + 1] - t_grid[k]  # Scalar tensor, will broadcast automatically\n        dy = ode_func(\n            tk, tuple(y_current)\n        )  # Expect tuple return, consistent with y structure\n        # assert isinstance(dy, tuple) and len(dy) == len(y)\n\n        # Update all integrated components except the last one\n        for i in range(n_integrate):\n            y_current[i] = y_current[i] + dt * dy[i]\n            y_history[i].append(y_current[i])\n\n        # Pass-through boundary output e.g. final spike output\n        # See odefunc_fx.md\n        for i in range(n_integrate, n_components):\n            y_current[i] = dy[i]\n            y_history[i].append(y_current[i])\n\n    return y_history\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.gl_integrate_tuple","title":"gl_integrate_tuple","text":"<pre><code>gl_integrate_tuple(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    per_layer_alpha: list[Any],\n    t_grid: Tensor,\n    memory: int | None = None,\n    per_layer_coefficient: list[Tensor | None] | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Solves FDEs using the Gr\u00fcnwald-Letnikov (GL) method with per-layer alpha support.</p> <p>Automatically switches to <code>GrunwaldLetnikovMultitermSNN</code> if any layer has multi-term alpha. Suitable for Riemann-Liouville formulations.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function <code>f(t, y_tuple)</code> returning derivatives.</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state tuple.</p> </li> <li> <code>per_layer_alpha</code>               (<code>list[Any]</code>)           \u2013            <p>List of alpha values, one per integrated component.              Each can be scalar (single-term) or list/tensor (multi-term).</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time points tensor.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional memory truncation length.</p> </li> <li> <code>per_layer_coefficient</code>               (<code>list[Tensor | None] | None</code>, default:                   <code>None</code> )           \u2013            <p>Coefficients for multi-term layers.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>List of lists containing the state trajectory.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def gl_integrate_tuple(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    per_layer_alpha: List[Any],\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n    per_layer_coefficient: Optional[List[Optional[torch.Tensor]]] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Solves FDEs using the Gr\u00fcnwald-Letnikov (GL) method with per-layer alpha support.\n\n    Automatically switches to `GrunwaldLetnikovMultitermSNN` if any layer has multi-term alpha.\n    Suitable for Riemann-Liouville formulations.\n\n    Args:\n        ode_func: Function `f(t, y_tuple)` returning derivatives.\n        y0_tuple: Initial state tuple.\n        per_layer_alpha: List of alpha values, one per integrated component.\n                         Each can be scalar (single-term) or list/tensor (multi-term).\n        t_grid: Time points tensor.\n        memory: Optional memory truncation length.\n        per_layer_coefficient: Coefficients for multi-term layers.\n\n    Returns:\n        List of lists containing the state trajectory.\n    \"\"\"\n    solver, _ = _get_solver_with_multiterm_fallback(\n        \"gl\", per_layer_alpha, per_layer_coefficient\n    )\n    return snn_solve(\n        ode_func,\n        y0_tuple,\n        per_layer_alpha,\n        t_grid,\n        solver,\n        memory=memory,\n        per_layer_coefficient=per_layer_coefficient,\n    )\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.trap_integrate_tuple","title":"trap_integrate_tuple","text":"<pre><code>trap_integrate_tuple(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    per_layer_alpha: list[Any],\n    t_grid: Tensor,\n    memory: int | None = None,\n    per_layer_coefficient: list[Tensor | None] | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Solves FDEs using the Product Trapezoidal method with per-layer alpha support.</p> Note <p>If any layer has multi-term alpha, automatically falls back to GL multiterm with a warning. Offers higher accuracy (\\(O(h^2)\\)) for single-term Riemann-Liouville equations.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function <code>f(t, y_tuple)</code> returning derivatives.</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state tuple.</p> </li> <li> <code>per_layer_alpha</code>               (<code>list[Any]</code>)           \u2013            <p>List of alpha values.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time points tensor.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional memory truncation length.</p> </li> <li> <code>per_layer_coefficient</code>               (<code>list[Tensor | None] | None</code>, default:                   <code>None</code> )           \u2013            <p>Coefficients for multi-term layers.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>List of lists containing the state trajectory.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def trap_integrate_tuple(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    per_layer_alpha: List[Any],\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n    per_layer_coefficient: Optional[List[Optional[torch.Tensor]]] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Solves FDEs using the Product Trapezoidal method with per-layer alpha support.\n\n    Note:\n        If any layer has multi-term alpha, automatically falls back to GL multiterm\n        with a warning. Offers higher accuracy ($O(h^2)$) for single-term Riemann-Liouville equations.\n\n    Args:\n        ode_func: Function `f(t, y_tuple)` returning derivatives.\n        y0_tuple: Initial state tuple.\n        per_layer_alpha: List of alpha values.\n        t_grid: Time points tensor.\n        memory: Optional memory truncation length.\n        per_layer_coefficient: Coefficients for multi-term layers.\n\n    Returns:\n        List of lists containing the state trajectory.\n    \"\"\"\n    solver, _ = _get_solver_with_multiterm_fallback(\n        \"trap\", per_layer_alpha, per_layer_coefficient\n    )\n    return snn_solve(\n        ode_func,\n        y0_tuple,\n        per_layer_alpha,\n        t_grid,\n        solver,\n        memory=memory,\n        per_layer_coefficient=per_layer_coefficient,\n    )\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.l1_integrate_tuple","title":"l1_integrate_tuple","text":"<pre><code>l1_integrate_tuple(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    per_layer_alpha: list[Any],\n    t_grid: Tensor,\n    memory: int | None = None,\n    per_layer_coefficient: list[Tensor | None] | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Solves FDEs using the L1 scheme with per-layer alpha support.</p> Note <p>If any layer has multi-term alpha, automatically falls back to GL multiterm with a warning. Commonly used for Caputo formulations with accuracy \\(O(h^{2-\\alpha})\\).</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function <code>f(t, y_tuple)</code> returning derivatives.</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state tuple.</p> </li> <li> <code>per_layer_alpha</code>               (<code>list[Any]</code>)           \u2013            <p>List of alpha values.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time points tensor.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional memory truncation length.</p> </li> <li> <code>per_layer_coefficient</code>               (<code>list[Tensor | None] | None</code>, default:                   <code>None</code> )           \u2013            <p>Coefficients for multi-term layers.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>List of lists containing the state trajectory.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def l1_integrate_tuple(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    per_layer_alpha: List[Any],\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n    per_layer_coefficient: Optional[List[Optional[torch.Tensor]]] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Solves FDEs using the L1 scheme with per-layer alpha support.\n\n    Note:\n        If any layer has multi-term alpha, automatically falls back to GL multiterm\n        with a warning. Commonly used for Caputo formulations with accuracy $O(h^{2-\\alpha})$.\n\n    Args:\n        ode_func: Function `f(t, y_tuple)` returning derivatives.\n        y0_tuple: Initial state tuple.\n        per_layer_alpha: List of alpha values.\n        t_grid: Time points tensor.\n        memory: Optional memory truncation length.\n        per_layer_coefficient: Coefficients for multi-term layers.\n\n    Returns:\n        List of lists containing the state trajectory.\n    \"\"\"\n    solver, _ = _get_solver_with_multiterm_fallback(\n        \"l1\", per_layer_alpha, per_layer_coefficient\n    )\n    return snn_solve(\n        ode_func,\n        y0_tuple,\n        per_layer_alpha,\n        t_grid,\n        solver,\n        memory=memory,\n        per_layer_coefficient=per_layer_coefficient,\n    )\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.pred_integrate_tuple","title":"pred_integrate_tuple","text":"<pre><code>pred_integrate_tuple(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    per_layer_alpha: list[Any],\n    t_grid: Tensor,\n    memory: int | None = None,\n    per_layer_coefficient: list[Tensor | None] | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Solves FDEs using the Adams-Bashforth predictor with per-layer alpha support.</p> Note <p>If any layer has multi-term alpha, automatically falls back to GL multiterm with a warning. Uses \\(f\\)-history instead of \\(y\\)-history.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function <code>f(t, y_tuple)</code> returning derivatives.</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state tuple.</p> </li> <li> <code>per_layer_alpha</code>               (<code>list[Any]</code>)           \u2013            <p>List of alpha values.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time points tensor.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional memory truncation length.</p> </li> <li> <code>per_layer_coefficient</code>               (<code>list[Tensor | None] | None</code>, default:                   <code>None</code> )           \u2013            <p>Coefficients for multi-term layers.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>List of lists containing the state trajectory.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def pred_integrate_tuple(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    per_layer_alpha: List[Any],\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n    per_layer_coefficient: Optional[List[Optional[torch.Tensor]]] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Solves FDEs using the Adams-Bashforth predictor with per-layer alpha support.\n\n    Note:\n        If any layer has multi-term alpha, automatically falls back to GL multiterm\n        with a warning. Uses $f$-history instead of $y$-history.\n\n    Args:\n        ode_func: Function `f(t, y_tuple)` returning derivatives.\n        y0_tuple: Initial state tuple.\n        per_layer_alpha: List of alpha values.\n        t_grid: Time points tensor.\n        memory: Optional memory truncation length.\n        per_layer_coefficient: Coefficients for multi-term layers.\n\n    Returns:\n        List of lists containing the state trajectory.\n    \"\"\"\n    solver, _ = _get_solver_with_multiterm_fallback(\n        \"pred\", per_layer_alpha, per_layer_coefficient\n    )\n    return snn_solve(\n        ode_func,\n        y0_tuple,\n        per_layer_alpha,\n        t_grid,\n        solver,\n        memory=memory,\n        per_layer_coefficient=per_layer_coefficient,\n    )\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.fdeint_adjoint","title":"fdeint_adjoint","text":"<pre><code>fdeint_adjoint(\n    func: Callable[[Tensor, Tuple[Tensor, ...]], tuple[Tensor, ...]],\n    y0_tuple: tuple[Tensor, ...],\n    alpha: float | Tensor | list[float],\n    t_grid: Tensor,\n    method: str,\n    memory: int | None = None,\n) -&gt; tuple[Tensor, ...]\n</code></pre> <p>Solves a Fractional Differential Equation (FDE) with adjoint sensitivity analysis.</p> <p>This function enables gradient-based optimization of both the initial states \\(y_0\\) and the parameters of the ODE function <code>func</code> (e.g., neural network weights) with respect to a loss function defined on the solution trajectory. It uses the continuous adjoint method adapted for fractional calculus.</p> <p>The workflow involves:</p> <ol> <li>Forward Pass: Solving \\(D^\\alpha y(t) = f(t, y(t), \\theta)\\) to obtain \\(y(T)\\).</li> <li>Backward Pass: Solving the augmented adjoint equation to compute \\(\\frac{\\partial L}{\\partial y_0}\\)    and \\(\\frac{\\partial L}{\\partial \\theta}\\).</li> </ol> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable[[Tensor, Tuple[Tensor, ...]], tuple[Tensor, ...]]</code>)           \u2013            <p>The ODE function \\(f(t, y, \\theta)\\). Must accept <code>(t, y_tuple)</code> and return a tuple of tensors.   Parameters \\(\\theta\\) are implicitly captured from the function's scope or registered modules.</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>A tuple of initial state tensors \\((y_1^0, \\dots, y_N^0)\\).</p> </li> <li> <code>alpha</code>               (<code>float | Tensor | list[float]</code>)           \u2013            <p>The fractional order(s). Can be a scalar, a tensor, or a list depending on the solver configuration.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>A 1D tensor of time points \\([t_0, t_1, \\dots, t_T]\\) defining the integration interval.</p> </li> <li> <code>method</code>               (<code>str</code>)           \u2013            <p>The numerical integration scheme identifier (e.g., <code>'gl-f'</code>, <code>'trap-f'</code>, <code>'l1-f'</code>).     Suffixes <code>-f</code> indicate full history storage required for adjoint, <code>-o</code> for optimized/no-history.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional integer to limit the memory length for convolution sums (short-memory principle).     If <code>None</code>, full history is used.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, ...]</code>           \u2013            <p>A tuple of tensors representing the solution at the final time point \\(y(t_T)\\), compatible with <code>torch.autograd</code> for backpropagation.</p> </li> </ul> Note <p>This function wraps <code>FDEAdjointMethod.apply</code>. Ensure <code>func</code> contains parameters that require gradients if parameter optimization is desired.</p> Source code in <code>spikeDE/solver.py</code> <pre><code>def fdeint_adjoint(\n    func: Callable[[torch.Tensor, Tuple[torch.Tensor, ...]], Tuple[torch.Tensor, ...]],\n    y0_tuple: Tuple[torch.Tensor, ...],\n    alpha: Union[float, torch.Tensor, List[float]],\n    t_grid: torch.Tensor,\n    method: str,\n    memory: Optional[int] = None,\n) -&gt; Tuple[torch.Tensor, ...]:\n    r\"\"\"\n    Solves a Fractional Differential Equation (FDE) with adjoint sensitivity analysis.\n\n    This function enables gradient-based optimization of both the initial states $y_0$ and\n    the parameters of the ODE function `func` (e.g., neural network weights) with respect to\n    a loss function defined on the solution trajectory. It uses the continuous adjoint method\n    adapted for fractional calculus.\n\n    The workflow involves:\n\n    1. **Forward Pass**: Solving $D^\\alpha y(t) = f(t, y(t), \\theta)$ to obtain $y(T)$.\n    2. **Backward Pass**: Solving the augmented adjoint equation to compute $\\frac{\\partial L}{\\partial y_0}$\n       and $\\frac{\\partial L}{\\partial \\theta}$.\n\n    Args:\n        func: The ODE function $f(t, y, \\theta)$. Must accept `(t, y_tuple)` and return a tuple of tensors.\n              Parameters $\\theta$ are implicitly captured from the function's scope or registered modules.\n        y0_tuple: A tuple of initial state tensors $(y_1^0, \\dots, y_N^0)$.\n        alpha: The fractional order(s). Can be a scalar, a tensor, or a list depending on the solver configuration.\n        t_grid: A 1D tensor of time points $[t_0, t_1, \\dots, t_T]$ defining the integration interval.\n        method: The numerical integration scheme identifier (e.g., `'gl-f'`, `'trap-f'`, `'l1-f'`).\n                Suffixes `-f` indicate full history storage required for adjoint, `-o` for optimized/no-history.\n        memory: Optional integer to limit the memory length for convolution sums (short-memory principle).\n                If `None`, full history is used.\n\n    Returns:\n        A tuple of tensors representing the solution at the final time point $y(t_T)$, compatible with `torch.autograd` for backpropagation.\n\n    Note:\n        This function wraps `FDEAdjointMethod.apply`. Ensure `func` contains parameters that require gradients if parameter optimization is desired.\n    \"\"\"\n    # params = tuple(p for p in func.parameters())  # \u6216\u53ea\u53d6 requires_grad=True \u7684\n    params = find_parameters(func)\n    n_state = len(y0_tuple)\n    n_params = len(params)\n    # \u6ce8\u610f\uff1aapply \u4e0d\u63a5\u53d7\u5173\u952e\u5b57\u53c2\u6570\uff1b\u628a\u8ba1\u6570\u653e\u5728\u524d\u4e24\u4f4d\u6700\u7b80\u5355\n    return FDEAdjointMethod.apply(\n        func, n_state, n_params, *y0_tuple, alpha, t_grid, method, *params, memory\n    )\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.forward_euler_wo_history","title":"forward_euler_wo_history","text":"<pre><code>forward_euler_wo_history(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    alpha: Any,\n    t_grid: Tensor,\n    memory: int | None = None,\n) -&gt; list[Tensor]\n</code></pre> <p>Explicit Euler integration without storing full history.</p> <p>Solves \\(y_{k+1} = y_k + h \\cdot f(t_k, y_k)\\). This variant is memory-efficient (\\(O(1)\\)) but insufficient for methods requiring history-dependent adjoints unless combined with checkpointing. Used primarily for integer-order baselines or specific optimized paths.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function \\(f(t, y)\\).</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state tuple.</p> </li> <li> <code>alpha</code>               (<code>Any</code>)           \u2013            <p>Fractional order (unused in standard Euler, kept for signature compatibility).</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time grid tensor.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Unused.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Tensor]</code>           \u2013            <p>List of final state tensors (not a list of lists).</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def forward_euler_wo_history(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    alpha: Any,\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n) -&gt; List[torch.Tensor]:\n    r\"\"\"\n    Explicit Euler integration without storing full history.\n\n    Solves $y_{k+1} = y_k + h \\cdot f(t_k, y_k)$.\n    This variant is memory-efficient ($O(1)$) but insufficient for methods requiring\n    history-dependent adjoints unless combined with checkpointing. Used primarily for\n    integer-order baselines or specific optimized paths.\n\n    Args:\n        ode_func: Function $f(t, y)$.\n        y0_tuple: Initial state tuple.\n        alpha: Fractional order (unused in standard Euler, kept for signature compatibility).\n        t_grid: Time grid tensor.\n        memory: Unused.\n\n    Returns:\n        List of final state tensors (not a list of lists).\n    \"\"\"\n    assert isinstance(y0_tuple, tuple)\n    N = len(t_grid)\n    assert N &gt;= 2\n    device = y0_tuple[0].device\n    dtype = y0_tuple[0].dtype\n    t_grid = t_grid.to(device=device, dtype=dtype)\n\n    # Initialize history lists for each component\n\n    # Clone initial values\n    y = list(y0_tuple)\n\n    # Euler integration: y_{k+1} = y_k + dt * f(t_k, y_k)\n    for k in range(N - 1):\n        tk = t_grid[k]\n        dt = t_grid[k + 1] - t_grid[k]  # Scalar tensor, will broadcast automatically\n        dy = ode_func(tk, tuple(y))  # Expect tuple return, consistent with y structure\n        # assert isinstance(dy, tuple) and len(dy) == len(y)\n        # Update all integrated components except the last one\n        # for i in range(len(y)):\n        #     y[i] = y[i] + dt * dy[i]\n        for i in range(len(y)):\n            y[i].add_(dy[i], alpha=dt)\n    return y\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.backward_euler_wo_history","title":"backward_euler_wo_history","text":"<pre><code>backward_euler_wo_history(\n    ode_func: Callable,\n    y_aug: tuple[list, list, list],\n    alpha: Any,\n    t_grid: Tensor,\n    y_finalstate: list[Tensor],\n    memory: int | None = None,\n) -&gt; tuple[list[Tensor], list[Tensor]]\n</code></pre> <p>Backward integration for Euler method without full history dependency.</p> <p>Since Euler has no memory term, the backward pass simply integrates the adjoint equation using the reconstructed forward trajectory (or re-evaluation).</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Augmented dynamics function.</p> </li> <li> <code>y_aug</code>               (<code>tuple[list, list, list]</code>)           \u2013            <p>Initial augmented state <code>(dummy_y, adj_y0, adj_params0)</code>.</p> </li> <li> <code>alpha</code>               (<code>Any</code>)           \u2013            <p>Unused.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Flipped time grid.</p> </li> <li> <code>y_finalstate</code>               (<code>list[Tensor]</code>)           \u2013            <p>Final state from forward pass (used as starting point for reconstruction if needed).</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Unused.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[Tensor], list[Tensor]]</code>           \u2013            <p>Tuple of <code>(final_adj_y, final_adj_params)</code>.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def backward_euler_wo_history(\n    ode_func: Callable,\n    y_aug: Tuple[List, List, List],\n    alpha: Any,\n    t_grid: torch.Tensor,\n    y_finalstate: List[torch.Tensor],\n    memory: Optional[int] = None,\n) -&gt; Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    r\"\"\"\n    Backward integration for Euler method without full history dependency.\n\n    Since Euler has no memory term, the backward pass simply integrates the adjoint\n    equation using the reconstructed forward trajectory (or re-evaluation).\n\n    Args:\n        ode_func: Augmented dynamics function.\n        y_aug: Initial augmented state `(dummy_y, adj_y0, adj_params0)`.\n        alpha: Unused.\n        t_grid: Flipped time grid.\n        y_finalstate: Final state from forward pass (used as starting point for reconstruction if needed).\n        memory: Unused.\n\n    Returns:\n        Tuple of `(final_adj_y, final_adj_params)`.\n    \"\"\"\n    with torch.no_grad():\n        N = len(t_grid)\n        h = torch.abs((t_grid[-1] - t_grid[-2]))  # uniform step size\n        h = float(h)\n\n        _, adj_y0, adj_params0 = y_aug\n        # \u521d\u59cb\u5316\n        # y_state = [x.detach().clone() for x in y_finalstate]\n        # adj_y = [x.detach().clone() for x in adj_y0]\n        # adj_params = tuple(p.detach().clone() for p in adj_params0) if adj_params0 else ()\n        y_state = list(y_finalstate)\n        adj_y = list(adj_y0)\n        adj_params = list(adj_params0)\n\n        # return [x.detach().clone() for x in adj_y0], [x.detach().clone() for x in adj_params0]\n\n        for k in range(N - 1):\n            tk = t_grid[k]\n\n            # \u8c03\u7528 augmented dynamics\n            func_eval, vjp_y, vjp_params = ode_func(tk, (y_state, adj_y, adj_params))\n\n            # \u66f4\u65b0\u72b6\u6001\n            for i in range(len(adj_y)):\n                # adj_y[i] = adj_y[i] + h * vjp_y[i]\n                # y_state[i] = y_state[i] - h * func_eval[i]\n                adj_y[i].add_(vjp_y[i], alpha=h)\n                y_state[i].add_(func_eval[i], alpha=-h)  # \u6ce8\u610f\u8fd9\u91cc\u662f -h\uff08\u51cf\u6cd5\uff09\n\n            # \u66f4\u65b0\u53c2\u6570\u68af\u5ea6\n            if adj_params and vjp_params:\n                for ap, vp in zip(adj_params, vjp_params):\n                    ap.add_(vp, alpha=h)  # \u76f4\u63a5\u4fee\u6539 tuple \u4e2d\u7684\u5f20\u91cf\n\n    return adj_y, adj_params\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.forward_euler_w_history","title":"forward_euler_w_history","text":"<pre><code>forward_euler_w_history(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    alpha: Any,\n    t_grid: Tensor,\n    memory: int | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Explicit Euler integration storing full history.</p> <p>Required for adjoint methods that expect a history list structure consistent with fractional solvers, even if the method itself is memory-less.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function \\(f(t, y)\\).</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state tuple.</p> </li> <li> <code>alpha</code>               (<code>Any</code>)           \u2013            <p>Unused.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time grid.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Unused.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>List of lists, where each inner list contains the trajectory of one state component.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def forward_euler_w_history(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    alpha: Any,\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Explicit Euler integration storing full history.\n\n    Required for adjoint methods that expect a history list structure consistent with\n    fractional solvers, even if the method itself is memory-less.\n\n    Args:\n        ode_func: Function $f(t, y)$.\n        y0_tuple: Initial state tuple.\n        alpha: Unused.\n        t_grid: Time grid.\n        memory: Unused.\n\n    Returns:\n        List of lists, where each inner list contains the trajectory of one state component.\n    \"\"\"\n    assert isinstance(y0_tuple, tuple)\n    N = len(t_grid)\n    assert N &gt;= 2\n    device = y0_tuple[0].device\n    dtype = y0_tuple[0].dtype\n    t_grid = t_grid.to(device=device, dtype=dtype)\n\n    # Initialize history lists for each component\n\n    # Clone initial values\n    y_current = list(y0_tuple)\n    y_history = [[] for _ in y0_tuple]\n\n    # Euler integration: y_{k+1} = y_k + dt * f(t_k, y_k)\n    for k in range(N - 1):\n        tk = t_grid[k]\n        dt = t_grid[k + 1] - t_grid[k]  # Scalar tensor, will broadcast automatically\n        dy = ode_func(\n            tk, tuple(y_current)\n        )  # Expect tuple return, consistent with y structure\n        # assert isinstance(dy, tuple) and len(dy) == len(y)\n\n        # Update all integrated components except the last one\n        for i in range(len(y_current)):\n            y_current[i] = y_current[i] + dt * dy[i]\n            # Final element is the output spike (pass-through)\n            y_history[i].append(y_current[i])\n    return y_history\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.backward_euler_w_history","title":"backward_euler_w_history","text":"<pre><code>backward_euler_w_history(\n    ode_func: Callable,\n    y_aug: tuple[list, list, list],\n    alpha: Any,\n    t_grid: Tensor,\n    yhistory: list[list[Tensor]],\n    memory: int | None = None,\n) -&gt; tuple[list[Tensor], list[Tensor]]\n</code></pre> <p>Backward integration for Euler method using stored history.</p> <p>Iterates backwards through the provided <code>yhistory</code> to compute adjoint updates.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Augmented dynamics.</p> </li> <li> <code>y_aug</code>               (<code>tuple[list, list, list]</code>)           \u2013            <p>Initial augmented state.</p> </li> <li> <code>alpha</code>               (<code>Any</code>)           \u2013            <p>Unused.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Flipped time grid.</p> </li> <li> <code>yhistory</code>               (<code>list[list[Tensor]]</code>)           \u2013            <p>Full forward trajectory.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Unused.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[Tensor], list[Tensor]]</code>           \u2013            <p>Tuple of <code>(final_adj_y, final_adj_params)</code>.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def backward_euler_w_history(\n    ode_func: Callable,\n    y_aug: Tuple[List, List, List],\n    alpha: Any,\n    t_grid: torch.Tensor,\n    yhistory: List[List[torch.Tensor]],\n    memory: Optional[int] = None,\n) -&gt; Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    r\"\"\"\n    Backward integration for Euler method using stored history.\n\n    Iterates backwards through the provided `yhistory` to compute adjoint updates.\n\n    Args:\n        ode_func: Augmented dynamics.\n        y_aug: Initial augmented state.\n        alpha: Unused.\n        t_grid: Flipped time grid.\n        yhistory: Full forward trajectory.\n        memory: Unused.\n\n    Returns:\n        Tuple of `(final_adj_y, final_adj_params)`.\n    \"\"\"\n    with torch.no_grad():\n        N = len(t_grid)\n        h = torch.abs((t_grid[-1] - t_grid[-2]))  # uniform step size\n        h = float(h)\n\n        _, adj_y0, adj_params0 = y_aug\n\n        adj_y = list(adj_y0)  # [y0.clone() for y0 in y0_tuple]\n        adj_params = list(adj_params0)\n\n        # return tuple(y_i.clone() for y_i in adj_y0), tuple(y_i.clone() for y_i in adj_params0)\n\n        # for k in range(N - 1):\n        #     tk = t_grid[k]\n        #     y_state = list([y[-1-k] for y in yhistory])\n        for k in range(N - 2):\n            tk = t_grid[k + 1]\n            # t_grid_flip = t_grid.flip(0) recal that t has been flipped already\n            # y_state = list([y[-k - 1] for y in yhistory])\n            y_state = list([y[-k - 2] for y in yhistory])\n            # \u8c03\u7528 augmented dynamics\n            func_eval, vjp_y, vjp_params = ode_func(tk, (y_state, adj_y, adj_params))\n            # vjp_y = tuple(torch.zeros_like(y_i) for y_i in adj_y)\n            # vjp_params = tuple(torch.zeros_like(y_i) for y_i in adj_params)\n            # \u66f4\u65b0\u72b6\u6001\n            # for i in range(len(adj_y)):\n            #     adj_y[i] = adj_y[i] + h * vjp_y[i]\n            for i in range(len(adj_y)):\n                adj_y[i].add_(vjp_y[i], alpha=h)\n\n            # \u66f4\u65b0\u53c2\u6570\u68af\u5ea6\n            # if adj_params and vjp_params:\n            #     adj_params = tuple(\n            #         ap + h * vp for ap, vp in zip(adj_params, vjp_params)\n            #     )\n            if adj_params and vjp_params:\n                for ap, vp in zip(adj_params, vjp_params):\n                    ap.add_(vp, alpha=h)  # \u76f4\u63a5\u4fee\u6539 tuple \u4e2d\u7684\u5f20\u91cf\n\n    del yhistory, vjp_params, func_eval, vjp_y\n\n    return adj_y, adj_params\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.forward_gl","title":"forward_gl","text":"<pre><code>forward_gl(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    alpha: float | Tensor,\n    t_grid: Tensor,\n    memory: int | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Forward Gr\u00fcnwald-Letnikov (GL) integration.</p> <p>Implements the Riemann-Liouville approximation:</p> \\[y_{k+1} = h^\\alpha f(t_k, y_k) - \\sum_{j=0}^{k} c_{k-j}^{(\\alpha)} y_j\\] <p>where coefficients \\(c_j^{(\\alpha)}\\) are computed recursively.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function \\(f(t, y)\\).</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state tuple.</p> </li> <li> <code>alpha</code>               (<code>float | Tensor</code>)           \u2013            <p>Fractional order \\(\\alpha \\in (0, 1)\\).</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Uniform time grid.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Max history length for truncation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>List of lists containing the state trajectory.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def forward_gl(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    alpha: Union[float, torch.Tensor],\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Forward Gr\u00fcnwald-Letnikov (GL) integration.\n\n    Implements the Riemann-Liouville approximation:\n\n    $$y_{k+1} = h^\\alpha f(t_k, y_k) - \\sum_{j=0}^{k} c_{k-j}^{(\\alpha)} y_j$$\n\n    where coefficients $c_j^{(\\alpha)}$ are computed recursively.\n\n    Args:\n        ode_func: Function $f(t, y)$.\n        y0_tuple: Initial state tuple.\n        alpha: Fractional order $\\alpha \\in (0, 1)$.\n        t_grid: Uniform time grid.\n        memory: Max history length for truncation.\n\n    Returns:\n        List of lists containing the state trajectory.\n    \"\"\"\n    assert isinstance(y0_tuple, tuple)\n    N = len(t_grid)\n    assert N &gt;= 2\n    device = y0_tuple[0].device\n    dtype = y0_tuple[0].dtype\n    t_grid = t_grid.to(device=device, dtype=dtype)\n    h = t_grid[-1] - t_grid[-2]  # uniform step size\n    h_alpha = torch.pow(h, alpha)\n\n    # GL coefficients: need up to c[N]\n    c = torch.zeros(N + 1, dtype=dtype, device=device)\n    c[0] = 1\n    for j in range(1, N + 1):\n        c[j] = (1 - (1 + alpha) / j) * c[j - 1]\n\n    # Initialize with y_0 (clone to avoid modifying input)\n    y_current = list(y0_tuple)  # tuple(y.clone() for y in y0_tuple)\n    # History: y_history[i][j] stores y_j for component i\n    # Initialize with y_0\n    # y_history = [[y, ] for y in y0_tuple]\n    y_history = [[] for y in y0_tuple]\n    # y_history = [[y.clone()] for y in y0_tuple]\n\n    for k in range(N - 1):\n        t_k = t_grid[k]\n        # Evaluate f(t_k, y_k)\n\n        dy = ode_func(t_k, tuple(y_current))\n        # assert isinstance(dy, tuple) and len(dy) == len(y_current)\n\n        # Determine memory range\n        if memory is None or memory == -1:\n            memory_length = k + 1  # Use all available history\n        else:\n            memory_length = min(memory, k + 1)\n\n        start_idx = max(0, k + 1 - memory_length)\n\n        # Update all integrated components\n        for i in range(len(y_current)):\n            # Accumulate: \u03a3 c_{k+1-j} * y_j for j from start_idx to k\n            if k &gt; 0:\n                convolution_sum = 0  # torch.zeros_like(y[i])\n                for j in range(start_idx, k):\n                    # GL coefficient for this lag\n                    # convolution_sum = convolution_sum + c[k+1-j] * y_history[i][j]\n                    convolution_sum = convolution_sum + c[k - j] * y_history[i][j]\n                # here we assume at time k, we have k elements (without y0=0)\n                # the most restrict formulation should be convolution_sum + c[k-j] * y_history[i][j]\n                # which however seems do have have good numerical stability\n            else:\n                convolution_sum = 0\n\n            # convolution_sum = None\n            # for j in range(start_idx, k+1):\n            # # # GL coefficient for this lag\n            #     if convolution_sum is None:\n            #         convolution_sum = c[k+1-j] * y_history[i][j]\n            #     else:\n            #         convolution_sum = convolution_sum + c[k+1-j] * y_history[i][j]\n\n            # y_{k+1} = h^alpha * f_k - convolution_sum\n            y_current[i] = h_alpha * dy[i] - convolution_sum\n            y_history[i].append(y_current[i])\n\n    return y_history\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.backward_gl","title":"backward_gl","text":"<pre><code>backward_gl(\n    ode_func: Callable,\n    y_aug: tuple[list, list, list],\n    alpha: float | Tensor,\n    t_grid: Tensor,\n    yhistory: list[list[Tensor]],\n    memory: int | None = None,\n) -&gt; tuple[list[Tensor], list[Tensor]]\n</code></pre> <p>Backward Gr\u00fcnwald-Letnikov integration for adjoint sensitivity.</p> <p>Solves the adjoint equation using the same GL discretization structure, accumulating gradients from the future (which is the past in reversed time).</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Augmented dynamics.</p> </li> <li> <code>y_aug</code>               (<code>tuple[list, list, list]</code>)           \u2013            <p>Initial augmented state (at reversed \\(t=0\\), i.e., forward \\(t=T\\)).</p> </li> <li> <code>alpha</code>               (<code>float | Tensor</code>)           \u2013            <p>Fractional order.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Flipped time grid.</p> </li> <li> <code>yhistory</code>               (<code>list[list[Tensor]]</code>)           \u2013            <p>Forward trajectory (accessed in reverse).</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Memory truncation limit.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[Tensor], list[Tensor]]</code>           \u2013            <p>Tuple of <code>(final_adj_y, final_adj_params)</code>.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def backward_gl(\n    ode_func: Callable,\n    y_aug: Tuple[List, List, List],\n    alpha: Union[float, torch.Tensor],\n    t_grid: torch.Tensor,\n    yhistory: List[List[torch.Tensor]],\n    memory: Optional[int] = None,\n) -&gt; Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    r\"\"\"\n    Backward Gr\u00fcnwald-Letnikov integration for adjoint sensitivity.\n\n    Solves the adjoint equation using the same GL discretization structure,\n    accumulating gradients from the future (which is the past in reversed time).\n\n    Args:\n        ode_func: Augmented dynamics.\n        y_aug: Initial augmented state (at reversed $t=0$, i.e., forward $t=T$).\n        alpha: Fractional order.\n        t_grid: Flipped time grid.\n        yhistory: Forward trajectory (accessed in reverse).\n        memory: Memory truncation limit.\n\n    Returns:\n        Tuple of `(final_adj_y, final_adj_params)`.\n    \"\"\"\n    with torch.no_grad():\n        N = len(t_grid)\n        h = torch.abs(t_grid[-1] - t_grid[-2])\n        h_alpha = torch.pow(h, alpha)\n\n        _, adj_y0, adj_params0 = y_aug\n        device = adj_y0[0].device\n        dtype = adj_y0[0].dtype\n\n        # GL coefficients\n        c = torch.zeros(N + 1, dtype=dtype, device=device)\n        c[0] = 1\n        for j in range(1, N + 1):\n            c[j] = (1 - (1 + alpha) / j) * c[j - 1]\n\n        # Initialize adjoint history lists for each component\n        adjy_history = [\n            [\n                xx,\n            ]\n            for xx in adj_y0\n        ]\n\n        # Clone initial adjoint values\n        adj_y = list(adj_y0)\n        adj_params = list(adj_params0)\n\n        for k in range(N - 2):\n            tk = t_grid[k + 1]\n            # t_grid_flip = t_grid.flip(0) recal that t has been flipped already\n            # y_state = list([y[-k - 1] for y in yhistory])\n            y_state = list([y[-k - 2] for y in yhistory])\n\n            func_eval, vjp_y, vjp_params = ode_func(tk, (y_state, adj_y, adj_params))\n\n            # Determine memory range\n            if memory is None or memory == -1:\n                memory_length = k + 1  # Use all available history\n            else:\n                memory_length = min(memory, k + 1)\n\n            start_idx = max(0, k + 1 - memory_length)\n\n            # Update all adjoint components\n            for i in range(len(adj_y)):\n                # Calculate history sum\n\n                if True:  # k &gt; 0:\n                    convolution_sum = 0  # torch.zeros_like(y[i])\n                    for j in range(start_idx, k + 1):\n                        # GL coefficient for this lag\n                        convolution_sum = (\n                            convolution_sum + c[k + 1 - j] * adjy_history[i][j]\n                        )\n                    # here we assume at time k, we have k elements (without y0=0)\n                    # the most restrict formulation should be convolution_sum + c[k-j] * y_history[i][j]\n                    # which however seems do have have good numerical stability\n                else:\n                    convolution_sum = 0\n\n                # convolution_sum = None\n                # for j in range(start_idx, k+1):\n                #     # # GL coefficient for this lag\n                #     if convolution_sum is None:\n                #         convolution_sum = c[k + 1 - j] * adjy_history[i][j]\n                #     else:\n                #         convolution_sum = convolution_sum + c[k + 1  - j] * adjy_history[i][j]\n\n                # Update adjoint state\n                adj_y[i] = h_alpha * vjp_y[i] - convolution_sum\n                adjy_history[i].append(adj_y[i])\n\n            # \u66f4\u65b0\u53c2\u6570\u68af\u5ea6\n            if adj_params and vjp_params:\n                for ap, vp in zip(adj_params, vjp_params):\n                    ap.add_(vp, alpha=h)  # \u76f4\u63a5\u4fee\u6539 tuple \u4e2d\u7684\u5f20\u91cf\n\n    del adjy_history, yhistory\n\n    return adj_y, adj_params\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.forward_trap","title":"forward_trap","text":"<pre><code>forward_trap(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    alpha: float | Tensor,\n    t_grid: Tensor,\n    memory: int | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Forward Product Trapezoidal method.</p> <p>Provides \\(O(h^2)\\) accuracy for Riemann-Liouville FDEs. Formula:</p> \\[y_{k+1} = \\frac{h^\\alpha}{\\Gamma(2-\\alpha)} f_k - \\sum_{j=0}^{k} A_{j,k+1} y_j\\] <p>where weights \\(A_{j,k+1}\\) depend on the distance from the current step.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function \\(f(t, y)\\).</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state.</p> </li> <li> <code>alpha</code>               (<code>float | Tensor</code>)           \u2013            <p>Fractional order.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time grid.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Memory limit.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>State trajectory.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def forward_trap(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    alpha: Union[float, torch.Tensor],\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Forward Product Trapezoidal method.\n\n    Provides $O(h^2)$ accuracy for Riemann-Liouville FDEs.\n    Formula:\n\n    $$y_{k+1} = \\frac{h^\\alpha}{\\Gamma(2-\\alpha)} f_k - \\sum_{j=0}^{k} A_{j,k+1} y_j$$\n\n    where weights $A_{j,k+1}$ depend on the distance from the current step.\n\n    Args:\n        ode_func: Function $f(t, y)$.\n        y0_tuple: Initial state.\n        alpha: Fractional order.\n        t_grid: Time grid.\n        memory: Memory limit.\n\n    Returns:\n        State trajectory.\n    \"\"\"\n    assert isinstance(y0_tuple, tuple)\n\n    N = len(t_grid)\n    device = y0_tuple[0].device\n    dtype = y0_tuple[0].dtype\n    t_grid = t_grid.to(device=device, dtype=dtype)\n\n    h = t_grid[-1] - t_grid[-2]  # uniform step size\n    h_alpha_gamma = torch.pow(h, alpha) * math.gamma(2 - alpha)\n    one_minus_alpha = 1 - alpha\n\n    # Initialize with y_0\n    y_current = list(y0_tuple)\n    y_history = [[] for y in y0_tuple]\n\n    # Main loop: compute y_{k+1} for k = 0, 1, ..., N-2\n    for k in range(N - 1):\n        t_k = t_grid[k]\n\n        # Evaluate f(t_k, y_k)\n        f_k = ode_func(t_k, tuple(y_current))\n\n        # Determine memory range\n        if memory is None:\n            memory_length = k + 1\n        else:\n            memory_length = min(memory, k + 1)\n            assert memory_length &gt; 0, \"memory must be greater than 0\"\n\n        start_idx = max(0, k + 1 - memory_length)\n\n        # Compute A_{j,k+1} weights for indices from start_idx to k\n        j_vals = torch.arange(start_idx, k + 1, dtype=dtype, device=device)\n\n        # General formula for j &gt;= 1:\n        # A_{j,k+1} = (k+2-j)^{1-\u03b1} + (k-j)^{1-\u03b1} - 2(k+1-j)^{1-\u03b1}\n        kjp2 = torch.pow(k + 2 - j_vals, one_minus_alpha)\n        kj = torch.pow(k - j_vals, one_minus_alpha)\n        kjp1 = torch.pow(k + 1 - j_vals, one_minus_alpha)\n        A_j_kp1 = kjp2 + kj - 2 * kjp1\n\n        # Special handling for j=0 if it's in the range:\n        # A_{0,k+1} = k^{1-\u03b1} - (k+\u03b1)(k+1)^{-\u03b1}\n        if start_idx == 0:\n            k_power = torch.pow(\n                torch.tensor(k, dtype=dtype, device=device), one_minus_alpha\n            )\n            kp1_neg_alpha = torch.pow(\n                torch.tensor(k + 1, dtype=dtype, device=device), -alpha\n            )\n            A_j_kp1[0] = k_power - (k + alpha) * kp1_neg_alpha\n\n        # Update ALL state components (forward integrates all, not len-1)\n        for i in range(len(y_current)):\n            # Compute convolution sum: sum_{j=start_idx}^{k-1} A_{j,k+1} * y_j[i]\n            if k &gt; 0:\n                convolution_sum = 0\n                for j in range(start_idx, k):\n                    # local_idx = j - start_idx\n                    local_idx = j - start_idx + 1\n                    # the most restrict formulation should be local_idx = j - start_idx + 1\n                    convolution_sum = (\n                        convolution_sum + A_j_kp1[local_idx] * y_history[i][j]\n                    )\n            else:\n                convolution_sum = 0\n\n            # y_{k+1} = \u0393(2-\u03b1) * h^\u03b1 * f_k - convolution_sum\n            y_current[i] = h_alpha_gamma * f_k[i] - convolution_sum\n            y_history[i].append(y_current[i])\n\n    return y_history\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.backward_trap","title":"backward_trap","text":"<pre><code>backward_trap(\n    ode_func: Callable,\n    y_aug: tuple[list, list, list],\n    alpha: float | Tensor,\n    t_grid: Tensor,\n    yhistory: list[list[Tensor]],\n    memory: int | None = None,\n) -&gt; tuple[list[Tensor], list[Tensor]]\n</code></pre> <p>Backward Product Trapezoidal method for adjoint sensitivity.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Augmented dynamics.</p> </li> <li> <code>y_aug</code>               (<code>tuple[list, list, list]</code>)           \u2013            <p>Initial augmented state.</p> </li> <li> <code>alpha</code>               (<code>float | Tensor</code>)           \u2013            <p>Fractional order.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Flipped time grid.</p> </li> <li> <code>yhistory</code>               (<code>list[list[Tensor]]</code>)           \u2013            <p>Forward trajectory.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Memory limit.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[Tensor], list[Tensor]]</code>           \u2013            <p>Final adjoint states and parameter gradients.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def backward_trap(\n    ode_func: Callable,\n    y_aug: Tuple[List, List, List],\n    alpha: Union[float, torch.Tensor],\n    t_grid: torch.Tensor,\n    yhistory: List[List[torch.Tensor]],\n    memory: Optional[int] = None,\n) -&gt; Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    r\"\"\"\n    Backward Product Trapezoidal method for adjoint sensitivity.\n\n    Args:\n        ode_func: Augmented dynamics.\n        y_aug: Initial augmented state.\n        alpha: Fractional order.\n        t_grid: Flipped time grid.\n        yhistory: Forward trajectory.\n        memory: Memory limit.\n\n    Returns:\n        Final adjoint states and parameter gradients.\n    \"\"\"\n    with torch.no_grad():\n        N = len(t_grid)\n        h = torch.abs(t_grid[-1] - t_grid[-2])\n        h_alpha_gamma = torch.pow(h, alpha) * math.gamma(2 - alpha)\n        one_minus_alpha = 1 - alpha\n\n        _, adj_y0, adj_params0 = y_aug\n        device = adj_y0[0].device\n        dtype = adj_y0[0].dtype\n\n        # Initialize adjoint history lists for each component (with initial values)\n        adjy_history = [\n            [\n                xx,\n            ]\n            for xx in adj_y0\n        ]\n\n        # Clone initial adjoint values\n        adj_y = list(adj_y0)\n        adj_params = list(adj_params0)\n\n        for k in range(N - 2):\n            tk = t_grid[k + 1]\n            # y_state = list([y[-k - 1] for y in yhistory])\n            y_state = list([y[-k - 2] for y in yhistory])\n\n            func_eval, vjp_y, vjp_params = ode_func(tk, (y_state, adj_y, adj_params))\n\n            # Determine memory range\n            if memory is None:\n                memory_length = k + 1\n            else:\n                memory_length = min(memory, k + 1)\n\n            start_idx = max(0, k + 1 - memory_length)\n\n            # Compute A_{j,k+1} weights for indices from start_idx to k\n            j_vals = torch.arange(start_idx, k + 1, dtype=dtype, device=device)\n\n            # General formula for j &gt;= 1:\n            # A_{j,k+1} = (k+2-j)^{1-\u03b1} + (k-j)^{1-\u03b1} - 2(k+1-j)^{1-\u03b1}\n            kjp2 = torch.pow(k + 2 - j_vals, one_minus_alpha)\n            kj = torch.pow(k - j_vals, one_minus_alpha)\n            kjp1 = torch.pow(k + 1 - j_vals, one_minus_alpha)\n            A_j_kp1 = kjp2 + kj - 2 * kjp1\n\n            # Special handling for j=0 if it's in the range:\n            # A_{0,k+1} = k^{1-\u03b1} - (k+\u03b1)(k+1)^{-\u03b1}\n            if start_idx == 0:\n                k_power = torch.pow(\n                    torch.tensor(k, dtype=dtype, device=device), one_minus_alpha\n                )\n                kp1_neg_alpha = torch.pow(\n                    torch.tensor(k + 1, dtype=dtype, device=device), -alpha\n                )\n                A_j_kp1[0] = k_power - (k + alpha) * kp1_neg_alpha\n\n            # Update all adjoint components\n            for i in range(len(adj_y)):\n                # Calculate history sum - note: range goes to k+1 (one more than forward)\n                convolution_sum = 0\n                for j in range(start_idx, k + 1):\n                    local_idx = j - start_idx\n                    convolution_sum = (\n                        convolution_sum + A_j_kp1[local_idx] * adjy_history[i][j]\n                    )\n\n                # Update adjoint state\n                adj_y[i] = h_alpha_gamma * vjp_y[i] - convolution_sum\n                adjy_history[i].append(adj_y[i])\n\n            # Update parameter gradients\n            if adj_params and vjp_params:\n                for ap, vp in zip(adj_params, vjp_params):\n                    ap.add_(vp, alpha=h)\n\n    del adjy_history, yhistory\n\n    return adj_y, adj_params\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.forward_l1","title":"forward_l1","text":"<pre><code>forward_l1(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    alpha: float | Tensor,\n    t_grid: Tensor,\n    memory: int | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Forward L1 scheme for Caputo FDEs.</p> <p>Accuracy \\(O(h^{2-\\alpha})\\). Formula:</p> \\[y_{k+1} = \\frac{h^\\alpha}{\\Gamma(2-\\alpha)} f_k - \\sum_{j=0}^{k} c_j^{(k)} y_j\\] <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function \\(f(t, y)\\).</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state.</p> </li> <li> <code>alpha</code>               (<code>float | Tensor</code>)           \u2013            <p>Fractional order.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time grid.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Memory limit.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>State trajectory.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def forward_l1(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    alpha: Union[float, torch.Tensor],\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Forward L1 scheme for Caputo FDEs.\n\n    Accuracy $O(h^{2-\\alpha})$.\n    Formula:\n\n    $$y_{k+1} = \\frac{h^\\alpha}{\\Gamma(2-\\alpha)} f_k - \\sum_{j=0}^{k} c_j^{(k)} y_j$$\n\n    Args:\n        ode_func: Function $f(t, y)$.\n        y0_tuple: Initial state.\n        alpha: Fractional order.\n        t_grid: Time grid.\n        memory: Memory limit.\n\n    Returns:\n        State trajectory.\n    \"\"\"\n    assert isinstance(y0_tuple, tuple)\n    N = len(t_grid)\n    device = y0_tuple[0].device\n    dtype = y0_tuple[0].dtype\n    t_grid = t_grid.to(device=device, dtype=dtype)\n    h = t_grid[-1] - t_grid[-2]  # uniform step size\n    h_alpha_gamma = torch.pow(h, alpha) * math.gamma(2 - alpha)\n    one_minus_alpha = 1 - alpha\n\n    # Initialize history lists for each component\n    y_history = [[] for _ in y0_tuple]\n    # Current state\n    y_current = list(y0_tuple)\n\n    # Main loop: compute y_{k+1} for k = 0, 1, ..., N-2\n    for k in range(N - 1):\n        t_k = t_grid[k]\n        # Evaluate f(t_k, y_k)\n        f_k = ode_func(t_k, y_current)\n\n        # Determine memory range\n        if memory is None:\n            memory_length = k + 1  # Use all available history\n        else:\n            memory_length = min(memory, k + 1)\n            assert memory_length &gt; 0, \"memory must be greater than 0\"\n\n        start_idx = max(0, k + 1 - memory_length)\n\n        # Compute c_j^(k) weights for indices from start_idx to k\n        # General formula for j &gt;= 1: c_j^(k) = (k-j+2)^{1-\u03b1} - 2(k-j+1)^{1-\u03b1} + (k-j)^{1-\u03b1}\n        # Special case for j = 0: c_0^(k) = -[(k+1)^{1-\u03b1} - k^{1-\u03b1}]\n        j_vals = torch.arange(start_idx, k + 1, dtype=dtype, device=device)\n        kjp2 = torch.pow(k + 2 - j_vals, one_minus_alpha)\n        kjp1 = torch.pow(k + 1 - j_vals, one_minus_alpha)\n        kj = torch.pow(k - j_vals, one_minus_alpha)\n        c_j_k = kjp2 - 2 * kjp1 + kj\n\n        # Special handling for j=0 if it's in the range\n        if start_idx == 0:\n            kp1_power = torch.pow(\n                torch.tensor(k + 1, dtype=dtype, device=device), one_minus_alpha\n            )\n            k_power = torch.pow(\n                torch.tensor(k, dtype=dtype, device=device), one_minus_alpha\n            )\n            c_j_k[0] = -(kp1_power - k_power)\n\n        # Update ALL state components (forward integrates all, not len-1)\n        for i in range(len(y_current)):\n            # Compute convolution sum: sum_{j=start_idx}^{k-1} c_j^(k) * y_j[i]\n            if k &gt; 0:\n                convolution_sum = 0\n                for j in range(start_idx, k):\n                    # local_idx = j - start_idx\n                    local_idx = j - start_idx + 1\n                    # the most restrict formulation should be local_idx = j - start_idx + 1\n                    convolution_sum = (\n                        convolution_sum + c_j_k[local_idx] * y_history[i][j]\n                    )\n            else:\n                convolution_sum = 0\n\n            # y_{k+1} = h^\u03b1 * \u0393(2-\u03b1) * f_k - convolution_sum\n            y_current[i] = h_alpha_gamma * f_k[i] - convolution_sum\n            y_history[i].append(y_current[i])\n\n    return y_history\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.backward_l1","title":"backward_l1","text":"<pre><code>backward_l1(\n    ode_func: Callable,\n    y_aug: tuple[list, list, list],\n    alpha: float | Tensor,\n    t_grid: Tensor,\n    yhistory: list[list[Tensor]],\n    memory: int | None = None,\n) -&gt; tuple[list[Tensor], list[Tensor]]\n</code></pre> <p>Backward L1 scheme for adjoint sensitivity.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Augmented dynamics.</p> </li> <li> <code>y_aug</code>               (<code>tuple[list, list, list]</code>)           \u2013            <p>Initial augmented state.</p> </li> <li> <code>alpha</code>               (<code>float | Tensor</code>)           \u2013            <p>Fractional order.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Flipped time grid.</p> </li> <li> <code>yhistory</code>               (<code>list[list[Tensor]]</code>)           \u2013            <p>Forward trajectory.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Memory limit.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[Tensor], list[Tensor]]</code>           \u2013            <p>Final adjoint states and parameter gradients.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def backward_l1(\n    ode_func: Callable,\n    y_aug: Tuple[List, List, List],\n    alpha: Union[float, torch.Tensor],\n    t_grid: torch.Tensor,\n    yhistory: List[List[torch.Tensor]],\n    memory: Optional[int] = None,\n) -&gt; Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    r\"\"\"\n    Backward L1 scheme for adjoint sensitivity.\n\n    Args:\n        ode_func: Augmented dynamics.\n        y_aug: Initial augmented state.\n        alpha: Fractional order.\n        t_grid: Flipped time grid.\n        yhistory: Forward trajectory.\n        memory: Memory limit.\n\n    Returns:\n        Final adjoint states and parameter gradients.\n    \"\"\"\n    with torch.no_grad():\n        N = len(t_grid)\n        h = torch.abs(t_grid[-1] - t_grid[-2])\n        h_alpha_gamma = torch.pow(h, alpha) * math.gamma(2 - alpha)\n        one_minus_alpha = 1 - alpha\n\n        _, adj_y0, adj_params0 = y_aug\n        device = adj_y0[0].device\n        dtype = adj_y0[0].dtype\n\n        # Initialize adjoint history lists for each component (with initial values)\n        adjy_history = [\n            [\n                xx,\n            ]\n            for xx in adj_y0\n        ]\n\n        # Clone initial adjoint values\n        adj_y = list(adj_y0)\n        adj_params = list(adj_params0)\n\n        for k in range(N - 2):\n            tk = t_grid[k + 1]\n            y_state = list([y[-k - 2] for y in yhistory])\n\n            func_eval, vjp_y, vjp_params = ode_func(tk, (y_state, adj_y, adj_params))\n\n            # Determine memory range\n            if memory is None:\n                memory_length = k + 1\n            else:\n                memory_length = min(memory, k + 1)\n\n            start_idx = max(0, k + 1 - memory_length)\n\n            # Compute c_j^(k) weights for indices from start_idx to k\n            # General formula for j &gt;= 1: c_j^(k) = (k-j+2)^{1-\u03b1} - 2(k-j+1)^{1-\u03b1} + (k-j)^{1-\u03b1}\n            # Special case for j = 0: c_0^(k) = -[(k+1)^{1-\u03b1} - k^{1-\u03b1}]\n            j_vals = torch.arange(start_idx, k + 1, dtype=dtype, device=device)\n            kjp2 = torch.pow(k + 2 - j_vals, one_minus_alpha)\n            kjp1 = torch.pow(k + 1 - j_vals, one_minus_alpha)\n            kj = torch.pow(k - j_vals, one_minus_alpha)\n            c_j_k = kjp2 - 2 * kjp1 + kj\n\n            # Special handling for j=0 if it's in the range\n            if start_idx == 0:\n                kp1_power = torch.pow(\n                    torch.tensor(k + 1, dtype=dtype, device=device), one_minus_alpha\n                )\n                k_power = torch.pow(\n                    torch.tensor(k, dtype=dtype, device=device), one_minus_alpha\n                )\n                c_j_k[0] = -(kp1_power - k_power)\n\n            # Update all adjoint components\n            for i in range(len(adj_y)):\n                # Calculate history sum - note: range goes to k+1 (one more than forward)\n                convolution_sum = 0\n                for j in range(start_idx, k + 1):\n                    local_idx = j - start_idx\n                    convolution_sum = (\n                        convolution_sum + c_j_k[local_idx] * adjy_history[i][j]\n                    )\n\n                # Update adjoint state\n                adj_y[i] = h_alpha_gamma * vjp_y[i] - convolution_sum\n                adjy_history[i].append(adj_y[i])\n\n            # Update parameter gradients\n            if adj_params and vjp_params:\n                for ap, vp in zip(adj_params, vjp_params):\n                    ap.add_(vp, alpha=h)\n\n    del adjy_history, yhistory\n\n    return adj_y, adj_params\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.forward_pred","title":"forward_pred","text":"<pre><code>forward_pred(\n    ode_func: Callable,\n    y0_tuple: tuple[Tensor, ...],\n    alpha: float | Tensor,\n    t_grid: Tensor,\n    memory: int | None = None,\n) -&gt; list[list[Tensor]]\n</code></pre> <p>Forward Adams-Bashforth predictor method.</p> <p>Uses history of function evaluations \\(f(t, y)\\) instead of states \\(y\\). Formula:</p> \\[y_{k+1} = \\sum_{j=0}^{k} b_{j,k+1} f(t_j, y_j)\\] <p>where \\(b_{j,k+1} = \\frac{h^\\alpha}{\\alpha \\Gamma(\\alpha)} [(k+1-j)^\\alpha - (k-j)^\\alpha]\\).</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Function \\(f(t, y)\\).</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Initial state.</p> </li> <li> <code>alpha</code>               (<code>float | Tensor</code>)           \u2013            <p>Fractional order.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Time grid.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Memory limit.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[list[Tensor]]</code>           \u2013            <p>State trajectory.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def forward_pred(\n    ode_func: Callable,\n    y0_tuple: Tuple[torch.Tensor, ...],\n    alpha: Union[float, torch.Tensor],\n    t_grid: torch.Tensor,\n    memory: Optional[int] = None,\n) -&gt; List[List[torch.Tensor]]:\n    r\"\"\"\n    Forward Adams-Bashforth predictor method.\n\n    Uses history of function evaluations $f(t, y)$ instead of states $y$.\n    Formula:\n\n    $$y_{k+1} = \\sum_{j=0}^{k} b_{j,k+1} f(t_j, y_j)$$\n\n    where $b_{j,k+1} = \\frac{h^\\alpha}{\\alpha \\Gamma(\\alpha)} [(k+1-j)^\\alpha - (k-j)^\\alpha]$.\n\n    Args:\n        ode_func: Function $f(t, y)$.\n        y0_tuple: Initial state.\n        alpha: Fractional order.\n        t_grid: Time grid.\n        memory: Memory limit.\n\n    Returns:\n        State trajectory.\n    \"\"\"\n    assert isinstance(y0_tuple, tuple)\n    N = len(t_grid)\n    device = y0_tuple[0].device\n    dtype = y0_tuple[0].dtype\n    t_grid = t_grid.to(device=device, dtype=dtype)\n    h = t_grid[-1] - t_grid[-2]  # uniform step size\n    # gamma_alpha = 1 / math.gamma(alpha)\n    h_alpha_over_alpha = torch.pow(h, alpha) / (alpha * math.gamma(alpha))\n\n    # Initialize history lists for each component\n    y_history = [[] for _ in y0_tuple]\n    # History for function evaluations (for ALL components in forward)\n    fhistory = [[] for _ in y0_tuple]\n    # Current state\n    y_current = list(y0_tuple)\n\n    # Main loop: compute y_{k+1} for k = 0, 1, ..., N-2\n    for k in range(N - 1):\n        t_k = t_grid[k]\n        # Evaluate f(t_k, y_k)\n        f_k = ode_func(t_k, y_current)\n\n        # Store function evaluations for ALL components\n        for i in range(len(y_current)):\n            fhistory[i].append(f_k[i])\n\n        # Determine memory range\n        if memory is None:\n            memory_length = k + 1\n        else:\n            memory_length = min(memory, k + 1)\n            assert memory_length &gt; 0, \"memory must be greater than 0\"\n\n        start_idx = max(0, k + 1 - memory_length)\n\n        # Compute weights b_{j,k+1} for indices from start_idx to k\n        # b_{j,k+1} = (h^\u03b1 / \u03b1) * [(k+1-j)^\u03b1 - (k-j)^\u03b1]\n        j_vals = torch.arange(start_idx, k + 1, dtype=dtype, device=device)\n        b_j_kp1 = h_alpha_over_alpha * (\n            torch.pow(k + 1 - j_vals, alpha) - torch.pow(k - j_vals, alpha)\n        )\n\n        # Update ALL state components\n        for i in range(len(y_current)):\n            # Compute convolution sum: sum_{j=start_idx}^{k} b_{j,k+1} * f_j[i]\n            convolution_sum = 0\n            for j in range(start_idx, k + 1):\n                local_idx = j - start_idx\n                convolution_sum = convolution_sum + b_j_kp1[local_idx] * fhistory[i][j]\n\n            # y_{k+1} = (1/\u0393(\u03b1)) * convolution_sum\n            y_current[i] = convolution_sum\n            y_history[i].append(y_current[i])\n\n    del fhistory\n    return y_history\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.backward_pred","title":"backward_pred","text":"<pre><code>backward_pred(\n    ode_func: Callable,\n    y_aug: tuple[list, list, list],\n    alpha: float | Tensor,\n    t_grid: Tensor,\n    yhistory: list[list[Tensor]],\n    memory: int | None = None,\n) -&gt; tuple[list[Tensor], list[Tensor]]\n</code></pre> <p>Backward Adams-Bashforth predictor method.</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable</code>)           \u2013            <p>Augmented dynamics.</p> </li> <li> <code>y_aug</code>               (<code>tuple[list, list, list]</code>)           \u2013            <p>Initial augmented state.</p> </li> <li> <code>alpha</code>               (<code>float | Tensor</code>)           \u2013            <p>Fractional order.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>Flipped time grid.</p> </li> <li> <code>yhistory</code>               (<code>list[list[Tensor]]</code>)           \u2013            <p>Forward trajectory.</p> </li> <li> <code>memory</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Memory limit.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[Tensor], list[Tensor]]</code>           \u2013            <p>Final adjoint states and parameter gradients.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def backward_pred(\n    ode_func: Callable,\n    y_aug: Tuple[List, List, List],\n    alpha: Union[float, torch.Tensor],\n    t_grid: torch.Tensor,\n    yhistory: List[List[torch.Tensor]],\n    memory: Optional[int] = None,\n) -&gt; Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    r\"\"\"\n    Backward Adams-Bashforth predictor method.\n\n    Args:\n        ode_func: Augmented dynamics.\n        y_aug: Initial augmented state.\n        alpha: Fractional order.\n        t_grid: Flipped time grid.\n        yhistory: Forward trajectory.\n        memory: Memory limit.\n\n    Returns:\n        Final adjoint states and parameter gradients.\n    \"\"\"\n    with torch.no_grad():\n        N = len(t_grid)\n        h = torch.abs(t_grid[-1] - t_grid[-2])\n        # gamma_alpha = 1 / math.gamma(alpha)\n        h_alpha_over_alpha = torch.pow(h, alpha) / (alpha * math.gamma(alpha))\n\n        _, adj_y0, adj_params0 = y_aug\n        device = adj_y0[0].device\n        dtype = adj_y0[0].dtype\n\n        # Initialize adjoint history lists with initial values\n        adjf_history = [\n            [\n                xx,\n            ]\n            for xx in adj_y0\n        ]\n\n        # Clone initial adjoint values\n        adj_y = list(adj_y0)\n        adj_params = list(adj_params0)\n\n        for k in range(N - 2):\n            tk = t_grid[k + 1]\n            y_state = list([y[-k - 2] for y in yhistory])\n\n            func_eval, vjp_y, vjp_params = ode_func(tk, (y_state, adj_y, adj_params))\n\n            # Store adjoint of function evaluation\n            for i in range(len(adj_y)):\n                adjf_history[i].append(vjp_y[i])\n\n            # Determine memory range\n            if memory is None:\n                memory_length = k + 1\n            else:\n                memory_length = min(memory, k + 1)\n\n            start_idx = max(0, k + 1 - memory_length)\n\n            # Compute weights b_{j,k+1}\n            # b_{j,k+1} = (h^\u03b1 / \u03b1) * [(k+1-j)^\u03b1 - (k-j)^\u03b1]\n            j_vals = torch.arange(start_idx, k + 1, dtype=dtype, device=device)\n            b_j_kp1 = h_alpha_over_alpha * (\n                torch.pow(k + 1 - j_vals, alpha) - torch.pow(k - j_vals, alpha)\n            )\n\n            # Update all adjoint components\n            for i in range(len(adj_y)):\n                # Compute convolution sum over adjoint history\n                convolution_sum = 0\n                for j in range(start_idx, k + 1):\n                    local_idx = j - start_idx\n                    convolution_sum = (\n                        convolution_sum + b_j_kp1[local_idx] * adjf_history[i][j]\n                    )\n\n                # Update adjoint state\n                adj_y[i] = adj_y0[i] + convolution_sum\n\n            # Update parameter gradients\n            if adj_params and vjp_params:\n                for ap, vp in zip(adj_params, vjp_params):\n                    ap.add_(vp, alpha=h)\n\n    del adjf_history, yhistory\n\n    return adj_y, adj_params\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.find_parameters","title":"find_parameters","text":"<pre><code>find_parameters(module: Module) -&gt; list[Tensor]\n</code></pre> <p>Extracts all trainable parameters from a PyTorch module.</p> <p>Handles special cases such as <code>DataParallel</code> replicas where parameters might not be registered in the standard <code>.parameters()</code> iterator.</p> <p>Parameters:</p> <ul> <li> <code>module</code>               (<code>Module</code>)           \u2013            <p>The <code>nn.Module</code> to inspect.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Tensor]</code>           \u2013            <p>A list of <code>torch.Tensor</code> parameters that require gradients.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def find_parameters(module: nn.Module) -&gt; List[torch.Tensor]:\n    r\"\"\"\n    Extracts all trainable parameters from a PyTorch module.\n\n    Handles special cases such as `DataParallel` replicas where parameters might not\n    be registered in the standard `.parameters()` iterator.\n\n    Args:\n        module: The `nn.Module` to inspect.\n\n    Returns:\n        A list of `torch.Tensor` parameters that require gradients.\n    \"\"\"\n\n    assert isinstance(module, nn.Module)\n\n    # If called within DataParallel, parameters won't appear in module.parameters().\n    if getattr(module, \"_is_replica\", False):\n\n        def find_tensor_attributes(module):\n            tuples = [\n                (k, v)\n                for k, v in module.__dict__.items()\n                if torch.is_tensor(v) and v.requires_grad\n            ]\n            return tuples\n\n        gen = module._named_members(get_members_fn=find_tensor_attributes)\n        return [param for _, param in gen]\n    else:\n        return list(module.parameters())\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.get_memory_bounds","title":"get_memory_bounds","text":"<pre><code>get_memory_bounds(k: int, memory: int | None) -&gt; tuple[int, int]\n</code></pre> <p>Calculates the range of history indices to include in the convolution sum.</p> <p>Supports memory truncation for long sequences to reduce computational complexity from \\(O(N^2)\\) to \\(O(N \\cdot M)\\), where \\(M\\) is the memory length.</p> <p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Current time step index.</p> </li> <li> <code>memory</code>               (<code>int | None</code>)           \u2013            <p>Maximum number of history steps to retain. If <code>None</code> or <code>-1</code>, uses full history.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int]</code>           \u2013            <p>A tuple <code>(start_idx, memory_length)</code> defining the slice of history to use.</p> <ul> <li><code>start_idx</code>: The starting index in the history list;</li> <li><code>memory_length</code>: The number of elements to include.</li> </ul> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def get_memory_bounds(k: int, memory: Optional[int]) -&gt; Tuple[int, int]:\n    r\"\"\"\n    Calculates the range of history indices to include in the convolution sum.\n\n    Supports memory truncation for long sequences to reduce computational complexity\n    from $O(N^2)$ to $O(N \\cdot M)$, where $M$ is the memory length.\n\n    Args:\n        k: Current time step index.\n        memory: Maximum number of history steps to retain. If `None` or `-1`, uses full history.\n\n    Returns:\n        A tuple `(start_idx, memory_length)` defining the slice of history to use.\n\n            - `start_idx`: The starting index in the history list;\n            - `memory_length`: The number of elements to include.\n    \"\"\"\n    if memory is None or memory == -1:\n        memory_length = k + 1\n    else:\n        memory_length = min(memory, k + 1)\n        assert memory_length &gt; 0, \"memory must be greater than 0\"\n\n    start_idx = max(0, k + 1 - memory_length)\n    return start_idx, memory_length\n</code></pre>"},{"location":"api/solver/#spikeDE.solver.step_dynamics","title":"step_dynamics","text":"<pre><code>step_dynamics(\n    ode_func: Callable[[Tensor, Tuple], tuple],\n    y0_tuple: tuple[Tensor, ...],\n    t_grid: Tensor,\n) -&gt; list[Tensor]\n</code></pre> <p>Steps through a discrete-time dynamical system, collecting boundary outputs.</p> <p>This function drives the for-loop of an SNN/RNN without numerical integration scaling (no \\(dt\\)). The update function directly computes the next state: \\(y_{k+1} = f(t_k, y_k)\\).</p> <p>Parameters:</p> <ul> <li> <code>ode_func</code>               (<code>Callable[[Tensor, Tuple], tuple]</code>)           \u2013            <p>Callable <code>(t, y_tuple) -&gt; tuple</code>. State update function.</p> </li> <li> <code>y0_tuple</code>               (<code>tuple[Tensor, ...]</code>)           \u2013            <p>Tuple of initial state tensors.</p> </li> <li> <code>t_grid</code>               (<code>Tensor</code>)           \u2013            <p>1D tensor of time points (length T+1).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Tensor]</code>           \u2013            <p>List of spike outputs (last component of state) at each time step.</p> </li> </ul> Source code in <code>spikeDE/solver.py</code> <pre><code>def step_dynamics(\n    ode_func: Callable[[torch.Tensor, Tuple], Tuple],\n    y0_tuple: Tuple[torch.Tensor, ...],\n    t_grid: torch.Tensor,\n) -&gt; List[torch.Tensor]:\n    r\"\"\"\n    Steps through a discrete-time dynamical system, collecting boundary outputs.\n\n    This function drives the for-loop of an SNN/RNN without numerical integration scaling (no $dt$).\n    The update function directly computes the next state: $y_{k+1} = f(t_k, y_k)$.\n\n    Args:\n        ode_func: Callable `(t, y_tuple) -&gt; tuple`. State update function.\n        y0_tuple: Tuple of initial state tensors.\n        t_grid: 1D tensor of time points (length T+1).\n\n    Returns:\n        List of spike outputs (last component of state) at each time step.\n    \"\"\"\n    assert isinstance(y0_tuple, tuple)\n    N = len(t_grid)\n    assert N &gt;= 2\n\n    device = y0_tuple[0].device\n    dtype = y0_tuple[0].dtype\n    t_grid = t_grid.to(device=device, dtype=dtype)\n\n    y = list(y0_tuple)\n    spike_history = []\n\n    for k in range(N - 1):\n        t_k = t_grid[k]\n        y = ode_func(t_k, tuple(y))\n        spike_history.append(y[-1])\n        ###here we only assume one boundary term.\n        ###will update to (boundary_1, boundary_2, ...) if necessary.\n\n    return spike_history\n</code></pre>"},{"location":"api/surrogate/","title":"SpikeDE.surrogate","text":"<p>This module provides a comprehensive collection of surrogate gradient functions and stochastic spiking mechanisms designed for training Spiking Neural Networks (SNNs) using backpropagation.</p> <p>Since the spiking operation (Heaviside step function) is non-differentiable, this library implements various smooth approximations to estimate gradients during the backward pass while maintaining discrete binary spikes in the forward pass. Additionally, it offers a noisy threshold approach that enables stochastic firing during training for improved regularization and biological plausibility.</p>"},{"location":"api/surrogate/#key-features","title":"Key Features","text":"<ul> <li>Multiple Surrogate Gradients: Includes Sigmoid, Arctangent, Piecewise Linear, and Gaussian derivatives, each with distinct mathematical properties suited for different network depths and convergence requirements.</li> <li>Stochastic Spiking: Implements <code>NoisyThresholdSpike</code>, which injects logistic noise into the threshold to create a differentiable soft-spike mechanism during training, automatically reverting to hard spikes during inference.</li> <li>Flexible API: Available as both reusable <code>torch.autograd.Function</code> classes for custom layer integration and functional wrappers for concise usage.</li> </ul>"},{"location":"api/surrogate/#spikeDE.surrogate.SigmoidSurrogate","title":"SigmoidSurrogate","text":"<p>               Bases: <code>Function</code></p> <p>Sigmoid-based surrogate gradient function for Spiking Neural Networks (SNNs).</p> <p>This class implements a custom autograd function where the forward pass uses a hard Heaviside step function to generate discrete spikes, while the backward pass approximates the undefined gradient using the derivative of a scaled sigmoid function.</p> <p>Forward Pass:</p> \\[ S(x) = H(x) = \\begin{cases} 1 &amp; \\text{if } x \\geq 0 \\\\ 0 &amp; \\text{if } x &lt; 0 \\end{cases} \\] <p>Backward Pass (Surrogate Gradient):</p> \\[ \\sigma'(x) = \\kappa \\cdot \\text{sigmoid}(\\kappa x) \\cdot (1 - \\text{sigmoid}(\\kappa x)) \\] <p>Where: \\(x\\) is the input (membrane potential minus threshold, \\(U - \\theta\\)), \\(\\kappa\\) (scale) controls the sharpness of the approximation.</p> <p>Attributes:</p> <ul> <li> <code>scale</code>               (<code>float</code>)           \u2013            <p>The scaling factor \\(\\kappa\\). Larger values approximate the true            step function more closely but may lead to vanishing gradients.</p> </li> </ul>"},{"location":"api/surrogate/#spikeDE.surrogate.SigmoidSurrogate.backward","title":"backward  <code>staticmethod</code>","text":"<pre><code>backward(ctx: FunctionCtx, grad_output: Tensor) -&gt; tuple[Tensor, None]\n</code></pre> <p>Computes the gradient using the sigmoid derivative as a surrogate.</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object containing saved tensors from the forward pass.</p> </li> <li> <code>grad_output</code>               (<code>Tensor</code>)           \u2013            <p>Gradient of the loss with respect to the output of the forward pass.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, None]</code>           \u2013            <p>A tuple containing the gradient with respect to the input                        and None for the non-differentiable scale parameter.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>@staticmethod\ndef backward(\n    ctx: torch.autograd.function.FunctionCtx, grad_output: torch.Tensor\n) -&gt; Tuple[torch.Tensor, None]:\n    r\"\"\"\n    Computes the gradient using the sigmoid derivative as a surrogate.\n\n    Args:\n        ctx: Context object containing saved tensors from the forward pass.\n        grad_output: Gradient of the loss with respect to the output of the forward pass.\n\n    Returns:\n        A tuple containing the gradient with respect to the input\n                                   and None for the non-differentiable scale parameter.\n    \"\"\"\n    (input,) = ctx.saved_tensors\n    scale = ctx.scale\n    sigmoid = torch.sigmoid(scale * input)\n    surrogate_grad = scale * sigmoid * (1 - sigmoid)\n    return grad_output * surrogate_grad, None\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.SigmoidSurrogate.forward","title":"forward  <code>staticmethod</code>","text":"<pre><code>forward(ctx: FunctionCtx, input: Tensor, scale: float) -&gt; Tensor\n</code></pre> <p>Performs the forward pass using a hard threshold (Heaviside step function).</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object to save tensors for the backward pass.</p> </li> <li> <code>input</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor representing the membrane potential minus threshold (\\(U - \\theta\\)).</p> </li> <li> <code>scale</code>               (<code>float</code>)           \u2013            <p>Scaling factor (\\(\\kappa\\)) controlling the sharpness of the surrogate gradient.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>A binary tensor of spikes (0.0 or 1.0).</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>@staticmethod\ndef forward(\n    ctx: torch.autograd.function.FunctionCtx, input: torch.Tensor, scale: float\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Performs the forward pass using a hard threshold (Heaviside step function).\n\n    Args:\n        ctx: Context object to save tensors for the backward pass.\n        input: Input tensor representing the membrane potential minus threshold ($U - \\theta$).\n        scale: Scaling factor ($\\kappa$) controlling the sharpness of the surrogate gradient.\n\n    Returns:\n        A binary tensor of spikes (0.0 or 1.0).\n    \"\"\"\n    ctx.save_for_backward(input)\n    ctx.scale = scale\n    return (input &gt;= 0).float()\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.ArctanSurrogate","title":"ArctanSurrogate","text":"<p>               Bases: <code>Function</code></p> <p>Arctangent-based surrogate gradient function for SNNs.</p> <p>This method uses the derivative of the arctangent function as the surrogate gradient. It features heavier tails compared to the sigmoid, allowing gradients to propagate even when the membrane potential is far from the threshold.</p> <p>Forward Pass:</p> \\[ S(x) = H(x) \\] <p>Backward Pass (Surrogate Gradient):</p> \\[ \\sigma'(x) = \\frac{\\kappa}{1 + (\\frac{\\pi}{2} \\kappa x)^2} \\] Note <p>The implementation includes a normalization factor involving \\(\\pi/2\\) to ensure stable gradient magnitudes, slightly modifying the standard arctan derivative.</p> <p>Attributes:</p> <ul> <li> <code>scale</code>               (<code>float</code>)           \u2013            <p>The scaling factor \\(\\kappa\\).</p> </li> </ul>"},{"location":"api/surrogate/#spikeDE.surrogate.ArctanSurrogate.backward","title":"backward  <code>staticmethod</code>","text":"<pre><code>backward(ctx: FunctionCtx, grad_output: Tensor) -&gt; tuple[Tensor, None]\n</code></pre> <p>Computes the gradient using the normalized arctangent derivative.</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object containing saved tensors.</p> </li> <li> <code>grad_output</code>               (<code>Tensor</code>)           \u2013            <p>Upstream gradient from the loss function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, None]</code>           \u2013            <p>Gradient w.r.t input and None for scale.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>@staticmethod\ndef backward(\n    ctx: torch.autograd.function.FunctionCtx, grad_output: torch.Tensor\n) -&gt; Tuple[torch.Tensor, None]:\n    r\"\"\"\n    Computes the gradient using the normalized arctangent derivative.\n\n    Args:\n        ctx: Context object containing saved tensors.\n        grad_output: Upstream gradient from the loss function.\n\n    Returns:\n        Gradient w.r.t input and None for scale.\n    \"\"\"\n    (input,) = ctx.saved_tensors\n    scale = ctx.scale\n    return (\n        scale / 2 / (1 + (math.pi / 2 * scale * input).pow_(2)) * grad_output,\n        None,\n    )\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.ArctanSurrogate.forward","title":"forward  <code>staticmethod</code>","text":"<pre><code>forward(ctx: FunctionCtx, input: Tensor, scale: float) -&gt; Tensor\n</code></pre> <p>Performs the forward pass using a hard threshold.</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object to save tensors for the backward pass.</p> </li> <li> <code>input</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor (\\(U - \\theta\\)).</p> </li> <li> <code>scale</code>               (<code>float</code>)           \u2013            <p>Scaling factor (\\(\\kappa\\)).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Binary spike tensor.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>@staticmethod\ndef forward(\n    ctx: torch.autograd.function.FunctionCtx, input: torch.Tensor, scale: float\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Performs the forward pass using a hard threshold.\n\n    Args:\n        ctx: Context object to save tensors for the backward pass.\n        input: Input tensor ($U - \\theta$).\n        scale: Scaling factor ($\\kappa$).\n\n    Returns:\n        Binary spike tensor.\n    \"\"\"\n    ctx.save_for_backward(input)\n    ctx.scale = scale\n    return (input &gt;= 0).float()\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.PiecewiseLinearSurrogate","title":"PiecewiseLinearSurrogate","text":"<p>               Bases: <code>Function</code></p> <p>Piecewise Linear (Triangular) surrogate gradient function.</p> <p>A computationally efficient approximation that defines a triangular window around the threshold. Gradients are constant within the window and zero outside.</p> <p>Forward Pass:</p> \\[ S(x) = H(x) \\] <p>Backward Pass (Surrogate Gradient):</p> \\[ \\sigma'(x) = \\begin{cases} \\frac{1}{2\\gamma} &amp; \\text{if } |x| \\leq \\gamma \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <p>Where \\(\\gamma\\) (gamma) defines the width of the active region.</p> <p>Attributes:</p> <ul> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Half-width of the linear region.</p> </li> </ul>"},{"location":"api/surrogate/#spikeDE.surrogate.PiecewiseLinearSurrogate.backward","title":"backward  <code>staticmethod</code>","text":"<pre><code>backward(ctx: FunctionCtx, grad_output: Tensor) -&gt; tuple[Tensor, None]\n</code></pre> <p>Computes the gradient using a rectangular window function.</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object containing saved tensors.</p> </li> <li> <code>grad_output</code>               (<code>Tensor</code>)           \u2013            <p>Upstream gradient.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, None]</code>           \u2013            <p>Gradient w.r.t input and None for gamma.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>@staticmethod\ndef backward(\n    ctx: torch.autograd.function.FunctionCtx, grad_output: torch.Tensor\n) -&gt; Tuple[torch.Tensor, None]:\n    r\"\"\"\n    Computes the gradient using a rectangular window function.\n\n    Args:\n        ctx: Context object containing saved tensors.\n        grad_output: Upstream gradient.\n\n    Returns:\n        Gradient w.r.t input and None for gamma.\n    \"\"\"\n    (input,) = ctx.saved_tensors\n    gamma = ctx.gamma\n    surrogate_grad = ((input &gt;= -gamma) &amp; (input &lt;= gamma)).float() / (2 * gamma)\n    return grad_output * surrogate_grad, None\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.PiecewiseLinearSurrogate.forward","title":"forward  <code>staticmethod</code>","text":"<pre><code>forward(ctx: FunctionCtx, input: Tensor, gamma: float) -&gt; Tensor\n</code></pre> <p>Performs the forward pass using a hard threshold.</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object to save tensors.</p> </li> <li> <code>input</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor (\\(U - \\theta\\)).</p> </li> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Width parameter (\\(\\gamma\\)).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Binary spike tensor.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>@staticmethod\ndef forward(\n    ctx: torch.autograd.function.FunctionCtx, input: torch.Tensor, gamma: float\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Performs the forward pass using a hard threshold.\n\n    Args:\n        ctx: Context object to save tensors.\n        input: Input tensor ($U - \\theta$).\n        gamma: Width parameter ($\\gamma$).\n\n    Returns:\n        Binary spike tensor.\n    \"\"\"\n    ctx.save_for_backward(input)\n    ctx.gamma = gamma\n    return (input &gt;= 0).float()\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.GaussianSurrogate","title":"GaussianSurrogate","text":"<p>               Bases: <code>Function</code></p> <p>Gaussian-based surrogate gradient function.</p> <p>Uses a normalized Gaussian function to approximate the derivative. It offers the smoothest profile with exponential decay, providing very localized gradient updates.</p> <p>Forward Pass:</p> \\[ S(x) = H(x) \\] <p>Backward Pass (Surrogate Gradient):</p> \\[ \\sigma'(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{x^2}{2\\sigma^2}} \\] <p>Where \\(\\sigma\\) (sigma) controls the spread (standard deviation) of the gradient.</p> <p>Attributes:</p> <ul> <li> <code>sigma</code>               (<code>float</code>)           \u2013            <p>Standard deviation of the Gaussian.</p> </li> </ul>"},{"location":"api/surrogate/#spikeDE.surrogate.GaussianSurrogate.backward","title":"backward  <code>staticmethod</code>","text":"<pre><code>backward(ctx: FunctionCtx, grad_output: Tensor) -&gt; tuple[Tensor, None]\n</code></pre> <p>Computes the gradient using the Gaussian PDF.</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object containing saved tensors.</p> </li> <li> <code>grad_output</code>               (<code>Tensor</code>)           \u2013            <p>Upstream gradient.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, None]</code>           \u2013            <p>Gradient w.r.t input and None for sigma.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>@staticmethod\ndef backward(\n    ctx: torch.autograd.function.FunctionCtx, grad_output: torch.Tensor\n) -&gt; Tuple[torch.Tensor, None]:\n    r\"\"\"\n    Computes the gradient using the Gaussian PDF.\n\n    Args:\n        ctx: Context object containing saved tensors.\n        grad_output: Upstream gradient.\n\n    Returns:\n        Gradient w.r.t input and None for sigma.\n    \"\"\"\n    (input,) = ctx.saved_tensors\n    sigma = ctx.sigma\n    surrogate_grad = torch.exp(-(input**2) / (2 * sigma**2)) / (\n        sigma * torch.sqrt(torch.tensor(2 * torch.pi))\n    )\n    return grad_output * surrogate_grad, None\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.GaussianSurrogate.forward","title":"forward  <code>staticmethod</code>","text":"<pre><code>forward(ctx: FunctionCtx, input: Tensor, sigma: float) -&gt; Tensor\n</code></pre> <p>Performs the forward pass using a hard threshold.</p> <p>Parameters:</p> <ul> <li> <code>ctx</code>               (<code>FunctionCtx</code>)           \u2013            <p>Context object to save tensors.</p> </li> <li> <code>input</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor (\\(U - \\theta\\)).</p> </li> <li> <code>sigma</code>               (<code>float</code>)           \u2013            <p>Standard deviation parameter (\\(\\sigma\\)).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Binary spike tensor.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>@staticmethod\ndef forward(\n    ctx: torch.autograd.function.FunctionCtx, input: torch.Tensor, sigma: float\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Performs the forward pass using a hard threshold.\n\n    Args:\n        ctx: Context object to save tensors.\n        input: Input tensor ($U - \\theta$).\n        sigma: Standard deviation parameter ($\\sigma$).\n\n    Returns:\n        Binary spike tensor.\n    \"\"\"\n    ctx.save_for_backward(input)\n    ctx.sigma = sigma\n    return (input &gt;= 0).float()\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.NoisyThresholdSpikeModule","title":"NoisyThresholdSpikeModule","text":"<pre><code>NoisyThresholdSpikeModule(scale=5.0, sample=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>PyTorch Module wrapper for <code>noisy_threshold_spike</code>.</p> <p>Automatically tracks the model's training state (<code>self.training</code>) to switch between stochastic soft spikes and deterministic hard spikes.</p> <p>Attributes:</p> <ul> <li> <code>scale</code>               (<code>float</code>)           \u2013            <p>Sharpness parameter (\\(\\kappa\\)).</p> </li> <li> <code>sample</code>               (<code>bool</code>)           \u2013            <p>Whether to sample noise or use mean-field.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>def __init__(self, scale=5.0, sample=True):\n    super().__init__()\n    self.scale = scale\n    self.sample = sample\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.sigmoid_surrogate","title":"sigmoid_surrogate","text":"<pre><code>sigmoid_surrogate(input: Tensor, scale: float = 5.0) -&gt; Tensor\n</code></pre> <p>Functional wrapper for the Sigmoid surrogate gradient.</p> <p>Allows gradients to flow through the non-differentiable spiking operation during backpropagation by replacing the step function's derivative with a smooth sigmoid derivative.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor representing membrane potential minus threshold (\\(U - \\theta\\)).</p> </li> <li> <code>scale</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Scaling factor (\\(\\kappa\\)). Higher values make the surrogate sharper.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>A tensor of binary spikes (0.0 or 1.0) with custom gradient flow.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>def sigmoid_surrogate(input: torch.Tensor, scale: float = 5.0) -&gt; torch.Tensor:\n    r\"\"\"\n    Functional wrapper for the Sigmoid surrogate gradient.\n\n    Allows gradients to flow through the non-differentiable spiking operation during\n    backpropagation by replacing the step function's derivative with a smooth sigmoid derivative.\n\n    Args:\n        input: Input tensor representing membrane potential minus threshold ($U - \\theta$).\n        scale: Scaling factor ($\\kappa$). Higher values make the surrogate sharper.\n\n    Returns:\n        A tensor of binary spikes (0.0 or 1.0) with custom gradient flow.\n    \"\"\"\n    return SigmoidSurrogate.apply(input, scale)\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.arctan_surrogate","title":"arctan_surrogate","text":"<pre><code>arctan_surrogate(input: Tensor, scale: float = 2.0) -&gt; Tensor\n</code></pre> <p>Functional wrapper for the Arctan surrogate gradient.</p> <p>Ideal for deep networks where gradient vanishing is a concern due to its heavy-tailed gradient distribution.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor (\\(U - \\theta\\)).</p> </li> <li> <code>scale</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Scaling factor (\\(\\kappa\\)).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Binary spike tensor with arctan-based gradient flow.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>def arctan_surrogate(input: torch.Tensor, scale: float = 2.0) -&gt; torch.Tensor:\n    r\"\"\"\n    Functional wrapper for the Arctan surrogate gradient.\n\n    Ideal for deep networks where gradient vanishing is a concern due to its heavy-tailed\n    gradient distribution.\n\n    Args:\n        input: Input tensor ($U - \\theta$).\n        scale: Scaling factor ($\\kappa$).\n\n    Returns:\n        Binary spike tensor with arctan-based gradient flow.\n    \"\"\"\n    return ArctanSurrogate.apply(input, scale)\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.piecewise_linear_surrogate","title":"piecewise_linear_surrogate","text":"<pre><code>piecewise_linear_surrogate(input: Tensor, gamma: float = 1.0) -&gt; Tensor\n</code></pre> <p>Functional wrapper for the Piecewise Linear surrogate gradient.</p> <p>Best for high-speed training on resource-constrained hardware or models requiring sparse gradient updates.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor (\\(U - \\theta\\)).</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Width of the active region (\\(\\gamma\\)).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Binary spike tensor with linear-based gradient flow.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>def piecewise_linear_surrogate(input: torch.Tensor, gamma: float = 1.0) -&gt; torch.Tensor:\n    r\"\"\"\n    Functional wrapper for the Piecewise Linear surrogate gradient.\n\n    Best for high-speed training on resource-constrained hardware or models requiring\n    sparse gradient updates.\n\n    Args:\n        input: Input tensor ($U - \\theta$).\n        gamma: Width of the active region ($\\gamma$).\n\n    Returns:\n        Binary spike tensor with linear-based gradient flow.\n    \"\"\"\n    return PiecewiseLinearSurrogate.apply(input, gamma)\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.gaussian_surrogate","title":"gaussian_surrogate","text":"<pre><code>gaussian_surrogate(input: Tensor, sigma: float = 1.0) -&gt; Tensor\n</code></pre> <p>Functional wrapper for the Gaussian surrogate gradient.</p> <p>Best for precision tasks where only neurons very close to firing should receive updates.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor (\\(U - \\theta\\)).</p> </li> <li> <code>sigma</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Spread of the gradient (\\(\\sigma\\)).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Binary spike tensor with Gaussian-based gradient flow.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>def gaussian_surrogate(input: torch.Tensor, sigma: float = 1.0) -&gt; torch.Tensor:\n    r\"\"\"\n    Functional wrapper for the Gaussian surrogate gradient.\n\n    Best for precision tasks where only neurons very close to firing should receive updates.\n\n    Args:\n        input: Input tensor ($U - \\theta$).\n        sigma: Spread of the gradient ($\\sigma$).\n\n    Returns:\n        Binary spike tensor with Gaussian-based gradient flow.\n    \"\"\"\n    return GaussianSurrogate.apply(input, sigma)\n</code></pre>"},{"location":"api/surrogate/#spikeDE.surrogate.noisy_threshold_spike","title":"noisy_threshold_spike","text":"<pre><code>noisy_threshold_spike(\n    input: Tensor,\n    scale: float = 5.0,\n    training: bool = True,\n    sample: bool = True,\n) -&gt; Tensor\n</code></pre> <p>Stochastic spiking function using a noisy threshold.</p> <p>Instead of a hard spike in the forward pass, this method injects logistic noise into the threshold, creating a stochastic soft spike during training. During inference (eval mode), it reverts to a hard spike. This acts as both the forward mechanism and its own differentiable path (real backward), unlike the surrogate methods above.</p> <p>Training Mode:</p> \\[ S(t) = \\text{sigmoid}(\\kappa(U(t) - \\theta) + \\epsilon) \\] <p>Where \\(\\epsilon \\sim \\text{Logistic}(0, 1)\\) sampled via inverse CDF:</p> \\[ \\epsilon = \\log(u) - \\log(1-u), \\quad u \\sim \\text{Uniform}(0, 1) \\] <p>Inference Mode:</p> \\[ S(t) = H(U(t) - \\theta) \\] <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor (\\(U - \\theta\\)).</p> </li> <li> <code>scale</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Sharpness parameter (\\(\\kappa\\)). Higher values make the sigmoid sharper.</p> </li> <li> <code>training</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, applies noise and soft sigmoid. If False, uses hard threshold.</p> </li> <li> <code>sample</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, samples noise per element. If False, uses the mean-field     approximation (standard sigmoid without noise).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Soft probabilities during training, binary spikes during eval.</p> </li> </ul> Source code in <code>spikeDE/surrogate.py</code> <pre><code>def noisy_threshold_spike(\n    input: torch.Tensor, scale: float = 5.0, training: bool = True, sample: bool = True\n) -&gt; torch.Tensor:\n    r\"\"\"\n    Stochastic spiking function using a noisy threshold.\n\n    Instead of a hard spike in the forward pass, this method injects logistic noise\n    into the threshold, creating a stochastic soft spike during training. During\n    inference (eval mode), it reverts to a hard spike. This acts as both the forward\n    mechanism and its own differentiable path (real backward), unlike the surrogate\n    methods above.\n\n    Training Mode:\n\n    $$ S(t) = \\text{sigmoid}(\\kappa(U(t) - \\theta) + \\epsilon) $$\n\n    Where $\\epsilon \\sim \\text{Logistic}(0, 1)$ sampled via inverse CDF:\n\n    $$ \\epsilon = \\log(u) - \\log(1-u), \\quad u \\sim \\text{Uniform}(0, 1) $$\n\n    Inference Mode:\n\n    $$ S(t) = H(U(t) - \\theta) $$\n\n    Args:\n        input: Input tensor ($U - \\theta$).\n        scale: Sharpness parameter ($\\kappa$). Higher values make the sigmoid sharper.\n        training: If True, applies noise and soft sigmoid. If False, uses hard threshold.\n        sample: If True, samples noise per element. If False, uses the mean-field\n                approximation (standard sigmoid without noise).\n\n    Returns:\n        Soft probabilities during training, binary spikes during eval.\n    \"\"\"\n    if not training:\n        # Evaluation: hard threshold (same as before)\n        return (input &gt;= 0).float()\n\n    # z = (v - threshold) * scale = (v - threshold) / \u03c4\n    z = input * scale\n\n    if sample:\n        # Sample g ~ Logistic(0, 1) via inverse CDF\n        u = torch.rand_like(input).clamp(1e-6, 1 - 1e-6)\n        g = torch.log(u) - torch.log1p(-u)\n        # Soft spike with noise: S = \u03c3(z + g)\n        spike = torch.sigmoid(z + g)\n    else:\n        # Mean-field: S = \u03c3(z)\n        spike = torch.sigmoid(z)\n\n    return spike\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2026/02/28/launching-spikede-fractional-snn/","title":"Introducing spikeDE: Where Fractional Calculus Meets Spiking Neural Networks","text":"<p>Today marks a significant milestone for our research team. We are incredibly proud to announce the public release of spikeDE, an open-source PyTorch based library designed to bring Fractional-Order Dynamics to the world of Spiking Neural Networks (SNNs).</p> <p>For years, SNNs have been celebrated for their biological plausibility and energy efficiency. However, traditional models like the Leaky Integrate-and-Fire (LIF) neuron rely on integer-order differential equations (\\(\\alpha=1\\)), which assume Markovian dynamics\u2014meaning the neuron's current state depends only on its immediate past. This simplification fails to capture the rich, complex temporal dependencies observed in real biological neurons, which exhibit long-term memory and power-law relaxation.</p> <p>With spikeDE, we change the paradigm.</p>"},{"location":"blog/2026/02/28/launching-spikede-fractional-snn/#the-core-idea-beyond-integer-orders","title":"The Core Idea: Beyond Integer Orders","text":"<p>As detailed in our upcoming paper at ICLR 2026, \"Fractional-order Spiking Neural Network\", biological systems often operate on multiple time scales simultaneously. A single integer-order neuron cannot efficiently represent this spectrum without stacking infinite layers.</p> <p>spikeDE introduces the Caputo fractional derivative (\\(D^\\alpha\\), where \\(0 &lt; \\alpha \\leq 1\\)) into the membrane potential dynamics:</p> \\[ \\tau D^\\alpha U(t) = f(t, U(t), I(t)) \\] <p>By tuning the fractional order \\(\\alpha\\), our f-LIF neurons naturally exhibit:</p> <ol> <li>Heavy-tailed Memory: Past inputs influence the current state via a Mittag-Leffler function decay, not just a simple exponential.</li> <li>Non-Markovian Behavior: The system inherently \"remembers\" its history, capturing long-range dependencies crucial for processing temporal data like event-based vision or dynamic graphs.</li> <li>Enhanced Robustness: Our theoretical analysis shows that fractional dynamics suppress noise accumulation sub-linearly (\\(t^\\alpha\\) vs \\(t\\)), making f-SNNs significantly more robust to input perturbations.</li> </ol>"},{"location":"blog/2026/02/28/launching-spikede-fractional-snn/#whats-new-in-spikede","title":"What's New in spikeDE?","text":"<p>The initial release of spikeDE is built from the ground up to be flexible, efficient, and strictly compatible with the PyTorch ecosystem.</p>"},{"location":"blog/2026/02/28/launching-spikede-fractional-snn/#native-fractional-solvers","title":"Native Fractional Solvers","text":"<p>We integrate optimized solvers directly into the computational graph. This allows for end-to-end training, even with non-local fractional operators.</p>"},{"location":"blog/2026/02/28/launching-spikede-fractional-snn/#per-layer-customization","title":"Per-Layer Customization","text":"<p>Not all layers need the same memory depth. spikeDE allows you to set distinct \\(\\alpha\\) values for each layer or even make \\(\\alpha\\) a learnable parameter, letting the network discover the optimal time-scale spectrum for your specific task.</p> <pre><code>from spikeDE import SNNWrapper, LIFNeuron\n\n# Make alpha learnable! The network decides how much memory it needs.\nnet = SNNWrapper(\n    base=my_snn_model,\n    integrator='fdeint',\n    alpha=[0.5, 0.8, 0.9], # Different memory depths per layer\n    learn_alpha=True       # Enable gradient updates for alpha\n)\n</code></pre>"},{"location":"blog/2026/02/28/launching-spikede-fractional-snn/#strict-generalization","title":"Strict Generalization","text":"<p>spikeDE is a strict superset of traditional SNNs. Setting \\(\\alpha=1.0\\) recovers the standard LIF dynamics exactly. This means you can seamlessly migrate existing CNN-to-SNN or direct-training workflows to the fractional domain with minimal code changes.</p>"},{"location":"blog/2026/02/28/launching-spikede-fractional-snn/#early-results-state-of-the-art-performance","title":"Early Results: State-of-the-Art Performance","text":"<p>Our experiments, documented in the ICLR 2026 paper, demonstrate that f-SNNs consistently outperform their integer-order counterparts:</p> <ul> <li>Neuromorphic Vision: On the HarDVS dataset, our f-SNN achieved 47.66% accuracy, surpassing the best integer-order baseline by +1.55%.</li> <li>Graph Learning: In dynamic graph tasks (e.g., Cora), the fractional Spiking Graph Convolutional Network showed a remarkable +6.2% improvement in node classification, proving the power of long-range temporal aggregation on graph structures.</li> <li>Robustness: Under heavy noise injection and time-jitter attacks, f-SNNs maintained stable performance where traditional SNNs degraded rapidly.</li> </ul>"},{"location":"blog/2026/02/28/launching-spikede-fractional-snn/#getting-started","title":"Getting Started","text":"<p>Getting started with spikeDE is easy. Whether you are a neuroscientist modeling biological circuits or a machine learning engineer building low-power AI, our documentation has you covered.</p>"},{"location":"blog/2026/02/28/launching-spikede-fractional-snn/#summary","title":"Summary","text":"<p>We believe that Fractional Calculus holds the key to unlocking the next generation of efficient, brain-inspired AI. By open-sourcing spikeDE, we hope to lower the barrier for researchers to explore this fascinating intersection of mathematics and neuroscience.</p> <p> Full Paper  Documentation  Source Code</p> <p>We welcome contributions, issues, and discussions. Let's build the future of memory-rich neural networks together! Happy Spiking!</p>"},{"location":"get_start/","title":"Getting Started","text":"<p>spikeDE empowers you to build Fractional-Order Spiking Neural Networks (f-SNNs) using an API that aligns closely with  PyTorch. This design ensures a seamless transition for  PyTorch users, allowing you to leverage existing skills while exploring advanced fractional dynamics with minimal learning curve.</p> <p>We support a diverse range of modern neural architectures, including Multilayer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), Residual Networks (ResNets) and Transformers.</p> <p>Whether you are researching neuromorphic vision, modeling complex time-series, or developing energy-efficient AI, spikeDE provides the robust tools needed to construct, train, and deploy high-performance spiking models.</p> <p>In this section, you will find step-by-step guides ranging from installation to building and training your first functional network. Choose the path that best fits your current needs:</p> <ul> <li> <p> Installation</p> <p>Get up and running quickly. Install spikeDE via <code>pip</code> or from source, with detailed instructions for setting up all necessary dependencies.</p> <p> Go to Installation Guide</p> </li> <li> <p> Introduction by Example</p> <p>Dive straight into code. Follow a complete walkthrough to define a fractional spiking model, encode inputs, train on a dataset, and evaluate performance.</p> <p> Try Your First f-SNN</p> </li> </ul> <p>New to Spiking Neural Networks?</p> <p>If you are unfamiliar with SNN concepts, start with the Introduction by Example. It assumes only basic familiarity with PyTorch and gently introduces core concepts such as spike encoding, fractional leaky integrate-and-fire (f-LIF) neurons, and surrogate gradients.</p> <p>Happy spiking!</p>"},{"location":"get_start/installation/","title":"Installation","text":"<p>spikeDE supports  Python 3.9 through 3.13.</p> <p>Use a virtual environment</p> <p>We strongly recommend installing spikeDE in an isolated virtual environment to avoid conflicts with system-wide packages. You can create one using venv,  Conda, or  Docker.</p>"},{"location":"get_start/installation/#install-dependencies","title":"Install Dependencies","text":"<p>spikeDE relies on several third-party libraries. Please follow the instructions below to install them.</p>"},{"location":"get_start/installation/#install-pytorch","title":"Install PyTorch","text":"<p>The primary dependency is  PyTorch. Since installation commands vary by platform and hardware, please refer to the official PyTorch installation guide for the most accurate instructions.</p> <p>GPU Acceleration</p> <p>To leverage GPU acceleration with spikeDE, ensure you install the CUDA-enabled version of PyTorch that matches your NVIDIA driver. The CPU-only version will work but will not utilize GPU resources.</p>"},{"location":"get_start/installation/#additional-dependencies","title":"Additional Dependencies","text":"<p>spikeDE utilizes neural differential equation solvers requiring the following specific packages:</p> <ul> <li> <p><code>torchdiffeq</code> (for Ordinary Differential Equations - ODEs):   <pre><code>pip install torchdiffeq\n</code></pre></p> </li> <li> <p><code>torchfde</code> (for Fractional Differential Equations - FDEs):   <pre><code>pip install git+https://github.com/kangqiyu/torchfde.git\n</code></pre></p> </li> </ul>"},{"location":"get_start/installation/#install-spikede","title":"Install spikeDE","text":"<p>Once the dependencies are ready, install spikeDE directly from the source repository:</p> <pre><code>pip install git+https://github.com/PhysAGI/spikeDE.git\n</code></pre> <p>GPU Support Enabled</p> <p>spikeDE automatically detects and utilizes GPU acceleration if a CUDA-enabled PyTorch installation and a compatible NVIDIA GPU are present. No additional configuration is required.</p>"},{"location":"get_start/introduction/","title":"Introduction by Example","text":"<p>We shortly introduce the fundamental concepts of spikeDE through a simple example: training a SNN on the MNIST dataset using fractional-order dynamics. This tutorial assumes no prior knowledge of SNNs or differential equation solvers\u2014everything you need will be explained along the way.</p> <p>Recommend Reading</p> <p>For an introduction to SNNs, we refer the interested reader to Training Spiking Neural Networks Using Lessons From Deep Learning.</p>"},{"location":"get_start/introduction/#what-is-spikede","title":"What is spikeDE?","text":"<p>spikeDE is a  PyTorch-based library designed to implement the Fractional-Order Spiking Neural Network (f-SNN) framework. Unlike traditional SNN libraries that rely on first-order Ordinary Differential Equations (ODEs) with Markovian properties\u2014where the current state depends only on the immediate past\u2014spikeDE governs neuron dynamics using Fractional-Order Differential Equations (FDEs). This approach is grounded in the observation that biological neurons often exhibit non-Markovian behaviors, such as power-law relaxation and long-range temporal correlations, which cannot be captured by integer-order models.</p> <p>Crucially, spikeDE serves as a generalized framework that strictly encompasses traditional integer-order SNNs. By setting the fractional order \\(\\alpha = 1\\), the library naturally recovers standard Leaky Integrate-and-Fire (LIF) and Integrate-and-Fire (IF) models, making it a superset of existing approaches rather than an alternative solver. When \\(0 &lt; \\alpha &lt; 1\\), the Caputo fractional derivative introduces a power-law memory kernel, allowing the membrane potential to depend on its entire history. This capability enables the modeling of complex phenomena like persistent memory, fractal dendritic structures, and enhanced robustness to input perturbations, offering a more biologically plausible and mathematically rich foundation for spiking networks.</p> <p>At its core, spikeDE provides:</p> <ul> <li>Fractional Neuron Models: Implementations of f-LIF and f-IF neurons that naturally encode long-term dependencies via fractional calculus.</li> <li>Generalized Wrapper (<code>SNNWrapper</code>): A flexible interface that converts any standard PyTorch network into an f-SNN, supporting both single-term and multi-term fractional dynamics.</li> <li>Advanced Numerical Solvers: Efficient discretization methods (e.g., fractional Adams\u2013Bashforth\u2013Moulton, Gr\u00fcnwald\u2013Letnikov) tailored for non-local fractional operators.</li> <li>Trainable Fractional Orders: Options to learn the fractional order \\(\\alpha\\) and memory coefficients end-to-end, allowing the network to adapt its temporal memory span automatically.</li> </ul> <p>This allows researchers to move beyond simple recurrence and explore how non-Markovian dynamics, history-dependent evolution, and fractional temporal scaling enhance learning in spiking networks across vision, graph, and sequence tasks.</p>"},{"location":"get_start/introduction/#step-by-step-walkthrough-mnist-classification-with-spikede","title":"Step-by-Step Walkthrough: MNIST Classification with spikeDE","text":"<p>Below, we walk through the key components of the provided example script. You can run this code after installing spikeDE and  PyTorch.</p> <p>Note</p> <p>The full script is designed as a standalone example. Only classes/functions imported from <code>spikeDE</code> (e.g., <code>SNN</code>, <code>SNNWrapper</code>, <code>LIFNeuron</code>) are part of the package. Everything else\u2014data loading, model definitions like <code>CNNExample</code>, utility functions like <code>spike_converter</code>\u2014are user-defined helpers written specifically for this demo.</p>"},{"location":"get_start/introduction/#importing-required-modules","title":"Importing Required Modules","text":"Importing requried modules<pre><code># Core pytorch components\nimport torch\nimport torch.nn as nn\n\n# Dataset loading components\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Core spikeDE components\nfrom spikeDE import SNN, SNNWrapper\nfrom spikeDE import LIFNeuron, IFNeuron\n</code></pre> <p>Here, only the last two lines involve spikeDE. The rest are standard  PyTorch utilities for data handling and training loops.</p>"},{"location":"get_start/introduction/#defining-your-base-network","title":"Defining Your Base Network","text":"<p>Before wrapping a network with spikeDE, you define a standard PyTorch model using regular layers\u2014but insert spiking neurons at activation points.</p> CNN based network<pre><code>class CNNExample(nn.Module):\n    def __init__(self, tau, threshold, surrogate_grad_scale):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.lif1 = LIFNeuron(tau, threshold, surrogate_grad_scale)  # \u2190 Spiking neuron!\n        self.pool1 = nn.MaxPool2d(2)\n\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.lif2 = LIFNeuron(tau, threshold, surrogate_grad_scale)\n        self.pool2 = nn.MaxPool2d(2)\n\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.lif3 = LIFNeuron(tau, threshold, surrogate_grad_scale)\n        self.fc2 = nn.Linear(128, 10)\n        self.lif4 = LIFNeuron(tau, threshold, surrogate_grad_scale)\n\n    def forward(self, x):\n        out = self.lif1(self.conv1(x))\n        out = self.pool1(out)\n        out = self.lif2(self.conv2(out))\n        out = self.pool2(out)\n        out = self.lif3(self.fc1(self.flatten(out)))\n        out = self.lif4(self.fc2(out))\n        return out\n</code></pre> <p>Key Insight</p> <ul> <li>This looks like a normal CNN\u2014but instead of ReLU, we use <code>LIFNeuron</code>.</li> <li>Each <code>LIFNeuron</code> maintains internal membrane potential and emits spikes based on dynamics defined by tau (time constant), threshold, and a surrogate gradient for backpropagation.  </li> <li>The actual spiking behavior is not computed here directly\u2014it\u2019s handled later by <code>SNNWrapper</code> during time integration.</li> </ul> MLP Based Network <p>You could similarly define an MLP: MLP based network<pre><code>class MLPExample(nn.Module):\n    def __init__(self, tau, threshold, surrogate_grad_scale):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(28*28, 2560, bias=False)\n        self.lif1 = LIFNeuron(tau, threshold, surrogate_grad_scale)\n        self.fc2 = nn.Linear(2560, 10, bias=False)\n        self.lif2 = LIFNeuron(tau, threshold, surrogate_grad_scale)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.lif1(self.fc1(x))\n        x = self.lif2(self.fc2(x))\n        return x\n</code></pre></p>"},{"location":"get_start/introduction/#converting-static-inputs-to-spike-trains","title":"Converting Static Inputs to Spike Trains","text":"<p>SNNs process temporal spike sequences, not static images. So we must convert each MNIST image into a series of spikes over time.</p> Spike converting<pre><code>def spike_converter(x, time_steps=100, flatten=False):\n    batch_size = x.size(0)\n    if flatten:\n        x = x.view(batch_size, -1)\n        p = x.unsqueeze(1).repeat(1, time_steps, 1)\n        spikes = torch.bernoulli(p)\n        return spikes.permute(1, 0, 2)  # [T, B, N]\n    else:\n        p = x.unsqueeze(1).repeat(1, time_steps, 1, 1, 1)\n        spikes = torch.bernoulli(p)\n        return spikes.permute(1, 0, 2, 3, 4)  # [T, B, C, H, W]\n</code></pre> <p>In the training loop, inputs are scaled (<code>data = 10 * data</code>) to increase spike rates\u2014this is a common heuristic.</p>"},{"location":"get_start/introduction/#wrapping-your-model-with-snnwrapper","title":"Wrapping Your Model with <code>SNNWrapper</code>","text":"<p>This is where the fractional framework is applied. The <code>SNNWrapper</code> transforms your static network into a dynamical system driven by FDEs.</p> Wrapping the base model<pre><code># Initialize the base CNN\nbase_network = CNNExample(tau=2.0, threshold=1.0, surrogate_grad_scale=0.3)\n\n# Wrap with fractional dynamics\nsnn_model = SNNWrapper(\n    base_network,\n    integrator=\"fdeint\",       # Use fractional solver\n    alpha=0.8,                 # Fractional order (0 &lt; alpha &lt;= 1)\n    multi_coefficient=None,    # None for single-term FDE\n    learn_alpha=True,          # Optionally learn the fractional order\n    learn_coefficient=False\n)\n\n# Initialize internal buffers based on input shape (C, H, W)\nsnn_model._set_neuron_shapes(input_shape=(1, 28, 28))\n</code></pre> <p>Key Parameters</p> <ul> <li><code>integrator</code>: Chooses the solver type:<ul> <li><code>'odeint'</code> / <code>'odeint_adjoint'</code> for classical ODEs (integer-order);</li> <li><code>'fdeint'</code> / <code>'fdeint_adjoint'</code> for FDEs.</li> </ul> </li> <li><code>alpha</code>: The fractional order (e.g., <code>0.5</code> for single alpha, <code>[0.3, 0.4, 0.5]</code> for multi-alpha).</li> <li><code>multi_coefficient</code>: Weights for each term (required if <code>alpha</code> has multiple values).</li> <li><code>learn_coefficient</code>: If <code>True</code>, coefficient(s) become trainable parameter(s).</li> <li><code>learn_alpha</code>: If <code>True</code>, \\(\\alpha(s)\\) become trainable parameter(s).</li> </ul>"},{"location":"get_start/introduction/#training-loop-time-integration-over-spikes","title":"Training Loop: Time Integration Over Spikes","text":"<p>During training, static inputs are first encoded into temporal spike trains with shape <code>[T, B, ...]</code>. These sequences are then passed to the model alongside a time grid that defines the evolution interval for the fractional solver:</p> Training loop<pre><code># Define time grid: from 0 to T_end with (T+1) points\ndata_time = torch.linspace(0, 0.01 * 100, 100 + 1, device=device).float()\n\n# Forward pass through the fractional dynamics solver\noutput = model(\n    data,\n    data_time,\n    method=\"gl\",\n    options={\"step_size\": 1.0, \"memory\": -1},\n)\n\n# Aggregate temporal outputs (e.g., average pooling) for final classification\noutput = output.mean(0)\n</code></pre> <p>Key arguments explained</p> <ul> <li><code>data_time</code>: Specifies the discrete time points \\(t_0, t_1, \\dots, t_T\\) over which the differential equation is solved.</li> <li><code>method</code>: Selects the numerical integration scheme (e.g., <code>'gl'</code> for the Gr\u00fcnwald\u2013Letnikov formula, suitable for capturing long-range memory).</li> <li><code>options</code>: Configures solver-specific parameters. For instance, <code>memory=-1</code> instructs the solver to utilize the full history of the state, which is essential for accurate fractional-order simulation.</li> </ul> <p>The model returns a sequence of outputs corresponding to each time step. To obtain a single prediction for classification, we typically aggregate these temporal responses (e.g., via averaging or summing).</p> Full Training Pipeline <p>The following block combines data loading, model instantiation, and the training loop. It demonstrates how to pass the time grid to the solver and handle the temporal outputs of the f-SNN. Standalone Code<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom spikeDE import SNNWrapper, LIFNeuron\n\n# --- 1. Configuration ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nTIME_STEPS = 100\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nFRACTIONAL_ORDER = 0.8  # Alpha &lt; 1 enables long-term memory\nEPOCHS = 5\n\n# --- 2. Data Loading ---\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n        lambda x: x.clamp(0, 1),  # Ensure values are in [0, 1]\n    ]\n)\ntrain_dataset = datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=transform\n)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=transform\n)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# --- 3. Model Definition ---\nclass CNNExample(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(1, 32, 3, padding=1)\n        self.lif1 = LIFNeuron(tau=2.0, threshold=1.0, surrogate_grad_scale=0.3)\n        self.pool = nn.MaxPool2d(2)\n        self.fc = nn.Linear(32 * 14 * 14, 10)\n        self.lif2 = LIFNeuron(tau=2.0, threshold=1.0, surrogate_grad_scale=0.3)\n\n    def forward(self, x):\n        x = self.lif1(self.conv(x))\n        x = self.pool(x)\n        x = x.flatten(1)\n        x = self.lif2(self.fc(x))\n        return x\n\n\nbase_net = CNNExample().to(DEVICE)\nmodel = SNNWrapper(\n    base_net, integrator=\"fdeint\", alpha=FRACTIONAL_ORDER, learn_alpha=False\n).to(DEVICE)\n\n# Initialize shapes (C, H, W)\nmodel._set_neuron_shapes(input_shape=(1, 28, 28))\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n\n# --- 4. Helper: Spike Encoding ---\ndef spike_converter(x, time_steps=100, flatten=False):\n    batch_size = x.size(0)\n    if flatten:\n        x = x.view(batch_size, -1)\n        p = x.unsqueeze(1).repeat(1, time_steps, 1)\n        spikes = torch.bernoulli(p)\n        return spikes.permute(1, 0, 2)  # [T, B, N]\n    else:\n        p = x.unsqueeze(1).repeat(1, time_steps, 1, 1, 1)\n        spikes = torch.bernoulli(p)\n        return spikes.permute(1, 0, 2, 3, 4)  # [T, B, C, H, W]\n\n\n# --- 5. Training Loop ---\nprint(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(DEVICE), target.to(DEVICE)\n\n        # Convert static images to spike trains [T, B, ...]\n        spike_input = spike_converter(data, TIME_STEPS).to(DEVICE)\n        spike_input = spike_input * 10\n\n        # Define time grid for the solver\n        # Time goes from 0 to T_end. Step size depends on your physical time scaling.\n        data_time = torch.linspace(0, 1.0, TIME_STEPS + 1).to(DEVICE)\n\n        optimizer.zero_grad()\n\n        # Forward pass through fractional solver\n        # Output shape: [T, B, Classes]\n        output_seq = model(\n            spike_input, data_time, method=\"gl\", options={\"step_size\": 1.0}\n        )\n\n        # Decision strategy: Sum or Average spikes over time\n        output = output_seq.mean(dim=0)\n\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        pred = output.argmax(dim=1)\n        correct += pred.eq(target).sum().item()\n        total += target.size(0)\n\n    acc = 100.0 * correct / total\n    print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Acc={acc:.2f}%\")\n    torch.cuda.empty_cache()\n\n# --- 6. Evaluation ---\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data, target in test_loader:\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        spike_input = spike_converter(data, TIME_STEPS)\n        data_time = torch.linspace(0, 1.0, TIME_STEPS + 1).to(DEVICE)\n\n        output_seq = model(\n            spike_input, data_time, method=\"gl\", options={\"step_size\": 1.0}\n        )\n        output = output_seq.mean(dim=0)\n\n        pred = output.argmax(dim=1)\n        correct += pred.eq(target).sum().item()\n        total += target.size(0)\n    torch.cuda.empty_cache()\n\nprint(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n</code></pre></p>"},{"location":"get_start/introduction/#next-steps","title":"Next Steps","text":"<ul> <li>Different Neuron Types: Try different neuron types (<code>IFNeuron</code>).</li> <li>Experiment with \\(\\alpha\\): Try setting <code>alpha=1.0</code> to compare against standard LIF, or <code>alpha=0.6</code> for stronger memory effects.</li> <li>Learnable Orders: Enable <code>learn_alpha=True</code> to let the network discover the optimal memory depth per layer.</li> <li>Multi-term Dynamics: Explore <code>multi_coefficient</code> to simulate complex biological relaxation processes.</li> <li>Visualization: Plot the membrane potential over time to observe the power-law decay characteristic of fractional systems.</li> </ul> <p>spikeDE opens the door to physics-informed spiking networks\u2014where neural dynamics obey principled mathematical laws beyond simple recurrence. Happy spiking!</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Welcome to the spikeDE Tutorials section. These guides are designed to take you from understanding the core concepts of fractional spiking neurons to mastering advanced configurations for complex temporal dynamics.</p> <p>Whether you are building your first Spiking Neural Network (SNN) or researching novel fractional-order architectures, these tutorials provide the theoretical background and practical code examples you need.</p>"},{"location":"tutorials/#basics","title":"Basics","text":"<p>Foundational concepts for building and training fractional SNNs.</p> <ul> <li> <p> Neuron</p> <p>Learn how spikeDE reimagines neurons as continuous dynamical systems. Discover how to upgrade standard Integrate-and-Fire models into Fractional-Order neurons with infinite memory.</p> <p> Read Tutorial</p> </li> <li> <p> Surrogate Gradient</p> <p>Overcome the non-differentiable nature of spiking. Explore various surrogate functions (Sigmoid, Arctan, etc.) that enable end-to-end backpropagation in SNNs.</p> <p> Read Tutorial</p> </li> <li> <p> Solver</p> <p>Understand the numerical engines powering temporal memory. Compare methods like Gr\u00fcnwald-Letnikov, L1, and Product Trapezoidal for solving Fractional Differential Equations.</p> <p> Read Tutorial</p> </li> </ul>"},{"location":"tutorials/#intermediate","title":"Intermediate","text":"<p>Advanced techniques for customizing network dynamics and architecture.</p> <ul> <li> <p> ODE Function</p> <p>Dive into the graph transformation process. See how <code>ODEFuncFromFX</code> uses PyTorch FX to convert discrete networks into continuous vector fields compatible with ODE solvers.</p> <p> Read Tutorial</p> </li> <li> <p> SNN Wrapper</p> <p>Master the central orchestrator. Learn to configure <code>SNNWrapper</code> for automatic architecture inference, input interpolation, and seamless switching between integer and fractional modes.</p> <p> Read Tutorial</p> </li> <li> <p> Per-Layer Alpha</p> <p>Customize memory dynamics with surgical precision. Configure heterogeneous fractional orders (\\(\\alpha\\)) per layer, enable multi-term derivatives, and make memory depth learnable.</p> <p> Read Tutorial</p> </li> </ul>"},{"location":"tutorials/#advanced","title":"Advanced","text":"<p>Real-world applications and complex task implementations directly adapted from our published research.</p> <ul> <li> <p> Neuromorphic Task </p> <p>Explore event-driven vision experiments. Learn how f-SNNs outperform traditional models on neuromorphic datasets like DVS128 Gesture and N-Caltech101 by capturing long-range temporal correlations.  </p> <p> Read Tutorial</p> </li> <li> <p> Graph Learning Task </p> <p>Dive into graph-structured data processing. Discover how fractional-order dynamics enhance node classification accuracy and robustness on citation and co-purchase networks compared to integer-order baselines.  </p> <p> Read Tutorial</p> </li> </ul>"},{"location":"tutorials/#whats-next","title":"What's Next?","text":"<p>Now that you have explored the core components and advanced configurations of spikeDE, you are ready to build your own models.</p> <ul> <li> Ready to code? Check out the Introduction by Example in the Getting Started section for a complete end-to-end workflow.</li> <li> Need detailed specs? Visit the API Reference for comprehensive documentation on classes, methods, and parameters.</li> </ul> <p>Happy spiking!</p>"},{"location":"tutorials/advanced/graph/","title":"Graph Learning Tasks","text":"<p>This guide provides an overview of the graph learning experiments conducted with spikeDE. These experiments demonstrate the effectiveness of our fractional-order Spiking Neural Networks (f-SNNs) in capturing long-range dependencies on graph-structured data, outperforming traditional integer-order SNNs.</p> <p>All source code for these experiments is open-source and reproducible. You can access the specific implementation scripts and configurations in our  GitHub repository.</p>"},{"location":"tutorials/advanced/graph/#overview","title":"Overview","text":"<p>Traditional Spiking Neural Networks (SNNs) typically model neuron dynamics using first-order Ordinary Differential Equations (ODEs), which assume a Markovian property where the current state depends only on the immediate past. This limits their ability to capture complex temporal correlations often present in graph data sequences.</p> <p>Our f-SNN framework integrates Fractional Differential Equations (FDEs) into Spiking Graph Neural Networks (SGNNs). By replacing standard integer-order neurons with fractional-order neurons, our models introduce a power-law memory kernel. This allows the network to retain information from distant past time steps, leading to:</p> <ul> <li>Higher Node Classification Accuracy: Consistently outperforming baselines like SpikingJelly and snnTorch across multiple datasets.</li> <li>Enhanced Robustness: Superior stability against feature masking and structural perturbations (edge dropping).</li> <li>Energy Efficiency: Comparable or lower energy consumption due to optimized firing rates, despite the added expressivity of fractional dynamics.</li> </ul>"},{"location":"tutorials/advanced/graph/#datasets","title":"Datasets","text":"<p>We evaluated our framework on six mainstream graph learning benchmarks, covering citation networks, co-purchase graphs, and large-scale academic graphs:</p> Dataset Description Task # Nodes # Classes Cora Citation network of machine learning papers. Node Classification 2,708 7 Citeseer Citation network of scientific publications. Node Classification 3,327 6 Pubmed Citation network of biomedical articles. Node Classification 19,717 3 Photo Amazon co-purchase graph (cameras). Node Classification 7,650 8 Computers Amazon co-purchase graph (computers). Node Classification 13,752 10 ogbn-arxiv Large-scale citation network of arXiv papers. Node Classification 169,343 40"},{"location":"tutorials/advanced/graph/#experimental-setup","title":"Experimental Setup","text":""},{"location":"tutorials/advanced/graph/#architecture-baselines","title":"Architecture &amp; Baselines","text":"<p>To ensure fair comparison, we integrated our f-LIF neurons into two established Spiking Graph Neural Network architectures:</p> <ul> <li>SGCN (Spiking Graph Convolutional Network)</li> <li>DRSGNN (Dynamic Reactive Spiking Graph Neural Network)</li> </ul> <p>Baselines: We compared our results against the original implementations of these models using standard LIF neurons from popular frameworks including SpikingJelly and snnTorch.</p>"},{"location":"tutorials/advanced/graph/#key-hyperparameters","title":"Key Hyperparameters","text":"<ul> <li>Encoding: Poisson spike encoding.</li> <li>Time Steps (\\(T\\)): 100 steps for all graph tasks.</li> <li>Batch Size: 32.</li> <li>Data Split: Training/Validation/Test ratio of 0.7/0.2/0.1.</li> <li>Fractional Order (\\(\\alpha\\)): Tuned per dataset. Setting \\(\\alpha=1\\) recovers the standard integer-order model.</li> <li>Positional Encoding (for DRSGNN): Dimension 32, using Laplacian Eigenvectors (LSPE) or Random Walk (RWPE).</li> </ul>"},{"location":"tutorials/advanced/graph/#key-results","title":"Key Results","text":"<p>Our fractional adaptations of SGCN and DRSGNN achieved superior accuracy across all datasets. For example, on the Cora dataset, SGCN (f-SNN) achieved 88.08% accuracy, significantly outperforming the SpikingJelly baseline (81.81%).</p> Method Cora Citeseer Pubmed Photo Computers ogbn-arxiv SGCN (SpikingJelly) 81.81 71.83 86.79 87.72 70.86 50.26 SGCN (snnTorch) 83.12 71.68 59.82 83.34 74.88 21.55 SGCN (f-SNN) 88.08 73.80 87.17 92.49 89.12 51.10 DRSGNN (SpikingJelly) 83.30 72.72 87.13 88.31 76.55 50.13 DRSGNN (snnTorch) 80.98 68.00 59.56 82.28 76.78 28.46 DRSGNN (f-SNN) 88.51 75.11 87.29 91.93 88.77 53.13 <p>Robustness Experiments</p> <p>The f-SNN framework demonstrated exceptional robustness under feature masking and edge dropping scenarios, maintaining high accuracy even when significant portions of graph information were corrupted. Detailed robustness analysis can be found in our ICLR 2026 Paper.</p>"},{"location":"tutorials/advanced/graph/#reproducing-the-results","title":"Reproducing the Results","text":"<p>The experiments are organized in the <code>examples/ICLR_Release/Graph</code> directory of our repository.</p>"},{"location":"tutorials/advanced/graph/#directory-structure","title":"Directory Structure","text":"<pre><code>examples/ICLR_Release/Graph/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 models.py             # SGCN and DRSGNN definitions with f-LIF support\n\u251c\u2500\u2500 run.py                # Main entry point for training and evaluation\n\u251c\u2500\u2500 utils.py              # Data loading and preprocessing utilities\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 run_sgcn_cora.sh\n    \u251c\u2500\u2500 run_drsgnn_citeseer.sh\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"tutorials/advanced/graph/#running-the-experiments","title":"Running the Experiments","text":"<p>You can reproduce the results using the provided shell scripts or by running the main python script directly.</p> <p>Using Shell Scripts: <pre><code># Run SGCN on Cora\nbash scripts/run_sgcn_cora.sh\n\n# Run DRSGNN on Citeseer\nbash scripts/run_drsgnn_citeseer.sh\n</code></pre></p> <p>Using Command Line: <pre><code>python run.py \\\n  --backbone SGCN \\\n  --dataset Cora \\\n  --time_steps 100 \\\n  --alpha 0.3 \\\n  --epochs 100\n</code></pre></p> <p>For detailed dataset download links and specific configuration parameters, please refer to the <code>README.md</code> inside the <code>examples/ICLR_Release/Graph</code> folder.</p> <p>Tip</p> <p>For more theoretical details on why fractional calculus improves graph learning, please refer to our ICLR 2026 Paper or other tutorials in this documentation.</p>"},{"location":"tutorials/advanced/neuromorphic/","title":"Neuromorphic Data Classification Tasks","text":"<p>This guide provides an overview of the neuromorphic event-driven vision experiments conducted with spikeDE. These experiments demonstrate the superiority of our fractional-order Spiking Neural Networks (f-SNNs) over traditional integer-order SNNs in processing spatiotemporal correlations inherent in neuromorphic data.</p> <p>All source code for these experiments is open-source and reproducible. You can access the specific implementation scripts and configurations in our  GitHub repository.</p>"},{"location":"tutorials/advanced/neuromorphic/#overview","title":"Overview","text":"<p>Neuromorphic data, captured by Dynamic Vision Sensors (DVS), consists of asynchronous event streams with microsecond-level temporal resolution. Traditional SNNs often struggle to capture long-range temporal dependencies due to their Markovian nature (memoryless). </p> <p>Our f-SNN framework replaces standard integer-order neuron dynamics with fractional-order differential equations (f-ODEs). This introduces a power-law memory kernel, allowing the network to retain information from distant past events, resulting in:</p> <ul> <li>Higher Classification Accuracy: Consistently outperforming LIF-based baselines (SpikingJelly, snnTorch).</li> <li>Enhanced Robustness: Superior stability against noise, occlusion, and temporal jitter.</li> <li>Energy Efficiency: Comparable energy consumption despite the added computational complexity of fractional dynamics, thanks to lower average firing rates.</li> </ul>"},{"location":"tutorials/advanced/neuromorphic/#datasets","title":"Datasets","text":"<p>We evaluated our framework on five major neuromorphic benchmarks:</p> Dataset Description Task N-MNIST Neuromorphic version of MNIST generated by moving digits in front of a DVS. Digit Classification (10 classes) DVS128 Gesture 11 dynamic hand gestures performed by 29 subjects under varying lighting. Gesture Recognition N-Caltech101 Event streams generated from static Caltech101 images via camera motion. Object Classification (101 classes) DVS-Lip High-temporal resolution recordings of lip movements for visual speech recognition. Lip Reading HarDVS A large-scale dataset containing over 100k samples of human activities. Human Action Recognition (300 classes)"},{"location":"tutorials/advanced/neuromorphic/#experimental-setup","title":"Experimental Setup","text":""},{"location":"tutorials/advanced/neuromorphic/#architecture-baselines","title":"Architecture &amp; Baselines","text":"<p>We implemented two backbone architectures to ensure fair comparison:</p> <ol> <li>CNN-based: Following the <code>DVSNet</code> architecture.</li> <li>Transformer-based: Using the <code>Spikformer</code> backbone.</li> </ol> <p>Baselines: We compared our f-LIF neurons against standard LIF neurons implemented in popular frameworks including SpikingJelly and snnTorch.</p>"},{"location":"tutorials/advanced/neuromorphic/#key-hyperparameters","title":"Key Hyperparameters","text":"<ul> <li>Optimizer: Adam</li> <li>Time Steps (\\(T\\)): <ul> <li>16 steps for N-MNIST, DVS128 Gesture, N-Caltech101, and DVS-Lip.</li> <li>8 steps for HarDVS (due to large sequence length).</li> </ul> </li> <li>Fractional Order (\\(\\alpha\\)): Tuned per dataset (e.g., \\(\\alpha=0.5\\) for DVS128 Gesture, \\(\\alpha=0.8\\) for HarDVS). Setting \\(\\alpha=1\\) recovers the standard integer-order model.</li> <li>Preprocessing: Event data was converted into frame representations using the standard SpikingJelly pipeline. Input resolution was uniformly adjusted to \\(128 \\times 128\\).</li> </ul>"},{"location":"tutorials/advanced/neuromorphic/#key-results","title":"Key Results","text":"<p>Our f-SNN models achieved state-of-the-art or superior results across all datasets. For instance, on the DVS128 Gesture dataset using a Transformer backbone, f-SNN achieved 95.83% accuracy, significantly outperforming the SpikingJelly baseline (93.40%).</p> Dataset Architecture LIF (SpikingJelly) LIF (snnTorch) f-LIF (spikeDE) N-MNIST CNN 99.27% 99.08% 99.48% DVS128 Gesture Transformer 93.40% 88.99% 95.83% N-Caltech101 Transformer 72.63% 65.67% 76.27% HarDVS CNN 46.10% 46.26% 47.66% <p>Robustness Experiments</p> <p>The f-SNN framework consistently maintained higher accuracy under high-intensity noise and occlusion compared to integer-order baselines, validating the theoretical stability of fractional-order systems. You can find the detailed experiments in our ICLR 2026 Paper.</p>"},{"location":"tutorials/advanced/neuromorphic/#reproducing-the-results","title":"Reproducing the Results","text":"<p>The experiments are organized in the <code>examples/ICLR_Release/Neuromorpic</code> directory of our repository.</p>"},{"location":"tutorials/advanced/neuromorphic/#directory-structure","title":"Directory Structure","text":"<pre><code>examples/ICLR_Release/Neuromorpic/\n\u251c\u2500\u2500 file/\n\u2502   \u251c\u2500\u2500 scripts/          # Launch scripts (e.g., run_n101_transformer.sh)\n\u2502   \u2514\u2500\u2500 logs/             # Training logs\n\u251c\u2500\u2500 train_n101_transformer.py\n\u251c\u2500\u2500 train_hardvs_cnn.py\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"tutorials/advanced/neuromorphic/#running-the-experiments","title":"Running the Experiments","text":"<p>You can run specific tasks using the provided shell scripts:</p> <pre><code># Run N-Caltech101 with Transformer\nbash file/scripts/run_n101_transformer.sh\n\n# Run HarDVS with CNN\nbash file/scripts/run_hardvs_cnn.sh\n\n# Or run all neuromorphic tasks\nbash file/scripts/run_all.sh\n</code></pre> <p>For detailed dataset download links and MD5 checksums, please refer to the <code>README.md</code> in the <code>examples/ICLR_Release/Neuromorpic</code> folder.</p> <p>Tip</p> <p>For more theoretical details on fractional calculus in SNNs, please refer to other tutorials in this documentation or our ICLR 2026 Paper.</p>"},{"location":"tutorials/basics/neuron/","title":"Neurons in spikeDE: Bridging Standard Dynamics and Fractional Calculus","text":"<p>In spikeDE, the neuron is reimagined. We move away from the traditional step-by-step update rule found in frameworks like SpikingJelly and towards a continuous dynamical system perspective. This shift allows us to seamlessly upgrade standard integer-order neurons into Fractional-Order Spiking Neurons, endowing them with infinite memory and complex temporal dynamics without rewriting your core logic.</p> <p>This tutorial introduces the core neuron models available in spikeDE, focusing on how fractional-order dynamics extend traditional spiking neural networks (SNNs). We will explore the theoretical motivation behind fractional calculus in neuroscience, examine the mathematical formulations of Integrate-and-Fire (IF) and Leaky Integrate-and-Fire (LIF) neurons within the spikeDE framework, and guide you through configuring, using, and customizing these neurons for your own research.</p>"},{"location":"tutorials/basics/neuron/#from-integer-to-fractional-order","title":"From Integer to Fractional Order","text":"<p>Traditional SNNs model neuronal membrane potential dynamics using integer-order ordinary differential equations (ODEs). In these models, the rate of change of the membrane potential \\(v(t)\\) at any instant depends solely on its current state and input, typically expressed as:</p> \\[ \\frac{dv(t)}{dt} = f(t, v(t), I_{\\text{in}}(t)) \\] <p>where \\(f(t, v, I_{\\text{in}})\\) represents the specific dynamics (e.g., leakage, input current). This formulation assumes the Markov property: the future state depends only on the present, with no memory of the past beyond the current value. While computationally efficient, this assumption limits the model's ability to capture complex temporal dependencies observed in biological neurons, such as long-range correlations and fractal dendritic structures.</p> <p>Fractional calculus offers a powerful generalization. By replacing the integer-order derivative \\(\\frac{d}{dt}\\) with a fractional-order derivative \\(D^\\alpha\\) (where \\(0 &lt; \\alpha \\leq 1\\)), we introduce non-locality and memory into the system:</p> \\[ D^\\alpha v(t) = f(t, v(t), I_{\\text{in}}(t)) \\] <p>The Caputo fractional derivative, commonly used in spikeDE, is defined as:</p> \\[ D^\\alpha v(t) = \\frac{1}{\\Gamma(1-\\alpha)} \\int_0^t (t-\\tau)^{-\\alpha} v'(\\tau) d\\tau \\] <p>Here, the current rate of change depends on a weighted integral of the entire history of the function \\(v(\\tau)\\), with weights decaying as a power law \\((t-\\tau)^{-\\alpha}\\). </p> <ul> <li>When \\(\\alpha = 1\\), the model recovers the standard integer-order ODE (memoryless).</li> <li>When \\(0 &lt; \\alpha &lt; 1\\), the system exhibits long-term memory: past states influence the present, enabling richer temporal patterns and improved robustness to noise.</li> </ul> <p>This transition from integer to fractional order allows spikeDE to model biological phenomena like spike-frequency adaptation and heavy-tailed relaxation processes that integer-order models cannot capture.</p>"},{"location":"tutorials/basics/neuron/#available-neurons-in-spikede","title":"Available Neurons in spikeDE","text":"<p>spikeDE currently provides two foundational neuron models, both extended to support fractional-order dynamics: the Integrate-and-Fire (IF) and Leaky Integrate-and-Fire (LIF) neurons. Below are their mathematical formulations.</p>"},{"location":"tutorials/basics/neuron/#fractional-integrate-and-fire","title":"Fractional Integrate-and-Fire","text":"<p>The standard IF neuron integrates input current without leakage. Its fractional-order extension is governed by:</p> \\[ \\tau D^\\alpha v(t) = I_{\\text{in}}(t) \\] <p>where:</p> <ul> <li>\\(v(t)\\) is the membrane potential.</li> <li>\\(I_{\\text{in}}(t)\\) is the input current.</li> <li>\\(\\tau\\) is the membrane time constant.</li> <li>\\(D^\\alpha\\) is the Caputo fractional derivative of order \\(\\alpha\\).</li> </ul> <p>When \\(\\alpha=1\\), this reduces to the classic IF equation \\(\\tau \\frac{dv}{dt} = I_{\\text{in}}(t)\\).</p>"},{"location":"tutorials/basics/neuron/#fractional-leaky-integrate-and-fire","title":"Fractional Leaky Integrate-and-Fire","text":"<p>The LIF neuron includes a leakage term that drives the potential toward zero. Its fractional dynamics are described by:</p> \\[ \\tau D^\\alpha v(t) = -v(t) + I_{\\text{in}}(t) \\] <p>Key properties:</p> <ul> <li>Memory Effect: The term \\(D^\\alpha v(t)\\) incorporates the history of \\(v(t)\\), leading to power-law relaxation (Mittag-Leffler decay) instead of exponential decay.</li> <li>Generalization: Setting \\(\\alpha=1\\) recovers the standard LIF equation \\(\\tau \\frac{dv}{dt} = -v(t) + I_{\\text{in}}(t)\\).</li> </ul> <p>Both models employ the same spiking mechanism: a spike \\(S(t)\\) is emitted when \\(v(t)\\) crosses a threshold \\(\\theta\\), followed by a reset (soft or hard). The fractional dynamics specifically govern the charging phase between spikes.</p>"},{"location":"tutorials/basics/neuron/#configuration-and-usage","title":"Configuration and Usage","text":"<p>In spikeDE, neurons are implemented as stateless modules that compute the derivative \\(dv/dt\\) (or the fractional equivalent) given the current membrane potential and input. The actual time integration and state management are handled by the <code>SNNWrapper</code> and its associated solvers.</p>"},{"location":"tutorials/basics/neuron/#basic-neuron-initialization","title":"Basic Neuron Initialization","text":"<p>You can instantiate neurons directly from the spikeDE library. </p> <pre><code>from spikeDE import LIFNeuron, IFNeuron\n\n# Initialize a standard LIF neuron\nlif_neuron = LIFNeuron(\n    tau=2.0, \n    threshold=1.0, \n    surrogate_opt='arctan',\n    tau_learnable=False\n)\n\n# Initialize an IF neuron with learnable tau\nif_neuron = IFNeuron(\n    tau=1.5, \n    threshold=0.8, \n    surrogate_opt='sigmoid',\n    tau_learnable=True\n)\n</code></pre> <p>Key hyperparameters include:</p> <ul> <li><code>tau</code>: Membrane time constant (\\(\\tau\\)).</li> <li><code>threshold</code>: Firing threshold (\\(\\theta\\)).</li> <li><code>surrogate_opt</code>: Surrogate gradient function for backpropagation (e.g., <code>'arctan'</code>, <code>'sigmoid'</code>).</li> <li><code>tau_learnable</code>: Whether \\(\\tau\\) is a learnable parameter.</li> </ul>"},{"location":"tutorials/basics/neuron/#integrating-with-snnwrapper","title":"Integrating with SNNWrapper","text":"<p>To leverage fractional-order dynamics, you must wrap your network using <code>SNNWrapper</code>. This wrapper handles the fractional integration (using methods like Gr\u00fcnwald-Letnikov) and manages the memory history required for \\(D^\\alpha\\).</p> <pre><code>import torch.nn as nn\nfrom spikeDE import SNNWrapper, LIFNeuron\n\nclass MySNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 20)\n        self.lif1 = LIFNeuron(tau=2.0)\n        self.fc2 = nn.Linear(20, 5)\n        self.lif2 = LIFNeuron(tau=2.0)\n\n    def forward(self, x):\n        x = self.lif1(self.fc1(x))\n        x = self.lif2(self.fc2(x))\n        return x\n\n# Wrap the network with fractional dynamics\nnet = SNNWrapper(\n    MySNN(),\n    integrator='fdeint',      # Use fractional ODE solver\n    alpha=0.7,                # Fractional order \u03b1 (0 &lt; \u03b1 \u2264 1)\n)\n\n# Critical: Initialize shapes before training\nnet._set_neuron_shapes(input_shape=(1, 10))\n</code></pre> <p>Key Hyperparameters in <code>SNNWrapper</code>:</p> <ul> <li><code>integrator</code>: Set to <code>'fdeint'</code> for fractional dynamics or <code>'odeint'</code> for integer-order.</li> <li><code>alpha</code>: The fractional order \\(\\alpha\\). Can be a scalar (same for all layers) or a list (per-layer).</li> </ul>"},{"location":"tutorials/basics/neuron/#define-your-own-neuron","title":"Define Your Own Neuron","text":"<p>One of the core design principles of spikeDE is the separation of neuron dynamics from state evolution. </p> <ul> <li>Neuron Module: Stateless. It accepts the current membrane potential \\(v_{mem}\\) and input current, then returns the derivative \\(dv/dt\\) and the spike signal. It does not store history or update state.</li> <li>Solver (<code>SNNWrapper</code>): Stateful. It maintains the history of \\(v_{mem}\\) required for fractional integration and updates the state over time using numerical schemes (e.g., Gr\u00fcnwald-Letnikov, Adams-Bashforth-Moulton).</li> </ul> <p>This architecture allows you to easily define custom neurons by simply implementing the <code>forward</code> method to compute the derivative. To create a custom neuron, inherit from <code>BaseNeuron</code> and override the <code>forward</code> method. You must return a tuple <code>(dv_dt, spike)</code>.</p> <pre><code>import torch\nimport torch.nn as nn\nfrom spikeDE.neuron import BaseNeuron\n\nclass CustomNeuron(BaseNeuron):\n    def forward(self, v_mem, current_input):\n        if current_input is None:\n            return v_mem\n\n        tau = self.get_tau()\n\n        # Define your custom dynamics here\n        # Example: A quadratic leak term instead of linear\n        dv_no_reset = (-v_mem**2 + current_input) / tau\n\n        # Compute post-charge potential (for spike generation)\n        v_post_charge = v_mem + dv_no_reset  # Assuming dt=1.0\n\n        # Generate spike using surrogate gradient\n        spike = self.surrogate_f(\n            v_post_charge - self.threshold, \n            self.surrogate_grad_scale\n        )\n\n        # Compute final derivative including reset effect\n        dv_dt = dv_no_reset - (spike.detach() * self.threshold) / tau\n\n        return dv_dt, spike\n</code></pre>"},{"location":"tutorials/basics/neuron/#how-fractional-dynamics-are-applied","title":"How Fractional Dynamics Are Applied","text":"<p>Once you define your custom neuron, <code>SNNWrapper</code> automatically applies fractional integration to it. You do not need to implement the fractional derivative logic inside the neuron itself.</p> <p>When <code>integrator='fdeint'</code> is set in <code>SNNWrapper</code>:</p> <ol> <li>The solver collects the history of \\(v_{mem}\\) for each layer.</li> <li>At each time step, it computes the fractional derivative approximation (e.g., using Gr\u00fcnwald-Letnikov coefficients) based on this history.</li> <li>It combines this with the \\(dv/dt\\) returned by your neuron to update the state.</li> </ol> <p>This means your custom neuron works seamlessly with both integer-order (<code>odeint</code>) and fractional-order (<code>fdeint</code>) solvers without modification. The <code>SNNWrapper</code> acts as the bridge, injecting the memory effects required for fractional calculus while keeping the neuron definition clean and focused on instantaneous dynamics.</p>"},{"location":"tutorials/basics/neuron/#summary","title":"Summary","text":"<p>In this tutorial, we explored how spikeDE extends traditional spiking neurons with fractional-order dynamics:</p> <ul> <li>From Integer to Fractional: We discussed how replacing \\(\\frac{d}{dt}\\) with \\(D^\\alpha\\) introduces memory and non-locality, enabling the modeling of complex temporal dependencies.</li> <li>Available Neurons: spikeDE supports f-IF and f-LIF neurons, which generalize their integer-order counterparts via fractional differential equations.</li> <li>Configuration: Neurons are configured with standard hyperparameters (<code>tau</code>, <code>threshold</code>), while fractional behavior is controlled via <code>SNNWrapper</code> arguments like <code>alpha</code> and <code>integrator</code>.</li> <li>Customization: By separating dynamics (neuron) from integration (solver), spikeDE allows users to define custom neurons easily. The <code>SNNWrapper</code> handles the complexity of fractional history management, ensuring your custom models benefit from fractional dynamics automatically.</li> </ul> <p>With these tools, you can build powerful, biologically plausible SNNs capable of capturing long-range temporal correlations and achieving superior robustness in real-world tasks.</p>"},{"location":"tutorials/basics/solver/","title":"Solvers in spikeDE: Powering Temporal Memory","text":"<p>In spikeDE, the solver is the engine that transforms standard neural dynamics into fractional-order systems. While your neuron defines the local instantaneous behavior (the derivative \\(f(t, v)\\)), the solver handles the global time evolution, enforcing the power-law memory kernel that characterizes fractional calculus.</p> <p>This guide details the numerical methods available in spikeDE, explains how to configure them for single-term or multi-term fractional orders, and demonstrates how to balance accuracy with computational efficiency using memory truncation.</p>"},{"location":"tutorials/basics/solver/#the-solvers-role-from-local-derivative-to-global-history","title":"The Solver's Role: From Local Derivative to Global History","text":"<p>Traditional SNN frameworks typically employ simple Euler integration, where the state at time \\(t\\) depends solely on the state at \\(t-\\Delta t\\). This Markovian property limits the network's ability to retain long-term temporal context.</p> <p>spikeDE replaces this with rigorous Fractional Differential Equation (FDE) solvers. Mathematically, given a neuron defined by:</p> \\[ D^\\alpha v(t) = f(t, v(t)) \\] <p>The solver computes the state \\(v(t_k)\\) not just from the immediate past, but as a weighted convolution of the entire history:</p> \\[ v(t_k) = v(0) + \\frac{1}{\\Gamma(\\alpha)} \\int_0^{t_k} (t_k - \\tau)^{\\alpha-1} f(\\tau, v(\\tau)) d\\tau \\] <p>Key Responsibilities of the spikeDE Solver:</p> <ol> <li>History Management: Automatically maintains the buffer of past states \\([v_0, \\dots, v_{k-1}]\\).</li> <li>Weight Computation: Calculates the specific coefficients (e.g., Gr\u00fcnwald-Letnikov, L1, Adams-Bashforth) based on the fractional order \\(\\alpha\\).</li> <li>Convolution: Performs the weighted sum efficiently at every time step.</li> <li>Differentiation: Supports backpropagation through the integration steps (via Adjoint methods or direct unrolling), allowing \\(\\alpha\\) and network weights to be learned jointly.</li> </ol> <p>You do not need to implement these complex summations. You simply select a <code>method</code> and set <code>alpha</code>; the solver handles the rest.</p>"},{"location":"tutorials/basics/solver/#available-numerical-methods","title":"Available Numerical Methods","text":"<p>spikeDE implements four distinct numerical schemes, each rooted in a specific discretization strategy for fractional operators. Understanding their mathematical basis helps in selecting the right tool for your theoretical framework and accuracy requirements.</p>"},{"location":"tutorials/basics/solver/#grunwald-letnikov","title":"Gr\u00fcnwald-Letnikov","text":"<p>The Gr\u00fcnwald-Letnikov (GL) definition is often considered the most natural discrete analog of the fractional derivative. It derives directly from the limit definition of the derivative extended to non-integer orders.</p> <p>Mathematically, the \\(\\alpha\\)-th order derivative is approximated by a finite difference scheme utilizing generalized binomial coefficients:</p> \\[ D^\\alpha v(t) \\approx \\frac{1}{h^\\alpha} \\sum_{j=0}^{k} (-1)^j \\binom{\\alpha}{j} v(t - jh) \\] <p>where \\(\\binom{\\alpha}{j} = \\frac{\\Gamma(\\alpha+1)}{j!\\Gamma(\\alpha-j+1)}\\).</p> <ul> <li>Why it matters: The coefficients \\((-1)^j \\binom{\\alpha}{j}\\) decay algebraically, naturally encoding the power-law memory without requiring complex integral approximations.</li> <li>Multi-Term Capability: Because the GL definition is linear, it seamlessly extends to multi-term equations (\\(\\sum c_i D^{\\alpha_i} v = f\\)) by simply summing the weighted histories of different orders. This makes it the only native solver in spikeDE for multi-term dynamics.</li> <li>Convergence: It offers first-order accuracy \\(O(h)\\), which is sufficient for most deep learning applications where stochastic gradient noise dominates numerical error.</li> </ul>"},{"location":"tutorials/basics/solver/#product-trapezoidal","title":"Product Trapezoidal","text":"<p>The Product Trapezoidal method reformulates the FDE as a Volterra integral equation of the second kind and applies the trapezoidal rule to approximate the integral term.</p> <p>Instead of differencing the state \\(v\\), it integrates the function \\(f(t, v)\\):</p> \\[ v(t_k) = v(0) + \\frac{1}{\\Gamma(\\alpha)} \\int_0^{t_k} (t_k - \\tau)^{\\alpha-1} f(\\tau, v(\\tau)) d\\tau \\] <p>The integral is discretized by piecewise linear interpolation of \\(f\\), leading to weights that involve terms like \\((j+1)^{\\alpha+1} - 2j^{\\alpha+1} + (j-1)^{\\alpha+1}\\).</p> <ul> <li>Why it matters: By integrating rather than differencing, this method achieves second-order accuracy \\(O(h^2)\\) for smooth functions. It effectively smooths out high-frequency oscillations that might arise in the GL scheme.</li> <li>Limitation: The derivation assumes a single fractional order \\(\\alpha\\) for the kernel \\((t-\\tau)^{\\alpha-1}\\). Consequently, it cannot natively resolve multi-term sums with distinct exponents and will fallback to <code>gl</code> if such a configuration is detected.</li> </ul>"},{"location":"tutorials/basics/solver/#l1-scheme","title":"L1 Scheme","text":"<p>The L1 scheme is specifically tailored for the Caputo derivative, which is preferred in physical modeling because it allows for standard initial conditions (\\(v(0)=v_0\\)) rather than fractional ones.</p> <p>It approximates the Caputo derivative by assuming the function \\(v(t)\\) is piecewise linear over each time interval \\([t_j, t_{j+1}]\\). The resulting discretization uses weights derived from the integral of the slope:</p> \\[ D^\\alpha_C v(t_k) \\approx \\frac{1}{\\Gamma(2-\\alpha) h^\\alpha} \\sum_{j=0}^{k-1} b_j (v_{k-j} - v_{k-j-1}) \\] <p>where \\(b_j = (j+1)^{1-\\alpha} - j^{1-\\alpha}\\).</p> <ul> <li>Why it matters: The L1 scheme provides an accuracy of \\(O(h^{2-\\alpha})\\), which is superior to GL when \\(\\alpha\\) is close to 1. It is the standard choice for problems where the physical interpretation of the initial state is critical.</li> <li>Limitation: Like the trapezoidal method, the standard L1 formulation is derived for a single order \\(\\alpha\\). Multi-term configurations trigger an automatic fallback to the <code>gl</code> solver.</li> </ul>"},{"location":"tutorials/basics/solver/#adams-bashforth-predictor","title":"Adams-Bashforth Predictor","text":"<p>This method utilizes the explicit Adams-Bashforth formulation applied to the equivalent Volterra integral equation. Unlike the previous methods which may implicitly depend on the current state (requiring iteration for implicit schemes), this is a purely explicit predictor.</p> <p>It stores the history of the function evaluations \\(f_j = f(t_j, v_j)\\) rather than the states \\(v_j\\) themselves:</p> \\[ v_{k+1} = v_0 + \\frac{h^\\alpha}{\\Gamma(\\alpha+2)} \\left( f_{k+1}^P + \\sum_{j=0}^{k} a_{j,k} f_j \\right) \\] <p>Note</p> <p>In the explicit predictor variant used here, \\(f_{k+1}\\) is estimated or omitted depending on the specific variant implementation.</p> <ul> <li>Why it matters: It decouples the history storage from the state values, storing only the derivatives. This can be advantageous for specific stiff systems or when the function \\(f\\) is computationally cheaper to store than the full state vector in certain architectures.</li> <li>Usage: Primarily useful for fast, explicit stepping in Caputo-based systems where stability constraints are manageable.</li> </ul>"},{"location":"tutorials/basics/solver/#configuration-and-usage","title":"Configuration and Usage","text":"<p>Configuring the solver is done via the <code>SNNWrapper</code>. You can specify the method globally or let the system auto-select based on your \\(\\alpha\\) configuration.</p>"},{"location":"tutorials/basics/solver/#basic-single-term-configuration","title":"Basic Single-Term Configuration","text":"<p>For standard fractional neurons where each layer has a single \\(\\alpha\\):</p> <pre><code>from spikeDE import SNNWrapper\n\n# Wrap your base model\nnet = SNNWrapper(\n    base=my_model,\n    alpha=0.85,          # Fractional order (0 &lt; alpha &lt;= 1)\n    method='gl',         # Options: 'gl', 'trap', 'l1', 'pred'\n    memory=-1            # Use full history (-1 or None)\n)\n</code></pre>"},{"location":"tutorials/basics/solver/#multi-term-fractional-orders","title":"Multi-Term Fractional Orders","text":"<p>spikeDE uniquely supports multi-term fractional derivatives, where a layer's dynamics are governed by a sum of fractional operators:</p> \\[ \\sum_{i} c_i D^{\\alpha_i} v(t) = f(t, v) \\] <p>To enable this, pass a list of orders (and optional coefficients) to <code>alpha</code>. The solver will automatically switch to the robust Multi-Term GL backend.</p> <pre><code># Define multi-term alpha for a specific layer\n# Format: [alpha_1, alpha_2, ...]\nlayer_alpha = [0.3, 0.7] \ncoefficients = [1.0, 0.5] # Optional weights c_i\n\nnet = SNNWrapper(\n    base=my_model,\n    per_layer_alpha=[layer_alpha], # List of alphas per integrated layer\n    per_layer_coefficient=[coefficients],\n    method='glmulti' # multi-term support\n)\n</code></pre> <p>Automatic Fallback</p> <p>If you request <code>method='trap'</code> or <code>method='l1'</code> but provide a multi-term \\(\\alpha\\), spikeDE will issue a warning and automatically switch to the <code>gl</code> (Gr\u00fcnwald-Letnikov) solver, as it is the only method currently supporting multi-term formulations.</p>"},{"location":"tutorials/basics/solver/#managing-memory-complexity","title":"Managing Memory Complexity","text":"<p>A defining feature of fractional calculus is infinite memory. However, storing the entire history \\([v_0, \\dots, v_k]\\) becomes computationally expensive (\\(O(N^2)\\) complexity) for long time sequences.</p> <p>spikeDE addresses this with Short-Memory Principle truncation.</p>"},{"location":"tutorials/basics/solver/#the-memory-parameter","title":"The <code>memory</code> Parameter","text":"<p>You can limit the history length used in the convolution sum by setting the <code>memory</code> argument.</p> <pre><code>net = SNNWrapper(\n    base=my_model,\n    alpha=0.7,\n    method='gl',\n    memory=50  # Only look back 50 time steps\n)\n</code></pre> <ul> <li><code>memory=None</code> or <code>-1</code>: Full Memory. Uses all historical steps since \\(t=0\\). Highest accuracy, highest cost.</li> <li> <p><code>memory=M</code> (int): Truncated Memory. Only the last \\(M\\) time steps are considered. The convolution sum becomes:</p> \\[ v_k \\approx \\text{Initial Terms} + \\sum_{j=k-M}^{k} w_j f(t_j, v_j) \\] </li> </ul> <p>Choosing Memory Length</p> <p>For many physical and biological systems, the power-law kernel \\((t-\\tau)^{\\alpha-1}\\) decays sufficiently fast that old history contributes negligibly. A memory length of 50\u2013200 steps often provides an excellent trade-off between speed and accuracy for inference. For training or highly sensitive chaotic systems, consider using full memory.</p>"},{"location":"tutorials/basics/solver/#summary","title":"Summary","text":"Method Type Formulation Multi-Term Support Accuracy <code>gl</code> Explicit Riemann-Liouville Yes \\(O(h)\\) <code>trap</code> Implicit-like Riemann-Liouville  (Fallback) \\(O(h^2)\\) <code>l1</code> Explicit Caputo  (Fallback) \\(O(h^{2-\\alpha})\\) <code>pred</code> Explicit Caputo  (Fallback) \\(O(h)\\) <p>For most applications in Spiking Neural Networks, <code>gl</code> is recommended due to its stability, support for learnable multi-term \\(\\alpha\\), and efficient implementation. Use <code>trap</code> or <code>l1</code> if you require higher precision for specific scientific modeling tasks and are working with single-term orders.</p>"},{"location":"tutorials/basics/surrogate/","title":"Surrogate Gradient Learning in spikeDE: Differentiating the Non-Differentiable","text":"<p>Training Spiking Neural Networks (SNNs) presents a fundamental challenge: the spiking mechanism is inherently non-differentiable. The generation of a spike is typically modeled by the Heaviside step function, which has a zero gradient almost everywhere and an undefined gradient at the threshold. This discontinuity prevents the direct application of standard backpropagation algorithms.</p> <p>spikeDE overcomes this barrier using Surrogate Gradient (SG) learning. This technique decouples the forward and backward passes:</p> <ul> <li>Forward Pass: The network uses the true, hard Heaviside step function to generate discrete spikes, preserving the event-driven nature and energy efficiency of SNNs.</li> <li>Backward Pass: During gradient computation, the undefined derivative of the step function is replaced by a smooth, differentiable surrogate function.</li> </ul> <p>This approach allows gradients to flow through the spiking nonlinearity, enabling end-to-end training of fractional-order spiking networks using standard optimizers like Adam or SGD.</p>"},{"location":"tutorials/basics/surrogate/#the-mathematical-formulation","title":"The Mathematical Formulation","text":"<p>Let \\(U(t)\\) be the membrane potential and \\(\\theta\\) be the firing threshold. The spike output \\(S(t)\\) in the forward pass is defined as:</p> \\[ S(t) = H(U(t) - \\theta) = \\begin{cases} 1 &amp; \\text{if } U(t) \\geq \\theta \\\\ 0 &amp; \\text{if } U(t) &lt; \\theta \\end{cases} \\] <p>Since \\(\\frac{\\partial S}{\\partial U}\\) is undefined, we approximate it with a surrogate gradient function \\(\\sigma'(U - \\theta)\\) during backpropagation. The chain rule then becomes:</p> \\[ \\frac{\\partial \\mathcal{L}}{\\partial U} = \\frac{\\partial \\mathcal{L}}{\\partial S} \\cdot \\underbrace{\\sigma'(U - \\theta)}_{\\text{Surrogate Gradient}} \\] <p>where \\(\\mathcal{L}\\) is the loss function. The choice of \\(\\sigma'\\) significantly impacts convergence speed, stability, and final accuracy.</p>"},{"location":"tutorials/basics/surrogate/#available-surrogate-functions","title":"Available Surrogate Functions","text":"<p>spikeDE provides a suite of built-in surrogate gradient functions, each with distinct mathematical properties and hyperparameters. You can select them via the <code>surrogate</code> argument in your neuron configuration.</p>"},{"location":"tutorials/basics/surrogate/#sigmoid-surrogate","title":"Sigmoid Surrogate","text":"<p>The most widely used surrogate, derived from the derivative of the scaled sigmoid function. It provides a smooth, bell-shaped gradient centered at the threshold.</p> <ul> <li> <p>Formula:</p> \\[ \\sigma'(x) = \\kappa \\cdot \\text{sigmoid}(\\kappa x) \\cdot (1 - \\text{sigmoid}(\\kappa x)) \\] </li> <li> <p>Hyperparameter: <code>scale</code> (\\(\\kappa\\), default: 5.0). Controls the sharpness. Larger \\(\\kappa\\) approximates the true step function more closely but may lead to vanishing gradients.</p> </li> <li>Best For: General-purpose training; robust baseline for most architectures.</li> </ul>"},{"location":"tutorials/basics/surrogate/#arctangent-surrogate","title":"Arctangent Surrogate","text":"<p>Based on the derivative of the arctangent function. It features heavier tails compared to the sigmoid, allowing gradients to propagate even when the membrane potential is far from the threshold.</p> <ul> <li> <p>Formula:</p> \\[ \\sigma'(x) = \\frac{\\kappa}{1 + (\\kappa x)^2} \\] </li> <li> <p>Hyperparameter: <code>scale</code> (\\(\\kappa\\), default: 2.0).</p> </li> <li>Best For: Deep networks where gradient vanishing is a concern; scenarios requiring broader credit assignment.</li> </ul> <p>Note</p> <p>spikeDE implements a normalized variant to ensure stable gradient magnitudes.</p>"},{"location":"tutorials/basics/surrogate/#piecewise-linear-surrogate","title":"Piecewise Linear Surrogate","text":"<p>A computationally efficient approximation that defines a triangular window around the threshold. Gradients are constant within the window and zero outside.</p> <ul> <li> <p>Formula:</p> \\[ \\sigma'(x) = \\begin{cases} \\frac{1}{2\\gamma} &amp; \\text{if } |x| \\leq \\gamma \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] </li> <li> <p>Hyperparameter: <code>gamma</code> (\\(\\gamma\\), default: 1.0). Defines the width of the active region.</p> </li> <li>Best For: High-speed training on resource-constrained hardware; models where sparse gradient updates are preferred.</li> </ul>"},{"location":"tutorials/basics/surrogate/#gaussian-surrogate","title":"Gaussian Surrogate","text":"<p>Uses a normalized Gaussian function to approximate the derivative. It offers the smoothest profile with exponential decay, providing very localized gradient updates.</p> <ul> <li> <p>Formula:</p> \\[ \\sigma'(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{x^2}{2\\sigma^2}} \\] </li> <li> <p>Hyperparameter: <code>sigma</code> (\\(\\sigma\\), default: 1.0). Controls the spread of the gradient.</p> </li> <li>Best For: Precision tasks where only neurons very close to firing should receive updates.</li> </ul>"},{"location":"tutorials/basics/surrogate/#noisy-threshold-spike","title":"Noisy Threshold Spike","text":"<p>Instead of a hard spike in the forward pass, this method injects logistic noise into the threshold, creating a stochastic soft spike during training while reverting to a hard spike during inference.</p> <ul> <li>Mechanism: \\(S(t) = \\text{sigmoid}(\\kappa(U(t) - \\theta) + \\epsilon)\\), where \\(\\epsilon \\sim \\text{Logistic}(0, 1)\\).</li> <li>Best For: Improving exploration during training and enhancing robustness to input noise.</li> </ul>"},{"location":"tutorials/basics/surrogate/#configuration-and-usage","title":"Configuration and Usage","text":"<p>Configuring the surrogate gradient is straightforward within the <code>spikeDE</code> neuron definitions. You can specify the surrogate function by name when initializing your neurons:</p> <pre><code>from spikeDE import LIFNeuron\n\nneuron = LIFNeuron(\n    tau=2.0, \n    threshold=1.0, \n    surrogate='arctan',      # Options: 'sigmoid', 'arctan', 'linear', 'gaussian', 'noisy'\n    surrogate_scale=2.0      # Specific hyperparameter for the chosen surrogate\n)\n</code></pre>"},{"location":"tutorials/basics/surrogate/#summary","title":"Summary","text":"Method Formula Shape Tail Behavior Computational Cost Best Use Case <code>sigmoid</code> Bell-shaped Exponential decay Low Default; balanced performance. <code>arctan</code> Bell-shaped Heavy (polynomial) decay Low Deep networks; avoiding vanishing gradients. <code>linear</code> Rectangular Zero (hard cutoff) Lowest Fast training; sparse updates. <code>gaussian</code> Bell-shaped Exponential decay Medium Precision tuning; localized updates. <code>noisy</code> Stochastic Sigmoid Exponential decay Medium Robustness; exploration during training. <p>For most applications in spikeDE, the <code>sigmoid</code> surrogate with a scale of 5.0 provides an excellent trade-off between accuracy and convergence speed. </p>"},{"location":"tutorials/intermediate/odefunc_fx/","title":"ODEFuncFromFX: Transforming SNNs into Continuous Dynamical Systems","text":"<p>In spikeDE, the <code>ODEFuncFromFX</code> module serves as the critical bridge between discrete Spiking Neural Networks (SNNs) and continuous-time numerical solvers. While your neuron defines what happens locally (the derivative logic) and the solver handles how time evolves (the integration method), <code>ODEFuncFromFX</code> is responsible for rewiring your network architecture to make it compatible with ODE/FDE solvers.</p> <p>Traditional SNN frameworks operate on fixed discrete time steps (\\(t, t+1, t+2\\)). In contrast, numerical ODE solvers (like those in <code>torchdiffeq</code>) require a function \\(f(t, v)\\) that can evaluate the system state at any arbitrary continuous time point \\(t\\). This guide explains how <code>ODEFuncFromFX</code> uses PyTorch FX graph tracing to automatically transform your standard <code>nn.Module</code> into this continuous form, handling input interpolation, state separation, and boundary detection without requiring you to rewrite your model code.</p>"},{"location":"tutorials/intermediate/odefunc_fx/#the-core-challenge-discrete-vs-continuous","title":"The Core Challenge: Discrete vs. Continuous","text":"<p>In a standard PyTorch SNN, the forward pass looks like a rigid sequence:</p> <pre><code>graph LR\n    A[Input] --&gt; B[Conv Layer]\n    B --&gt; C[Neuron 1]\n    C --&gt; D[Conv Layer]\n    D --&gt; E[Neuron 2]\n    E --&gt; F[... ]\n    F --&gt; G[Neuron N]\n    G --&gt; H[Post-Processing]\n    H --&gt; I[Output]</code></pre> <p>At each discrete step \\(t\\), the neuron updates its membrane potential \\(v_t\\) based on the input \\(x_t\\) and previous state \\(v_{t-1}\\).</p> <p>However, an ODE solver needs a function that satisfies:</p> \\[ \\frac{dv}{dt} = f(t, v(t), x(t)) \\] <p>Here, \\(t\\) is a continuous scalar, and \\(x(t)\\) must be reconstructed from discrete samples via interpolation. <code>ODEFuncFromFX</code> automates the transformation of your discrete graph into this continuous vector field.</p>"},{"location":"tutorials/intermediate/odefunc_fx/#architecture-overview","title":"Architecture Overview","text":"<p>When you wrap your model with <code>ODEFuncFromFX</code>, it performs a \"surgical split\" of your computation graph into two distinct modules:</p> <ol> <li>The ODE Graph (<code>ode_gm</code>): Contains all logic required to compute derivatives. It accepts continuous time \\(t\\) and membrane potentials \\(v\\), interpolates inputs, and returns \\((\\frac{dv_1}{dt}, \\dots, \\frac{dv_N}{dt})\\) along with necessary boundary signals.</li> <li>The Post-Neuron Module (<code>post_neuron_module</code>): Contains all layers appearing after the last spiking neuron (e.g., voting layers, classification heads). This part remains discrete and is only executed once after the ODE integration finishes.</li> </ol>"},{"location":"tutorials/intermediate/odefunc_fx/#step-by-step-transformation-pipeline","title":"Step-by-Step Transformation Pipeline","text":"<p>To understand how <code>ODEFuncFromFX</code> works internally, let's trace the transformation of a simple 2-layer SNN:</p> <pre><code>class MySNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 20)\n        self.lif1 = LIFNeuron()  # Neuron Layer 0\n        self.fc2 = nn.Linear(20, 5)\n        self.lif2 = LIFNeuron()  # Neuron Layer 1 (Last Neuron)\n        self.classifier = nn.Linear(5, 2)\n\n    def forward(self, x):\n        x = self.lif1(self.fc1(x))\n        x = self.lif2(self.fc2(x))\n        return self.classifier(x)\n</code></pre>"},{"location":"tutorials/intermediate/odefunc_fx/#symbolic-tracing-with-custom-leaves","title":"Symbolic Tracing with Custom Leaves","text":"<p>The process begins by tracing the network using PyTorch FX. However, we cannot simply decompose everything. Neurons have special semantics (returning both derivative and spike), and post-processing layers should remain intact.</p> <p>We use a custom <code>SNNLeafTracer</code> that treats <code>BaseNeuron</code>, <code>VotingLayer</code>, and <code>ClassificationHead</code> as leaf nodes. This prevents the tracer from stepping inside these modules, preserving them as single atomic operations in the graph.</p>"},{"location":"tutorials/intermediate/odefunc_fx/#constructing-the-ode-interface","title":"Constructing the ODE Interface","text":"<p>A new graph is created with inputs specifically designed for ODE solvers:</p> <ul> <li><code>t</code>: The current continuous time point (scalar).</li> <li><code>v_mems</code>: A tuple of membrane potentials \\((v_1, v_2, \\dots)\\) for all neuron layers.</li> <li><code>x</code>: The original discrete input tensor (shape <code>[T, Batch, ...]</code>).</li> <li><code>x_time</code>: The timestamps corresponding to the input steps.</li> </ul> <p>Input Interpolation: Since the solver may query time \\(t=1.5\\) even if inputs only exist at \\(t=1\\) and \\(t=2\\), <code>ODEFuncFromFX</code> automatically inserts an interpolation node.</p> \\[ x(t) = \\text{interpolate}(x, x\\_time, t, \\text{method}=\\text{'linear'}) \\] <p>Supported methods include <code>'linear'</code>, <code>'nearest'</code>, <code>'cubic'</code>, and <code>'akima'</code>.</p>"},{"location":"tutorials/intermediate/odefunc_fx/#rewiring-neuron-layers","title":"Rewiring Neuron Layers","text":"<p>The transformer iterates through the original graph. When it encounters a <code>BaseNeuron</code>:</p> <ol> <li>It extracts the specific membrane potential \\(v_i\\) from the <code>v_mems</code> tuple.</li> <li>It calls the neuron with \\((v_i, \\text{input})\\).</li> <li>It splits the output:<ul> <li>The derivative \\(\\frac{dv_i}{dt}\\) is collected into the final output list.</li> <li>The spike output is passed forward to subsequent layers (just like in the original network).</li> </ul> </li> </ol> <p>This effectively changes the data flow: instead of updating state internally, the neuron now purely computes the rate of change given the current state.</p>"},{"location":"tutorials/intermediate/odefunc_fx/#intelligent-boundary-detection","title":"Intelligent Boundary Detection","text":"<p>One of the most sophisticated features of <code>ODEFuncFromFX</code> is determining where the ODE part ends and the post-processing begins. It doesn't just cut after the last neuron; it analyzes dependencies.</p> <p>If your network has branching paths or if post-neuron layers depend on intermediate tensors from earlier in the graph, the system identifies these as boundary nodes.</p> <ul> <li>Single Boundary: If the graph is linear, the output of the last neuron is passed directly to the post-module.</li> <li>Multi-Boundary: If the post-module depends on multiple tensors (e.g., skip connections), the ODE graph returns a tuple of all required boundary values, and the post-module is rewritten to unpack them.</li> </ul>"},{"location":"tutorials/intermediate/odefunc_fx/#finalizing-the-modules","title":"Finalizing the Modules","text":"<p>The result is two ready-to-use modules:</p> <ol> <li><code>self.ode_gm</code>: A <code>GraphModule</code> that computes \\(( \\frac{dv}{dt}, \\text{boundaries} )\\).</li> <li><code>self.post_neuron_module</code>: A <code>GraphModule</code> that takes <code>boundaries</code> and produces the final class logits.</li> </ol>"},{"location":"tutorials/intermediate/odefunc_fx/#configuration-and-usage","title":"Configuration and Usage","text":"<p>Using <code>ODEFuncFromFX</code> is typically handled internally by the <code>SNNWrapper</code>, but understanding the configuration helps in debugging and advanced customization.</p>"},{"location":"tutorials/intermediate/odefunc_fx/#basic-initialization","title":"Basic Initialization","text":"<pre><code>from spikeDE import ODEFuncFromFX, LIFNeuron\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 16, 3)\n        self.neuron = LIFNeuron(tau=2.0)\n        self.fc = nn.Linear(16, 10)\n\n    def forward(self, x):\n        x = self.neuron(self.conv(x))\n        return self.fc(x)\n\nmodel = MyModel()\n\n# Wrap with ODEFuncFromFX\node_func = ODEFuncFromFX(\n    backbone=model, \n    interpolation_method='linear'  # Options: 'linear', 'cubic', 'akima', 'nearest'\n)\n</code></pre>"},{"location":"tutorials/intermediate/odefunc_fx/#integration-with-solvers","title":"Integration with Solvers","text":"<p>Once wrapped, <code>ode_func</code> can be passed directly to an ODE solver. The <code>SNNWrapper</code> automates this loop:</p> <pre><code># Pseudo-code representation of what SNNWrapper does\nv_mems_init = [torch.zeros_like(p) for p in model.parameters() if 'mem' in p] # Simplified\n\n# Solve the ODE: integrates dv/dt over time\nresult = ode_solver(\n    func=ode_func, \n    y0=v_mems_init, \n    t_span=(0, T), \n    method='dopri5'\n)\n\n# Extract boundary outputs (spikes) from the solver result\n# and pass them to the post-neuron module\nfinal_output = ode_func.get_post_neuron_module&gt;(result.boundaries)\n</code></pre>"},{"location":"tutorials/intermediate/odefunc_fx/#selecting-interpolation-methods","title":"Selecting Interpolation Methods","text":"<p>The choice of interpolation affects how the network perceives input between time steps:</p> Method Description Best Use Case <code>'linear'</code> Linear interpolation between samples. Default. Balanced accuracy and speed for most event-based data. <code>'nearest'</code> Holds the value of the nearest sample (Zero-Order Hold). Digital inputs or when strict causality without smoothing is required. <code>'cubic'</code> Catmull-Rom cubic spline. Smooth sensory data (e.g., audio, video) where higher-order continuity helps. <code>'akima'</code> Akima spline (robust against oscillations). Data with sharp transitions where cubic splines might overshoot."},{"location":"tutorials/intermediate/odefunc_fx/#advanced-custom-neuron-compatibility","title":"Advanced: Custom Neuron Compatibility","text":"<p>A key design goal of <code>ODEFuncFromFX</code> is transparency. If you define a custom neuron by inheriting from <code>BaseNeuron</code>, it works automatically with this pipeline. You do not need to modify your neuron to support continuous time.</p> <p>As long as your custom neuron follows the standard signature: <pre><code>def forward(self, v_mem, x):\n    # Compute derivative\n    dv_dt = ...\n    # Compute spike\n    spike = ...\n    return dv_dt, spike\n</code></pre> <code>ODEFuncFromFX</code> will correctly trace it, extract the <code>dv_dt</code> for the solver, and route the <code>spike</code> to downstream layers.</p>"},{"location":"tutorials/intermediate/odefunc_fx/#summary","title":"Summary","text":"<p><code>ODEFuncFromFX</code> is the engine that enables continuous-depth Spiking Neural Networks. By leveraging PyTorch FX, it:</p> <ol> <li>Decouples state evolution (handled by solvers) from state definition (handled by neurons).</li> <li>Interpolates inputs dynamically, allowing evaluation at arbitrary time points.</li> <li>Splits the computation graph intelligently, ensuring efficient separation between continuous dynamics and discrete readout.</li> </ol> <p>This allows you to write standard PyTorch code while unlocking the power of adaptive step-size solvers, fractional-order calculus, and precise temporal modeling provided by spikeDE.</p>"},{"location":"tutorials/intermediate/per_layer_alpha/","title":"Per-Layer Alpha: Customizing Memory Dynamics in Fractional SNNs","text":"<p>In the realm of Fractional Spiking Neural Networks (f-SNNs), the fractional order \\(\\alpha\\) is not just a hyperparameter; it is the control knob for memory. It dictates how much historical information a neuron retains when computing its current state.</p> <p>While setting a global \\(\\alpha\\) for the entire network is a good starting point, biological neural systems and complex temporal tasks often require heterogeneous dynamics. Some layers might need to act as short-term buffers (high \\(\\alpha\\), close to 1.0), while others function as long-term integrators (low \\(\\alpha\\), closer to 0.0).</p> <p>The Per-Layer Alpha configuration in spikeDE empowers you to define these dynamics with surgical precision. You can assign unique fractional orders to each layer, enable multi-term distributed orders for complex time-scale modeling, and even make these orders learnable during training.</p>"},{"location":"tutorials/intermediate/per_layer_alpha/#the-physics-of-alpha-why-per-layer-matters","title":"The Physics of Alpha: Why Per-Layer Matters","text":"<p>Before diving into code, it's crucial to understand what changing \\(\\alpha\\) actually does to your network's behavior.</p> <p>The membrane potential \\(V(t)\\) in an f-SNN evolves according to the Caputo fractional derivative:</p> \\[ D^{\\alpha} V(t) = f(t, V(t)) \\] <ul> <li>\\(\\alpha \\approx 1.0\\) (Integer-like): The system has short memory. It behaves almost like a standard ODE, reacting quickly to recent inputs but forgetting the past rapidly. Ideal for layers detecting fast transients or edges.</li> <li>\\(\\alpha \\ll 1.0\\) (Strongly Fractional): The system has long memory. The current state is heavily influenced by the entire history of inputs due to the power-law kernel of the fractional derivative. Ideal for layers responsible for context accumulation, integration, or pattern recognition over long windows.</li> </ul> <p>By configuring <code>alpha</code> per layer, you allow your network to automatically specialize: early layers might process rapid signal changes, while deeper layers integrate evidence over time.</p>"},{"location":"tutorials/intermediate/per_layer_alpha/#configuration-modes","title":"Configuration Modes","text":"<p>spikeDE supports four distinct modes for configuring fractional orders, ranging from simple global settings to highly complex, layer-specific distributed orders.</p>"},{"location":"tutorials/intermediate/per_layer_alpha/#global-scalar","title":"Global Scalar","text":"<p>Every neuron layer in your network shares the exact same fractional order \\(\\alpha\\). This is equivalent to standard f-SNN behavior.</p> <pre><code># All layers use \u03b1 = 0.6\nnet = SNNWrapper(\n    my_model, \n    integrator='fdeint', \n    alpha=0.6, \n    learn_alpha=False\n)\n</code></pre>"},{"location":"tutorials/intermediate/per_layer_alpha/#per-layer-single-term","title":"Per-Layer Single-Term","text":"<p>Each layer \\(\\ell\\) is assigned its own specific order \\(\\alpha_\\ell\\). This is the most common advanced use case, allowing you to manually tune the \"memory depth\" of each stage in your network.</p> \\[ D^{\\alpha_\\ell} V_\\ell(t) = f_\\ell(t, V_\\ell(t)) \\] <p>Usage: <pre><code># Layer 0: Strong memory (\u03b1=0.3)\n# Layer 1: Weak memory (\u03b1=0.8)\nnet = SNNWrapper(\n    my_model, \n    integrator='fdeint', \n    alpha=[0.3, 0.8],       # List length must match number of neuron layers\n    alpha_mode='per_layer', # Crucial: Explicitly declare mode\n    learn_alpha=True        # Optional: Let the network optimize these values\n)\n</code></pre></p>"},{"location":"tutorials/intermediate/per_layer_alpha/#multi-term-broadcast","title":"Multi-Term Broadcast","text":"<p>Instead of a single derivative, each layer computes a weighted sum of multiple fractional derivatives. This allows a single layer to model dynamics across multiple time scales simultaneously. The same set of orders and weights is applied to all layers.</p> \\[ \\sum_{j} w_j D^{\\alpha_j} V_\\ell(t) = f_\\ell(t, V_\\ell(t)) \\] <p>Usage: <pre><code># All layers compute: 1.0*D^0.3 V + 0.5*D^0.5 V + 0.2*D^0.7 V\nnet = SNNWrapper(\n    my_model, \n    integrator='fdeint', \n    alpha=[0.3, 0.5, 0.7],             # The set of orders\n    multi_coefficient=[1.0, 0.5, 0.2], # Corresponding weights\n    alpha_mode='multiterm',            # Crucial: Declare multi-term mode\n    learn_coefficient=True             # Optional: Learn the weights w_j\n)\n</code></pre></p>"},{"location":"tutorials/intermediate/per_layer_alpha/#per-layer-multi-term","title":"Per-Layer Multi-Term","text":"<p>This is the most powerful configuration. Each layer can have its own unique set of fractional orders and weighting coefficients. This effectively gives every layer its own custom differential equation structure.</p> <p>Usage: <pre><code>net = SNNWrapper(\n    my_model, \n    integrator='fdeint', \n    # Layer 0: 2-term, Layer 1: 3-term\n    alpha=[\n        [0.3, 0.5],           # Orders for Layer 0\n        [0.4, 0.6, 0.8]       # Orders for Layer 1\n    ],\n    multi_coefficient=[\n        [1.0, 0.5],           # Weights for Layer 0\n        [1.0, 0.3, 0.1]       # Weights for Layer 1\n    ],\n    # Note: alpha_mode is auto-detected here due to nested list structure\n)\n</code></pre></p>"},{"location":"tutorials/intermediate/per_layer_alpha/#disambiguating-configuration","title":"Disambiguating Configuration","text":"<p>A common point of confusion arises when passing a flat list to <code>alpha</code>. For a 2-layer network, does <code>alpha=[0.3, 0.7]</code> mean:</p> <ol> <li>Layer 0 gets 0.3, Layer 1 gets 0.7?</li> <li>Both layers get a 2-term equation with orders [0.3, 0.7]?</li> </ol> <p>To prevent silent errors, spikeDE requires you to be explicit using the <code>alpha_mode</code> argument whenever ambiguity exists.</p> Input Structure <code>alpha_mode</code> Setting Resulting Behavior <code>0.5</code> (float) (ignored) Global Scalar: All layers \\(\\alpha=0.5\\). <code>[0.3, 0.7]</code> <code>'per_layer'</code> Per-Layer: Layer 0: \\(\\alpha_0\\), Layer 1: \\(\\alpha_1\\). <code>[0.3, 0.7]</code> <code>'multiterm'</code> Broadcast Multi-Term: All layers use terms \\(\\{0.3, 0.7\\}\\). <code>[[0.3], [0.7]]</code> (auto) Per-Layer Multi-Term: Detected automatically via nesting. <p>Best Practice</p> <p>Always explicitly set <code>alpha_mode='per_layer'</code> or <code>alpha_mode='multiterm'</code> when passing lists. Relying on heuristics (<code>alpha_mode='auto'</code>) may trigger warnings or unexpected behavior.</p>"},{"location":"tutorials/intermediate/per_layer_alpha/#making-dynamics-learnable","title":"Making Dynamics Learnable","text":"<p>One of the most compelling features of this framework is the ability to treat \\(\\alpha\\) and the multi-term coefficients as trainable parameters. Instead of manually searching for the optimal memory depth, you can let gradient descent find it.</p> <p>Simply set the <code>learn_alpha</code> or <code>learn_coefficient</code> flags. These parameters are automatically registered with your PyTorch module and included in <code>model.parameters()</code>.</p> <pre><code>net = SNNWrapper(\n    my_model,\n    integrator='fdeint',\n    alpha=[0.5, 0.5],          # Initial guess\n    alpha_mode='per_layer',\n    learn_alpha=True,          # Enable gradient updates for alpha\n    learn_coefficient=False\n)\n\n# Standard PyTorch optimization loop\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss = criterion(net(batch.x, batch.t), batch.y)\n    loss.backward()\n    optimizer.step()\n\n# Inspect learned values\nprint(\"Learned Alphas:\", net.get_per_layer_alpha())\n</code></pre> <p>During training, the network might discover that early layers perform better with \\(\\alpha \\approx 0.9\\) (fast reaction) while deeper layers converge to \\(\\alpha \\approx 0.4\\) (strong integration), adapting the mathematical structure of the network to the data distribution.</p>"},{"location":"tutorials/intermediate/per_layer_alpha/#summary","title":"Summary","text":"<p>The Per-Layer Alpha configuration transforms your SNN from a static architecture into a dynamically adaptable system. By decoupling the memory characteristics of each layer, you unlock:</p> <ul> <li>Specialization: Layers can optimize for different temporal frequencies.</li> <li>Expressivity: Multi-term configurations capture complex, non-Markovian dynamics.</li> <li>Automation: Learnable \\(\\alpha\\) removes the burden of manual hyperparameter tuning for fractional orders.</li> </ul> <p>Whether you are modeling biological plausibility or pushing the boundaries of temporal deep learning, per-layer control is the key to unlocking the full potential of fractional calculus in neural networks.</p>"},{"location":"tutorials/intermediate/snnWrapper/","title":"SNNWrapper: Building Continuous-Time Spiking Neural Networks","text":"<p>In the spikeDE framework, the <code>SNNWrapper</code> is the central orchestrator that transforms your standard, discrete-time PyTorch Spiking Neural Network (SNN) into a continuous-depth dynamical system. </p> <p>While other components handle specific roles\u2014Neurons define local dynamics, Solvers handle numerical integration, and ODEFunc rewires the graph\u2014the <code>SNNWrapper</code> brings them all together. It manages the lifecycle of the simulation, from inferring network architecture to configuring fractional memory dynamics and executing the time-stepping loop.</p> <p>This tutorial guides you through the philosophy, configuration, and usage of <code>SNNWrapper</code>, helping you unlock adaptive step-size solving, fractional-order calculus, and precise temporal modeling with minimal code changes.</p>"},{"location":"tutorials/intermediate/snnWrapper/#the-core-philosophy-separation-of-concerns","title":"The Core Philosophy: Separation of Concerns","text":"<p>Traditional SNN frameworks often couple the neuron's state update logic directly with the time-stepping loop. This makes it difficult to swap integration methods or introduce complex dynamics like fractional derivatives.</p> <p><code>SNNWrapper</code> adopts a modular architecture based on three distinct responsibilities:</p> <ol> <li>Architecture Inference: Automatically detecting neuron layers and output shapes without manual specification.</li> <li>Dynamics Configuration: Managing fractional orders (\\(\\alpha\\)) and memory coefficients per layer.</li> <li>Execution Engine: Delegating the actual integration to specialized solvers while handling data interpolation and boundary routing.</li> </ol> <p>This separation allows you to write standard PyTorch <code>nn.Module</code> code while gaining access to advanced continuous-time features.</p>"},{"location":"tutorials/intermediate/snnWrapper/#quick-start","title":"Quick Start","text":"<p>Wrapping your existing model is straightforward. The only requirement is that your network uses neurons compatible with spikeDE.</p> <pre><code>import torch.nn as nn\nfrom spikeDE import SNNWrapper, LIFNeuron\n\n# 1. Define your standard SNN\nclass MySNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 16, 3)\n        self.neuron = LIFNeuron(tau=2.0)\n        self.fc = nn.Linear(16, 10)\n\n    def forward(self, x):\n        # Standard discrete forward pass\n        x = self.neuron(self.conv(x))\n        return self.fc(x)\n\nmodel = MySNN()\n\n# 2. Wrap with SNNWrapper\nnet = SNNWrapper(\n    base=model,\n    integrator='fdeint',   # Use fractional solver\n    alpha=0.8,             # Global fractional order\n    method='gl'            # Gr\u00fcnwald-Letnikov scheme\n)\n\n# 3. Initialize shapes (Critical Step)\n# This triggers architecture inference\nnet._set_neuron_shapes(input_shape=(1, 3, 32, 32))\n\n# 4. Run simulation\n# Input shape: [Time_Steps, Batch, Channels, Height, Width]\nx_time = torch.linspace(0, 1, 100)\nx_input = torch.randn(100, 1, 3, 32, 32)\n\noutput = net(x_input, x_time)\n</code></pre> <p>Remember to Initialize the Neuron Shape</p> <p>The call to <code>_set_neuron_shapes</code> is mandatory before the first forward pass. It performs a dry run to detect the number of neuron layers and their output dimensions, which are required to initialize the fractional memory states.</p>"},{"location":"tutorials/intermediate/snnWrapper/#key-features","title":"Key Features","text":""},{"location":"tutorials/intermediate/snnWrapper/#automatic-architecture-inference","title":"Automatic Architecture Inference","text":"<p>You do not need to manually count neuron layers or specify tensor shapes. <code>SNNWrapper</code> uses PyTorch FX to trace your model graph:</p> <ul> <li>Neuron Detection: Identifies all instances of <code>BaseNeuron</code> in your network.</li> <li>Shape Recording: Runs a dummy forward pass to record the exact output shape of each neuron layer.</li> <li>Boundary Detection: Intelligently splits the graph into the ODE part and the Post-Neuron part, handling complex cases like skip connections automatically.</li> </ul>"},{"location":"tutorials/intermediate/snnWrapper/#flexible-fractional-configuration","title":"Flexible Fractional Configuration","text":"<p><code>SNNWrapper</code> supports heterogeneous memory dynamics, allowing you to configure the fractional order \\(\\alpha\\) globally, per layer, or as a multi-term sum. For detailed configuration guidelines and mode explanations, please refer to Per-Layer Alpha: Customizing Memory Dynamics in Fractional SNNs.</p>"},{"location":"tutorials/intermediate/snnWrapper/#internal-workflow","title":"Internal Workflow","text":"<p>Understanding the data flow helps in debugging and advanced customization.</p>"},{"location":"tutorials/intermediate/snnWrapper/#initialization","title":"Initialization","text":"<p>Before any simulation runs, the wrapper must understand the network's geometry.</p> <ol> <li>Tracing: It traces the <code>base</code> model using FX.</li> <li>Dry Run: Executes a forward pass with dummy data (zeros) to record:<ul> <li><code>neuron_shapes</code>: Output dimensions of every neuron layer.</li> <li><code>boundary_shapes</code>: Dimensions of tensors passed to the post-processing module.</li> </ul> </li> <li>Parameter Registration: Based on the detected layer count, it parses your <code>alpha</code> configuration and registers \\(\\alpha\\) and coefficients as <code>nn.Parameter</code> objects if <code>learn_alpha=True</code>.</li> <li>Compilation: Optionally compiles the ODE function using <code>torch.compile</code> for accelerated inference.</li> </ol>"},{"location":"tutorials/intermediate/snnWrapper/#forward-pass","title":"Forward Pass","text":"<p>During the actual simulation:</p> <ol> <li>Input Interpolation: The input \\(x(t)\\) is reconstructed from discrete samples using the specified method (linear, cubic, etc.) to allow evaluation at arbitrary time points \\(t\\).</li> <li>State Initialization: Membrane potentials \\(v_{mem}\\) and boundary states are initialized to zero (or custom values).</li> <li>Integration Loop: The selected solver (e.g., <code>gl_integrate_tuple</code>) iterates through time steps:<ul> <li>Calls the ODE function to compute derivatives \\((dv/dt)\\).</li> <li>Updates states using the fractional history buffer.</li> </ul> </li> <li>Boundary Processing: After integration completes, the final boundary outputs (spikes) are stacked and passed through the <code>post_neuron_module</code> (e.g., a classification head) to produce the final result.</li> </ol>"},{"location":"tutorials/intermediate/snnWrapper/#advanced-usage","title":"Advanced Usage","text":""},{"location":"tutorials/intermediate/snnWrapper/#custom-interpolation","title":"Custom Interpolation","text":"<p>By default, inputs are interpolated linearly. For smoother sensory data (like audio), you can use cubic splines: <pre><code>net = SNNWrapper(model, interpolation_method='cubic')\n</code></pre> Supported methods: <code>'linear'</code>, <code>'nearest'</code>, <code>'cubic'</code>, <code>'akima'</code>.</p>"},{"location":"tutorials/intermediate/snnWrapper/#solver-and-method-selection","title":"Solver and Method Selection","text":"<p>You can seamlessly switch between integer-order and fractional-order dynamics by selecting the appropriate <code>integrator</code>. For fractional solving, <code>SNNWrapper</code> supports various discretization schemes via the <code>method</code> argument.</p> <pre><code># Integer-order ODE solving\nnet_ode = SNNWrapper(model, integrator='odeint', method='euler')\n\n# Fractional solving with Gr\u00fcnwald-Letnikov scheme\nnet_gl = SNNWrapper(model, integrator='fdeint', method='gl')\n\n# Fractional solving with Predictor-Corrector scheme\nnet_pred = SNNWrapper(model, integrator='fdeint', method='pred')\n\n# Fractional solving with L1 scheme\nnet_l1 = SNNWrapper(model, integrator='fdeint', method='l1')\n</code></pre> <p>Common <code>method</code> options include <code>'euler'</code> (exclusive to <code>'odeint'</code>), <code>'gl'</code>, <code>'pred'</code>, <code>'l1'</code>, and <code>'trap'</code>. For a deep dive into the mathematical foundations and performance characteristics of each solver, please refer to Solvers in spikeDE: Powering Temporal Memory.</p>"},{"location":"tutorials/intermediate/snnWrapper/#memory-truncation","title":"Memory Truncation","text":"<p>Fractional calculus theoretically requires infinite history. For long sequences, you can truncate the memory to improve speed: <pre><code>net = SNNWrapper(\n    model, \n    alpha=0.7, \n    method='gl',\n    options={'memory': 50}  # Only look back 50 steps\n)\n</code></pre></p>"},{"location":"tutorials/intermediate/snnWrapper/#summary","title":"Summary","text":"<p>The <code>SNNWrapper</code> is the bridge between static PyTorch definitions and dynamic continuous-time simulations. By automating architecture inference, managing complex fractional configurations, and delegating heavy lifting to optimized solvers, it allows you to focus on designing neural architectures rather than wrestling with differential equation solvers.</p> <p>Whether you need simple integer-order ODEs or complex, learnable, multi-term fractional dynamics, <code>SNNWrapper</code> provides a unified, pythonic interface to power your next-generation Spiking Neural Networks.</p>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/category/release/","title":"Release","text":""},{"location":"blog/category/research/","title":"Research","text":""}]}